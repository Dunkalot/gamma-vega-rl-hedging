{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1357,
   "id": "fd33a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "288a5229",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_gamma_spread5.groupby('episode').aggregate('sum')['step_pnl'] -df_gamma_spread0.groupby('episode').aggregate('sum')['step_pnl']).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fef39a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gamma_spread0.groupby('episode').aggregate('sum')['step_pnl'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2b4e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import utils\n",
    "from importlib import reload\n",
    "reload(utils)\n",
    "hedge_swaption, liab_swaption, _hedge_swap, liab_swap, liab_swaption_position, cov_hed_all, cov_liab_all, ttm_mat = utils.Utils().generate_swaption_market_data()\n",
    "hedge_swap_ep = _hedge_swap[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce572ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean(hedge_swaption[1:100,0,0,-1]*hedge_swaption[1:100,0,0,3]*0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f83509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(_hedge_swap[1:100,0,0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f478ca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(hedge_swaption[1:100,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8058b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(hedge_swaption[1:100,:,0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f93fef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "time_pure_log = '123128'\n",
    "time= '181957'\n",
    "df = pd.read_csv(f'/home/hedge/gamma-vega-rl-hedging/logs/run_20250515_{time}/RL/Huber/spread=0.005_obj=cvar_threshold=0.95_critic=qr-huber_v=0.3_hedttm=30/logs/learner/logs.csv')\n",
    "#df_eval = pd.read_csv(f'/home/hedge/gamma-vega-rl-hedging/logs/run_20250514_{time}/RL/Huber/spread=0.005_obj=cvar_threshold=0.95_critic=qr-huber_v=0.3_hedttm=30/logs/eval_env/logs.csv')\n",
    "#time = '120609'\n",
    "#df_old = pd.read_csv(f'/home/hedge/gamma-vega-rl-hedging/logs/run_20250509_{time}/RL/Huber/spread=0.05_obj=cvar_threshold=0.95_critic=qr-huber_v=0.3_hedttm=30/logs/learner/logs.csv')\n",
    "df_train = pd.read_csv(f'/home/hedge/gamma-vega-rl-hedging/logs/run_20250515_{time}/RL/Huber/spread=0.005_obj=cvar_threshold=0.95_critic=qr-huber_v=0.3_hedttm=30/logs/train_loop/logs.csv'\n",
    "                    )\n",
    "#df_eval_old = pd.read_csv(f'/home/hedge/gamma-vega-rl-hedging/logs/run_20250509_{time}/RL/Huber/spread=0.05_obj=cvar_threshold=0.95_critic=qr-huber_v=0.3_hedttm=30/logs/eval_env/logs.csv'\n",
    "#                    )\n",
    "#df_eval = pd.read_csv(f'/home/hedge/gamma-vega-rl-hedging/logs/run_20250514_{time}/RL/Huber/spread=0.005_obj=cvar_threshold=0.95_critic=qr-huber_v=0.3_hedttm=30/logs/eval_env/logs.csv')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from importlib import reload\n",
    "reload(matplotlib)\n",
    "#typ = 'policy'\n",
    "typ = 'critic'\n",
    "def plot_loss(typ,df):\n",
    "    clip = 1000\n",
    "    window = 200\n",
    "    df[f'{typ}_loss'].iloc[clip:].plot() \n",
    "    mean = df[f'{typ}_loss'].rolling(window=window).mean()[clip:]\n",
    "    std = df[f'{typ}_loss'].rolling(window=window).std()[clip:]\n",
    "    plt.plot(mean)\n",
    "    plt.plot(mean+std)\n",
    "    plt.plot(mean-std)\n",
    "    plt.plot(std*2)\n",
    "    plt.plot()\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plt.clf()\n",
    "plt.clf()\n",
    "plot_loss('critic',df)\n",
    "#print(\"old\")\n",
    "#plot_loss('critic',df_old)\n",
    "\n",
    "plot_loss('policy',df)\n",
    "#print(\"old\")\n",
    "#plot_loss('policy',df_old)\n",
    "#plt.clf()\n",
    "#df['critic_loss'].plot()\n",
    "#plt.legend()\n",
    "#df_train['episode_return'].plot()\n",
    "\n",
    "# Calculate rolling mean for episode_return\n",
    "episode_return_mean = df_train['episode_return'].rolling(window=100).mean() # Adjust window size as needed\n",
    "\n",
    "# Calculate rolling 0.05 quantile for episode_return (CVaR 95%)\n",
    "episode_return_cvar95 = df_train['episode_return'].rolling(window=200).quantile(0.05) # Adjust window size as needed\n",
    "\n",
    "# # Plotting\n",
    "# plt.figure() # Optional: Adjust figure size\n",
    "# plt.plot(episode_return_mean, label='Rolling Mean of Episode Return')\n",
    "# plt.plot(episode_return_cvar95, label='Rolling CVaR 95% of Episode Return')\n",
    "\n",
    "# plt.title('Episode Return Over Time')\n",
    "# plt.xlabel('Time Step / Episode') # Adjust label as appropriate for your data\n",
    "# #plt.ylabel('Episode Return')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "# print(len(df_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f6165",
   "metadata": {},
   "source": [
    "# RL Agent Learning Diagnostics\n",
    "\n",
    "This section provides visualizations and analysis to help monitor how well the critic is fitting the return distribution and other metrics to diagnose training issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bfb7ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Set Seaborn style for better visualizations\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "from time import sleep\n",
    "\n",
    "def load_training_data(run_id=None, time=None, critic_type='qr-huber', spread=0.005, obj_func='cvar', \n",
    "                       threshold=0.95, vov=0.3, hedttm=30):\n",
    "    \"\"\"\n",
    "    Load training data from logs folder based on run_id or time.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    run_id : str\n",
    "        The run ID in format 'YYYYMMDD_HHMMSS'\n",
    "    time : str\n",
    "        Time component of the run ID if run_id is not provided\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary with DataFrames for learner, train_loop, and eval_env\n",
    "    \"\"\"\n",
    "    if run_id is None and time is None:\n",
    "        # Get the most recent run folder\n",
    "        run_folders = sorted(glob.glob('/home/hedge/gamma-vega-rl-hedging/logs/run_*'))\n",
    "        if not run_folders:\n",
    "            raise ValueError(\"No run folders found\")\n",
    "        run_path = run_folders[-1]\n",
    "        run_id = os.path.basename(run_path).replace('run_', '')\n",
    "    elif time is not None:\n",
    "        # Use provided time component to construct run_id\n",
    "        date_part = run_id.split('_')[0] if run_id else '20250514'\n",
    "        run_id = f\"{date_part}_{time}\"\n",
    "    \n",
    "    # Construct path to logs\n",
    "    work_folder = f'spread={spread}_obj={obj_func}_threshold={threshold}_critic={critic_type}_v={vov}_hedttm={hedttm}'\n",
    "    base_path = f'/home/hedge/gamma-vega-rl-hedging/logs/run_{run_id}/RL/Huber/{work_folder}'\n",
    "    \n",
    "    print(f\"Loading data from: {base_path}\")\n",
    "    \n",
    "    # Load data\n",
    "    data = {}\n",
    "    try:\n",
    "        data['learner'] = pd.read_csv(f'{base_path}/logs/learner/logs.csv')\n",
    "        print(f\"Loaded learner data: {len(data['learner'])} rows\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Learner logs not found\")\n",
    "    \n",
    "    try:\n",
    "        data['train_loop'] = pd.read_csv(f'{base_path}/logs/train_loop/logs.csv')\n",
    "        print(f\"Loaded train_loop data: {len(data['train_loop'])} rows\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Train loop logs not found\")\n",
    "    \n",
    "    try:\n",
    "        data['eval_env'] = pd.read_csv(f'{base_path}/logs/eval_env/logs.csv')\n",
    "        print(f\"Loaded eval_env data: {len(data['eval_env'])} rows\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Evaluation environment logs not found\")\n",
    "        \n",
    "    return data, base_path\n",
    "\n",
    "def plot_enhanced_loss_curves(learner_df, window_sizes=[50, 200, 500], clip_start=0):\n",
    "    \"\"\"\n",
    "    Plot critic and policy loss curves with more detailed statistics and visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    learner_df : DataFrame\n",
    "        The learner log data\n",
    "    window_sizes : list\n",
    "        List of window sizes to use for different smoothing levels\n",
    "    clip_start : int\n",
    "        Number of initial data points to exclude\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Plot critic loss\n",
    "    plt.subplot(2, 1, 1)\n",
    "    raw_critic_loss = learner_df['critic_loss'].iloc[clip_start:]\n",
    "    \n",
    "    # Plot raw data with low opacity\n",
    "    plt.plot(raw_critic_loss.index, raw_critic_loss, alpha=0.2, color='gray', linewidth=0.5, label='Raw Loss')\n",
    "    \n",
    "    # Plot smoothed lines with different window sizes\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(window_sizes)))\n",
    "    \n",
    "    for i, window in enumerate(window_sizes):\n",
    "        mean_critic = raw_critic_loss.rolling(window=window).mean()\n",
    "        std_critic = raw_critic_loss.rolling(window=window).std()\n",
    "        \n",
    "        plt.plot(mean_critic.index, mean_critic, color=colors[i], linewidth=2, \n",
    "                 label=f'Mean (window={window})')\n",
    "        plt.fill_between(mean_critic.index, \n",
    "                        mean_critic - std_critic,\n",
    "                        mean_critic + std_critic,\n",
    "                        alpha=0.2, color=colors[i])\n",
    "    \n",
    "    # Add a trend line (linear regression)\n",
    "    x = np.array(range(len(raw_critic_loss)))\n",
    "    if len(x) > 1:  # Ensure we have enough data for regression\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(x, raw_critic_loss)\n",
    "        trend_line = intercept + slope * x\n",
    "        plt.plot(raw_critic_loss.index, trend_line, 'r--', \n",
    "                 label=f'Trend (slope={slope*(len(learner_df)-clip_start):.6f}, R²={r_value**2:.3f})')\n",
    "    \n",
    "    plt.title('Critic Loss Over Training Steps', fontsize=14)\n",
    "    plt.xlabel('Training Steps', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics as text\n",
    "    stats_text = (\n",
    "        f\"Min: {raw_critic_loss.min():.6f}\\n\"\n",
    "        f\"Max: {raw_critic_loss.max():.6f}\\n\"\n",
    "        f\"Mean: {raw_critic_loss.mean():.6f}\\n\"\n",
    "        f\"Std Dev: {raw_critic_loss.std():.6f}\\n\"\n",
    "        f\"Latest: {raw_critic_loss.iloc[-1]:.6f}\"\n",
    "    )\n",
    "    plt.text(0.02, 0.95, stats_text, transform=plt.gca().transAxes, \n",
    "             bbox=dict(facecolor='white', alpha=0.7), fontsize=10, va='top')\n",
    "    \n",
    "    # Plot policy loss with same approach\n",
    "    plt.subplot(2, 1, 2)\n",
    "    raw_policy_loss = learner_df['policy_loss'].iloc[clip_start:]\n",
    "    \n",
    "    plt.plot(raw_policy_loss.index, raw_policy_loss, alpha=0.2, color='gray', linewidth=0.5, label='Raw Loss')\n",
    "    \n",
    "    for i, window in enumerate(window_sizes):\n",
    "        mean_policy = raw_policy_loss.rolling(window=window).mean()\n",
    "        std_policy = raw_policy_loss.rolling(window=window).std()\n",
    "        \n",
    "        plt.plot(mean_policy.index, mean_policy, color=colors[i], linewidth=2, \n",
    "                 label=f'Mean (window={window})')\n",
    "        plt.fill_between(mean_policy.index, \n",
    "                        mean_policy - std_policy,\n",
    "                        mean_policy + std_policy,\n",
    "                        alpha=0.2, color=colors[i])\n",
    "    \n",
    "    # Add a trend line for policy loss\n",
    "    if len(x) > 1:  # Ensure we have enough data for regression\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(x, raw_policy_loss)\n",
    "        trend_line = intercept + slope * x\n",
    "        plt.plot(raw_policy_loss.index, trend_line, 'r--', \n",
    "                 label=f'Trend (slope={slope*(len(learner_df)-clip_start):.8f}, R²={r_value**2:.3f})')\n",
    "    \n",
    "    plt.title('Policy Loss Over Training Steps', fontsize=14)\n",
    "    plt.xlabel('Training Steps', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics for policy loss\n",
    "    stats_text = (\n",
    "        f\"Min: {raw_policy_loss.min():.8f}\\n\"\n",
    "        f\"Max: {raw_policy_loss.max():.8f}\\n\"\n",
    "        f\"Mean: {raw_policy_loss.mean():.8f}\\n\"\n",
    "        f\"Std Dev: {raw_policy_loss.std():.8f}\\n\"\n",
    "        f\"Latest: {raw_policy_loss.iloc[-1]:.8f}\"\n",
    "    )\n",
    "    plt.text(0.02, 0.95, stats_text, transform=plt.gca().transAxes, \n",
    "             bbox=dict(facecolor='white', alpha=0.7), fontsize=10, va='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_episode_returns(train_df, window_sizes=[20, 100, 200]):\n",
    "    \"\"\"\n",
    "    Plot episode returns with detailed statistical analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_df : DataFrame\n",
    "        The training loop data\n",
    "    window_sizes : list\n",
    "        List of window sizes for different smoothing levels\n",
    "    \"\"\"\n",
    "    if 'episode_return' not in train_df.columns:\n",
    "        print(\"Episode return data not found in training data\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    gs = gridspec.GridSpec(2, 2, height_ratios=[2, 1])\n",
    "    \n",
    "    # Main plot of episode returns\n",
    "    ax1 = plt.subplot(gs[0, :])\n",
    "    \n",
    "    returns = train_df['episode_return']\n",
    "    episodes = train_df['episodes']\n",
    "    \n",
    "    # Plot individual returns as scatter points\n",
    "    ax1.scatter(episodes, returns, alpha=0.3, s=10, color='gray', label='Individual Episodes')\n",
    "    \n",
    "    # Plot smoothed returns with different window sizes\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(window_sizes)))\n",
    "    \n",
    "    for i, window in enumerate(window_sizes):\n",
    "        mean_returns = returns.rolling(window=window).mean()\n",
    "        std_returns = returns.rolling(window=window).std()\n",
    "        \n",
    "        ax1.plot(episodes, mean_returns, linewidth=2, color=colors[i], \n",
    "                 label=f'Mean (window={window})')\n",
    "        ax1.fill_between(episodes, \n",
    "                        mean_returns - std_returns,\n",
    "                        mean_returns + std_returns,\n",
    "                        alpha=0.2, color=colors[i])\n",
    "    \n",
    "    # Add CVaR 95% line\n",
    "    if len(returns) >= 200:  # Only calculate if we have enough data\n",
    "        cvar_window = max(window_sizes)\n",
    "        cvar95 = returns.rolling(window=cvar_window).quantile(0.05)\n",
    "        ax1.plot(episodes, cvar95, color='red', linestyle='--', \n",
    "                 label=f'CVaR 95% (window={cvar_window})')\n",
    "    \n",
    "    ax1.set_title('Episode Returns During Training', fontsize=14)\n",
    "    ax1.set_xlabel('Episode', fontsize=12)\n",
    "    ax1.set_ylabel('Return', fontsize=12)\n",
    "    ax1.legend(loc='upper left', fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add a text box with statistics\n",
    "    latest_window = min(100, len(returns))\n",
    "    recent_returns = returns.iloc[-latest_window:]\n",
    "    \n",
    "    stats_text = (\n",
    "        f\"Overall Statistics:\\n\"\n",
    "        f\"  Mean: {returns.mean():.4f}\\n\"\n",
    "        f\"  Std Dev: {returns.std():.4f}\\n\"\n",
    "        f\"  Min: {returns.min():.4f}\\n\"\n",
    "        f\"  Max: {returns.max():.4f}\\n\\n\"\n",
    "        f\"Last {latest_window} Episodes:\\n\"\n",
    "        f\"  Mean: {recent_returns.mean():.4f}\\n\"\n",
    "        f\"  Std Dev: {recent_returns.std():.4f}\\n\"\n",
    "        f\"  CVaR 95%: {np.percentile(recent_returns, 5):.4f}\"\n",
    "    )\n",
    "    \n",
    "    ax1.text(0.02, 0.97, stats_text, transform=ax1.transAxes, \n",
    "             bbox=dict(facecolor='white', alpha=0.7), fontsize=10, va='top')\n",
    "    \n",
    "    # Histogram of returns\n",
    "    ax2 = plt.subplot(gs[1, 0])\n",
    "    sns.histplot(returns, kde=True, ax=ax2)\n",
    "    ax2.axvline(returns.mean(), color='red', linestyle='--', label=f'Mean: {returns.mean():.4f}')\n",
    "    ax2.axvline(np.percentile(returns, 5), color='green', linestyle='--', \n",
    "                label=f'5th Percentile: {np.percentile(returns, 5):.4f}')\n",
    "    ax2.axvline(np.percentile(recent_returns, 5), color='green', linestyle='--', \n",
    "                label=f'5th Percentile (recent returns): {np.percentile(recent_returns, 5):.4f}')\n",
    "    ax2.set_title('Distribution of Episode Returns', fontsize=12)\n",
    "    ax2.legend(fontsize=10)\n",
    "    \n",
    "    # QQ plot to check for normality\n",
    "    ax3 = plt.subplot(gs[1, 1])\n",
    "    stats.probplot(returns, plot=ax3)\n",
    "    ax3.set_title('QQ Plot of Episode Returns', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return key statistics\n",
    "    return {\n",
    "        'mean': returns.mean(),\n",
    "        'std': returns.std(),\n",
    "        'min': returns.min(),\n",
    "        'max': returns.max(),\n",
    "        'median': returns.median(),\n",
    "        'cvar95': np.percentile(returns, 5),\n",
    "        'latest_mean': recent_returns.mean(),\n",
    "        'latest_std': recent_returns.std()\n",
    "    }\n",
    "\n",
    "data, base_path = load_training_data(time=time)\n",
    "# Plot enhanced loss curves if learner data is available\n",
    "if 'learner' in data and len(data['learner']) > 0:\n",
    "    # Determine appropriate clip value - typically early training is noisy\n",
    "    total_steps = len(data['learner'])\n",
    "    clip_start = min(int(total_steps * 0.05), 200_000)  # Skip first 5% or 2500 steps, whichever is smaller\n",
    "    \n",
    "    print(f\"Plotting loss curves with initial {clip_start} steps clipped\")\n",
    "    plot_enhanced_loss_curves(data['learner'], clip_start=clip_start)\n",
    "else:\n",
    "    print(\"Learner data not available for loss curve analysis\")\n",
    "\n",
    "\n",
    "# Plot training episode returns if data is available\n",
    "if 'train_loop' in data and len(data['train_loop']) > 0:\n",
    "    statss = plot_training_episode_returns(data['train_loop'])\n",
    "    print(\"\\nEpisode Return Statistics:\")\n",
    "    for key, value in statss.items():\n",
    "        print(f\"  {key}: {value:.6f}\")\n",
    "else:\n",
    "    print(\"Training loop data not available for episode return analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df5d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_action_vs_state(eval_env_df):\n",
    "    \"\"\"\n",
    "    Analyze how actions relate to state variables.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eval_env_df : DataFrame\n",
    "        The evaluation environment log data\n",
    "    \"\"\"\n",
    "    if eval_env_df is None or len(eval_env_df) == 0:\n",
    "        print(\"No evaluation environment data available\")\n",
    "        return\n",
    "    \n",
    "    # Identify relevant state variables\n",
    "    state_vars = [col for col in eval_env_df.columns if col in \n",
    "                 ['gamma_ratio', 'iv_norm', 'rate_norm', 'hed_cost_norm', 'ttm']]\n",
    "    \n",
    "    if not state_vars:\n",
    "        print(\"No relevant state variables found in evaluation data\")\n",
    "        return\n",
    "    \n",
    "    # Create a figure with subplots for each state variable\n",
    "    n_vars = len(state_vars)\n",
    "    fig, axes = plt.subplots(1, n_vars, figsize=(15, 5))\n",
    "    \n",
    "    if n_vars == 1:  # Make axes iterable if there's only one subplot\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot action vs each state variable\n",
    "    for i, var in enumerate(state_vars):\n",
    "        axes[i].scatter(eval_env_df[var], eval_env_df['action_gamma'], alpha=0.1)\n",
    "        axes[i].set_title(f'Action vs {var}', fontsize=12)\n",
    "        axes[i].set_xlabel(var, fontsize=10)\n",
    "        axes[i].set_ylabel('action_gamma', fontsize=10)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Try to fit a regression line if enough points\n",
    "        if len(eval_env_df) > 10:\n",
    "            try:\n",
    "                from scipy.stats import linregress\n",
    "                mask = ~np.isnan(eval_env_df[var]) & ~np.isnan(eval_env_df['action_gamma'])\n",
    "                if sum(mask) > 10:  # Only fit if we have enough valid data points\n",
    "                    x = eval_env_df[var][mask]\n",
    "                    y = eval_env_df['action_gamma'][mask]\n",
    "                    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "                    \n",
    "                    # Plot regression line\n",
    "                    x_range = np.linspace(x.min(), x.max(), 100)\n",
    "                    axes[i].plot(x_range, intercept + slope*x_range, 'r', \n",
    "                                label=f'R²={r_value**2:.3f}')\n",
    "                    axes[i].legend(loc='best', fontsize=8)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not fit regression for {var}: {e}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create more detailed analysis of the most important relationship\n",
    "    # Let's find which state variable has the highest correlation with actions\n",
    "    correlations = {}\n",
    "    for var in state_vars:\n",
    "        mask = ~np.isnan(eval_env_df[var]) & ~np.isnan(eval_env_df['action_gamma'])\n",
    "        if sum(mask) > 10:\n",
    "            correlations[var] = eval_env_df[var][mask].corr(eval_env_df['action_gamma'][mask])\n",
    "    \n",
    "    if correlations:\n",
    "        # Sort by absolute correlation value\n",
    "        sorted_corrs = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "        print(\"\\nCorrelations between state variables and actions:\")\n",
    "        for var, corr in sorted_corrs:\n",
    "            print(f\"  {var}: {corr:.4f}\")\n",
    "        \n",
    "        # Plot the most correlated variable in more detail\n",
    "        best_var = sorted_corrs[0][0]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1])\n",
    "        \n",
    "        # Scatter plot with colored points by episode\n",
    "        ax1 = plt.subplot(gs[0])\n",
    "        \n",
    "        # Select a limited number of episodes to show with different colors\n",
    "        episodes = eval_env_df['episode'].unique()\n",
    "        selected_episodes = episodes[:min(10, len(episodes))]  # Select up to 10 episodes\n",
    "        \n",
    "        for ep in selected_episodes:\n",
    "            ep_data = eval_env_df[eval_env_df['episode'] == ep]\n",
    "            ax1.scatter(ep_data[best_var], ep_data['action_gamma'], alpha=0.7, label=f'Episode {ep}')\n",
    "        \n",
    "        # Add trend line for all data\n",
    "        mask = ~np.isnan(eval_env_df[best_var]) & ~np.isnan(eval_env_df['action_gamma'])\n",
    "        if sum(mask) > 10:\n",
    "            x = eval_env_df[best_var][mask]\n",
    "            y = eval_env_df['action_gamma'][mask]\n",
    "            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "            \n",
    "            x_range = np.linspace(x.min(), x.max(), 100)\n",
    "            ax1.plot(x_range, intercept + slope*x_range, 'r--', linewidth=2,\n",
    "                    label=f'Trend: y = {slope:.4f}x + {intercept:.4f}, R²={r_value**2:.4f}')\n",
    "        \n",
    "        ax1.set_title(f'Action vs {best_var} (by episode)', fontsize=14)\n",
    "        ax1.set_xlabel(best_var, fontsize=12)\n",
    "        ax1.set_ylabel('action_gamma', fontsize=12)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend(loc='best', fontsize=8)\n",
    "        \n",
    "        # Histogram of the best variable\n",
    "        ax2 = plt.subplot(gs[1])\n",
    "        sns.histplot(eval_env_df[best_var], kde=True, ax=ax2)\n",
    "        ax2.set_title(f'Distribution of {best_var}', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "# Analyze actions vs state variables if evaluation data is available\n",
    "if 'eval_env' in data and len(data['eval_env']) > 0:\n",
    "    analyze_action_vs_state(data['eval_env'])\n",
    "else:\n",
    "    print(\"Evaluation environment data not available for action analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4728d5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "320bd7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "052876a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pnl_distribution(dataframes, labels=None, xlim=None, legend_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Plot KDE distributions of PnL for multiple dataframes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframes : dict or list\n",
    "        If dict: {name: dataframe} pairs\n",
    "        If list: List of dataframes\n",
    "    labels : list, optional\n",
    "        Labels for each dataframe (used if dataframes is a list)\n",
    "    xlim : tuple, optional\n",
    "        x-axis limits (min, max)\n",
    "    legend_prefix : str, optional\n",
    "        Prefix to add to legend entries\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # Convert list to dict if needed\n",
    "    if isinstance(dataframes, list):\n",
    "        if labels is None:\n",
    "            labels = [f\"Dataset {i+1}\" for i in range(len(dataframes))]\n",
    "        dataframes = dict(zip(labels, dataframes))\n",
    "    \n",
    "    # Process each dataframe\n",
    "    colors = sns.color_palette()\n",
    "    for i, (name, df) in enumerate(dataframes.items()):\n",
    "        # Calculate PnL sum per episode\n",
    "        pnl_sum = df.groupby('episode')['step_pnl'].sum()\n",
    "        \n",
    "        # Plot KDE\n",
    "        sns.kdeplot(pnl_sum, label=f\"{name}\")\n",
    "        \n",
    "        # Add vertical line for mean\n",
    "        mean_val = pnl_sum.mean()\n",
    "        plt.axvline(mean_val, color=colors[i], linestyle='--',\n",
    "                   label=f'Mean {name}: {mean_val:.4f}')\n",
    "        alpha = 0.05\n",
    "        var95 = pnl_sum.quantile(alpha)\n",
    "        cvar95 = pnl_sum[pnl_sum <= var95].mean()\n",
    "        \n",
    "        # Add vertical line for 0.05 quantile (CVaR 95%)\n",
    "        #quantile_val = pnl_sum.quantile(0.05)\n",
    "        plt.axvline(cvar95, color=colors[i], linestyle='-',\n",
    "                   label=f'CVaR95 {name}: {cvar95:.4f}')\n",
    "        \n",
    "    \n",
    "    # Add plot details\n",
    "    plt.title('PnL Distribution by Strategy')\n",
    "    plt.xlabel('PnL')\n",
    "    plt.ylabel('Density')\n",
    "    \n",
    "    if xlim:\n",
    "        plt.xlim(xlim)\n",
    "    \n",
    "    #plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return summary statistics\n",
    "    results = {}\n",
    "    for name, df in dataframes.items():\n",
    "        pnl_sum = df.groupby('episode')['step_pnl'].sum()\n",
    "        results[name] = {\n",
    "            'mean': pnl_sum.mean(),\n",
    "            'std': pnl_sum.std(),\n",
    "            'quantile_05': pnl_sum.quantile(0.05),\n",
    "            'min': pnl_sum.min(),\n",
    "            'max': pnl_sum.max()\n",
    "        }\n",
    "    return results\n",
    "\n",
    "\n",
    "def load_df(name, spread, dataset):\n",
    "    df = pd.read_csv(f'/home/hedge/gamma-vega-rl-hedging/logs/greekhedge_spread={spread}/{dataset}/{name}/logs/eval_env/logs.csv'\n",
    "                           )\n",
    "    return df\n",
    "spread = 0.005\n",
    "dataset = 'stress'\n",
    "names = ['delta', 'gamma', 'gamma_partial70']\n",
    "\n",
    "df_list = [load_df(name,spread, 'test') for name in names]\n",
    "df_list.append(df_eval)\n",
    "names.append('RL')\n",
    "plot_pnl_distribution(df_list, names, xlim=(-5,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1918,
   "id": "817a030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.mean(df_eval.groupby('episode').aggregate('sum')['cost_swaption_hed']))\n",
    "print(np.mean(df_gamma_spread5.groupby('episode').aggregate('sum')['cost_swaption_hed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_eval.groupby('episode').quantile(0.05)['step_pnl'])\n",
    "print(np.mean(df_gamma_partial50.groupby('episode').quantile(0.05)['step_pnl'])-np.mean(df_gamma_spread5.groupby('episode').quantile(0.05)['step_pnl']))\n",
    "np.mean(df_gamma_spread5.groupby('episode').quantile(0.05)['step_pnl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1872,
   "id": "1ee04f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_eval_old.groupby('episode').quantile(0.05)['step_pnl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca9d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gamma_partial50.groupby('episode').aggregate('sum')['step_pnl'].mean()*100-df_gamma_spread5.groupby('episode').aggregate('sum')['step_pnl'].mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06736fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29edbc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETTING FLOAT PRECISION TO 32\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3070 Laptop GPU, compute capability 8.6\n",
      "Loading agent from: ./logs/run_20250516_084625/RL/Huber/spread=0.005_obj=cvar_threshold=0.95_critic=qr-huber_v=0.3_hedttm=30\n",
      "Path exists: ./logs/run_20250516_084625/RL/Huber/spread=0.005_obj=cvar_threshold=0.95_critic=qr-huber_v=0.3_hedttm=30\n",
      "Contents: ['logs', 'observation', 'policy', 'ok']\n",
      "Policy directory exists: ./logs/run_20250516_084625/RL/Huber/spread=0.005_obj=cvar_threshold=0.95_critic=qr-huber_v=0.3_hedttm=30/policy\n",
      "Contents: ['variables', 'saved_model.pb', 'assets']\n",
      "Observation directory exists: ./logs/run_20250516_084625/RL/Huber/spread=0.005_obj=cvar_threshold=0.95_critic=qr-huber_v=0.3_hedttm=30/observation\n",
      "Contents: ['variables', 'saved_model.pb', 'assets']\n",
      "utils initiated with spread=0.005, poisson_rate=1, n_episodes=100\n",
      "\n",
      "Memory usage before lmm: 772.41 MB\n",
      "t_max set to 7.25\n",
      "!!!! CONTRACT SIZE IS  100.0\n",
      "\n",
      "XXXXXXXXXXXXXXXXXXXXXX\n",
      " The spread is 0.005   \n",
      " nXXXXXXXXXXXXXXXXXXXXXX\n",
      "Memory usage after: 801.19 MB\n",
      "\n",
      "Memory usage after gc: 801.19 MB\n",
      "\n",
      "TRAINING WITH LOGGER: True\n",
      "\n",
      "Using  data/stress/20250512-204102_5y_60000 dataset\n",
      "initializing classes\n",
      "done initializing classes\n",
      "Using  data/stress/20250512-204102_5y_60000 dataset\n",
      "63\n",
      "episode offset 49999\n",
      "USING QUANTILE NETWORK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 11:42:15.682275: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized networks - policy: 10 variables, observation: 0 variables (observation network is function)\n",
      "Observation network is a function, not a Sonnet module with trainable variables.\n",
      "Will only load policy network weights.\n",
      "Loading policy network with 10 variables\n",
      "Target policy network has 10 variables\n",
      "Starting evaluation with 100 episodes...\n",
      "Action stats - Mean: 0.0000, Std: 0.0000\n",
      "Min: 0.0000, Max: 0.0000\n",
      "Percentiles - p1: 0.0000, p5: 0.0000, p50: 0.0000, p95: 0.0000, p99: 0.0000\n",
      "Clipped values - Min: 0.0000 (0.0%), Max: 0.0000 (0.0%)\n",
      "Action stats - Mean: 0.5866, Std: 0.2136\n",
      "Min: 0.2108, Max: 0.9927\n",
      "Percentiles - p1: 0.2178, p5: 0.2631, p50: 0.5669, p95: 0.9047, p99: 0.9922\n",
      "Clipped values - Min: 0.0000 (0.0%), Max: 0.0000 (0.0%)\n",
      "Action stats - Mean: 0.7745, Std: 0.1730\n",
      "Min: 0.4635, Max: 1.0000\n",
      "Percentiles - p1: 0.4818, p5: 0.5100, p50: 0.7718, p95: 0.9999, p99: 1.0000\n",
      "Clipped values - Min: 0.0000 (0.0%), Max: 0.0000 (0.0%)\n",
      "Action stats - Mean: 0.6342, Std: 0.1848\n",
      "Min: 0.0000, Max: 1.0000\n",
      "Percentiles - p1: 0.1574, p5: 0.3369, p50: 0.6617, p95: 0.8722, p99: 0.9292\n",
      "Clipped values - Min: 0.0000 (0.0%), Max: 0.0008 (0.1%)\n",
      "Action stats - Mean: 0.6820, Std: 0.1079\n",
      "Min: 0.5038, Max: 1.0000\n",
      "Percentiles - p1: 0.5179, p5: 0.5555, p50: 0.6372, p95: 0.8554, p99: 0.9961\n",
      "Clipped values - Min: 0.0000 (0.0%), Max: 0.0000 (0.0%)\n",
      "Evaluation Results:\n",
      "Mean Return: -0.0091\n",
      "Std Return: 0.5527\n",
      "VaR (95.0%): 0.2437\n",
      "CVaR (5.000000000000004%): -0.2653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hedge/venv/lib/python3.9/site-packages/traitlets/traitlets.py:1385: DeprecationWarning: Passing unrecognized arguments to super(Toolbar).__init__().\n",
      "__init__() missing 1 required positional argument: 'canvas'\n",
      "This is deprecated in traitlets 4.2.This error will be raised in a future release of traitlets.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import sonnet as snt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add project root to path to import modules\n",
    "sys.path.append('/home/hedge/gamma-vega-rl-hedging')\n",
    "\n",
    "from acme import specs\n",
    "from acme import wrappers\n",
    "from acme.agents.tf import actors\n",
    "from acme.tf.savers import make_snapshot\n",
    "import acme.utils.loggers as log_utils\n",
    "import acme\n",
    "\n",
    "from environment.Environment import TradingEnv, EvalLog\n",
    "from environment.utils import Utils\n",
    "from run import load_agent, make_quantile_networks, make_networks, make_iqn_networks\n",
    "\n",
    "def initialize_networks(policy_net, observation_net, environment_spec):\n",
    "    \"\"\"Initialize networks by performing a dummy forward pass.\"\"\"\n",
    "    # Create a dummy observation matching the environment spec\n",
    "    dummy_obs = tf.zeros([1, environment_spec.observations.shape[0]], dtype=tf.float32)\n",
    "    \n",
    "    # Pass through observation network (which could be a function or a module)\n",
    "    processed_obs = observation_net(dummy_obs)\n",
    "    \n",
    "    # Pass through policy network\n",
    "    _ = policy_net(processed_obs)\n",
    "    \n",
    "    # Check if observation_net has trainable variables (it might be a simple function)\n",
    "    obs_vars = getattr(observation_net, 'trainable_variables', [])\n",
    "    \n",
    "    print(f\"Initialized networks - policy: {len(policy_net.trainable_variables)} variables, \"\n",
    "          f\"observation: {len(obs_vars)} variables (observation network is {type(observation_net).__name__})\")\n",
    "\n",
    "# Updated function to check if a model was successfully loaded\n",
    "def verify_model_loaded(policy_net):\n",
    "    \"\"\"Verify the model has trainable variables and weights are non-zero.\"\"\"\n",
    "    if len(policy_net.trainable_variables) == 0:\n",
    "        return False\n",
    "        \n",
    "    # Check if weights are properly loaded (not all zeros or default initialization)\n",
    "    has_meaningful_weights = False\n",
    "    for var in policy_net.trainable_variables:\n",
    "        if np.any(var.numpy() != 0) and not np.allclose(var.numpy(), 0):\n",
    "            has_meaningful_weights = True\n",
    "            break\n",
    "            \n",
    "    return has_meaningful_weights\n",
    "\n",
    "# Debug function to inspect a model path\n",
    "def debug_model_path(model_path):\n",
    "    \"\"\"Check if a model path exists and what files it contains.\"\"\"\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Path exists: {model_path}\")\n",
    "        contents = os.listdir(model_path)\n",
    "        print(f\"Contents: {contents}\")\n",
    "        \n",
    "        # Check if policy and observation directories exist\n",
    "        policy_path = os.path.join(model_path, 'policy')\n",
    "        obs_path = os.path.join(model_path, 'observation')\n",
    "        \n",
    "        if os.path.exists(policy_path):\n",
    "            print(f\"Policy directory exists: {policy_path}\")\n",
    "            print(f\"Contents: {os.listdir(policy_path)}\")\n",
    "        else:\n",
    "            print(f\"Policy directory doesn't exist: {policy_path}\")\n",
    "            \n",
    "        if os.path.exists(obs_path):\n",
    "            print(f\"Observation directory exists: {obs_path}\")\n",
    "            print(f\"Contents: {os.listdir(obs_path)}\")\n",
    "        else:\n",
    "            print(f\"Observation directory doesn't exist: {obs_path}\")\n",
    "    else:\n",
    "        print(f\"Path doesn't exist: {model_path}\")\n",
    "        # Try to find parent directories\n",
    "        parent = os.path.dirname(model_path)\n",
    "        if os.path.exists(parent):\n",
    "            print(f\"Parent directory exists: {parent}\")\n",
    "            print(f\"Contents: {os.listdir(parent)}\")\n",
    "\n",
    "# Fix for the previous cell's load_and_evaluate_model function\n",
    "def load_and_evaluate_model(run_id=None, \n",
    "                           agent_path=None,\n",
    "                           work_folder=None,\n",
    "                           critic_type='qr-huber',\n",
    "                           spread=0.005,\n",
    "                           obj_func='cvar',\n",
    "                           threshold=0.95,\n",
    "                           vov=0.3,\n",
    "                           hed_ttm=30,\n",
    "                           n_episodes=1000,\n",
    "                           plot_results=True):\n",
    "    \"\"\"\n",
    "    Load a trained model and evaluate it with custom parameters.\n",
    "    \"\"\"\n",
    "    # Construct work folder if not provided\n",
    "    if work_folder is None:\n",
    "        work_folder = f'spread={spread}_obj={obj_func}_threshold={threshold}_critic={critic_type}_v={vov}_hedttm={hed_ttm}'\n",
    "    \n",
    "    # Construct agent path if not provided\n",
    "    if agent_path is None and run_id is not None:\n",
    "        agent_path = f'./logs/run_{run_id}/RL/Huber/{work_folder}'\n",
    "    elif agent_path is None:\n",
    "        raise ValueError(\"Either run_id or agent_path must be provided\")\n",
    "        \n",
    "    print(f\"Loading agent from: {agent_path}\")\n",
    "    debug_model_path(agent_path)  # Debug the model path\n",
    "    \n",
    "    # Set up evaluation environment\n",
    "    eval_utils = Utils(n_episodes=n_episodes, tenor=4, spread=spread, test=True)\n",
    "    \n",
    "    # Create logger for results\n",
    "    results_dir = f\"./ablation_results/{Path(agent_path).name}_ablation_{spread}_{obj_func}_{vov}\"\n",
    "    os.makedirs(f\"{results_dir}/logs/eval\", exist_ok=True)\n",
    "    \n",
    "    logger = log_utils.CSVLogger(results_dir, label='eval', add_uid=False)\n",
    "    logger = log_utils.Dispatcher([logger], log_utils.to_numpy)\n",
    "    logger = log_utils.NoneFilter(logger)\n",
    "    \n",
    "    # Create environment with logger\n",
    "    eval_log_func = EvalLog()\n",
    "    eval_env = wrappers.GymWrapper(TradingEnv(\n",
    "        utils=eval_utils,\n",
    "        log_bef=eval_log_func._log_before,\n",
    "        log_af=eval_log_func._log_after,\n",
    "        logger=logger  # Pass the logger here\n",
    "    ))\n",
    "    eval_env = wrappers.SinglePrecisionWrapper(eval_env)\n",
    "    \n",
    "    # Get environment spec\n",
    "    environment_spec = specs.make_environment_spec(eval_env)\n",
    "    \n",
    "    # Create networks based on critic type\n",
    "    if critic_type == 'c51':\n",
    "        agent_networks = make_networks(action_spec=environment_spec.actions, max_time_steps=eval_utils.num_period)\n",
    "    elif 'qr' in critic_type:\n",
    "        agent_networks = make_quantile_networks(action_spec=environment_spec.actions)\n",
    "    elif critic_type == 'iqn':\n",
    "        agent_networks = make_iqn_networks(action_spec=environment_spec.actions, cvar_th=threshold, max_time_steps=62)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown critic type: {critic_type}\")\n",
    "    \n",
    "    # Create policy and observation networks\n",
    "    policy_net = agent_networks['policy']\n",
    "    observation_net = agent_networks['observation']\n",
    "    \n",
    "    # Initialize networks before loading\n",
    "    initialize_networks(policy_net, observation_net, environment_spec)\n",
    "    \n",
    "    # Load the agent - handle the case when observation_net is a function\n",
    "    if not hasattr(observation_net, 'trainable_variables'):\n",
    "        print(\"Observation network is a function, not a Sonnet module with trainable variables.\")\n",
    "        print(\"Will only load policy network weights.\")\n",
    "        \n",
    "        # Initialize policy with a dummy pass\n",
    "        dummy_obs = tf.zeros([1, environment_spec.observations.shape[0]], dtype=tf.float32)\n",
    "        processed_obs = observation_net(dummy_obs)\n",
    "        _ = policy_net(processed_obs)\n",
    "        \n",
    "        # Load only the policy network\n",
    "        try:\n",
    "            trainable_variables_snapshot = {}\n",
    "            load_policy_net = tf.saved_model.load(f\"{agent_path}/policy\")\n",
    "            \n",
    "            print(f\"Loading policy network with {len(load_policy_net.trainable_variables)} variables\")\n",
    "            print(f\"Target policy network has {len(policy_net.trainable_variables)} variables\")\n",
    "            \n",
    "            for var in load_policy_net.trainable_variables:\n",
    "                var_name = '/'.join(var.name.split('/')[1:])\n",
    "                trainable_variables_snapshot[var_name] = var.numpy()\n",
    "                \n",
    "            for var in policy_net.trainable_variables:\n",
    "                var_name_wo_name_scope = '/'.join(var.name.split('/')[1:])\n",
    "                if var_name_wo_name_scope in trainable_variables_snapshot:\n",
    "                    var.assign(trainable_variables_snapshot[var_name_wo_name_scope])\n",
    "                else:\n",
    "                    print(f\"WARNING: Variable {var_name_wo_name_scope} not found in saved model\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading policy: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        # Load both policy and observation networks\n",
    "        load_agent(policy_net, observation_net, agent_path)\n",
    "    \n",
    "    # Verify model was loaded\n",
    "    if not verify_model_loaded(policy_net):\n",
    "        raise RuntimeError(\"Model failed to load properly. Check the model path and initialization.\")\n",
    "    \n",
    "    # Create evaluation policy\n",
    "    eval_policy = snt.Sequential([\n",
    "        observation_net,\n",
    "        policy_net,\n",
    "    ])\n",
    "    \n",
    "    # Create evaluation actor\n",
    "    eval_actor = actors.FeedForwardActor(policy_network=eval_policy)\n",
    "    \n",
    "    # The environment logger is already set up above, but we need a separate one for the evaluation loop\n",
    "    eval_loop_logger = log_utils.CSVLogger(results_dir, label='eval_loop', add_uid=False)\n",
    "    eval_loop_logger = log_utils.Dispatcher([eval_loop_logger], log_utils.to_numpy)\n",
    "    eval_loop_logger = log_utils.NoneFilter(eval_loop_logger)\n",
    "    \n",
    "    # Run evaluation\n",
    "    print(f\"Starting evaluation with {n_episodes} episodes...\")\n",
    "    eval_loop = acme.EnvironmentLoop(eval_env, eval_actor, label='eval_loop', logger=eval_loop_logger)\n",
    "    eval_loop.run(num_episodes=n_episodes)\n",
    "    \n",
    "    # Load and process results\n",
    "    results_df = pd.read_csv(f\"{results_dir}/logs/eval/logs.csv\")\n",
    "    \n",
    "    # Calculate episode returns\n",
    "    episode_returns = results_df.groupby('episode')['step_pnl'].sum()\n",
    "    \n",
    "    # Calculate key metrics\n",
    "    mean_return = episode_returns.mean()\n",
    "    std_return = episode_returns.std()\n",
    "    cvar = episode_returns.quantile(1-threshold)\n",
    "    var = episode_returns.quantile(threshold)\n",
    "    \n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"Mean Return: {mean_return:.4f}\")\n",
    "    print(f\"Std Return: {std_return:.4f}\")\n",
    "    print(f\"VaR ({threshold*100}%): {var:.4f}\")\n",
    "    print(f\"CVaR ({(1-threshold)*100}%): {cvar:.4f}\")\n",
    "    \n",
    "    if plot_results:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot PnL distribution\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(episode_returns, kde=True)\n",
    "        plt.axvline(mean_return, color='r', linestyle='-', label=f'Mean: {mean_return:.4f}')\n",
    "        plt.axvline(cvar, color='g', linestyle='--', label=f'CVaR {(1-threshold)*100}%: {cvar:.4f}')\n",
    "        plt.title('PnL Distribution')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot cumulative returns over episodes\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(episode_returns.cumsum() / np.arange(1, len(episode_returns) + 1))\n",
    "        plt.title('Cumulative Average Return')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return results_df, eval_env\n",
    "\n",
    "# Try the function with debug output:\n",
    "try:\n",
    "    results, env = load_and_evaluate_model(\n",
    "        run_id='20250516_084625',\n",
    "        critic_type='qr-huber', \n",
    "        spread=0.005,\n",
    "        obj_func='cvar',\n",
    "        threshold=0.95,\n",
    "        vov=0.3,\n",
    "        hed_ttm=30,\n",
    "        n_episodes=100  # Reduced for testing\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10734119",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "600afba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_single = results.iloc[:60]\n",
    "results_single\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "923e44d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_gamma = results_single['action_gamma']\n",
    "iv = results_single.iv_norm\n",
    "gamma = results_single.gamma_ratio\n",
    "plt.scatter(gamma,action_gamma)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "0d022ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path to import modules if needed\n",
    "sys.path.append('/home/hedge/gamma-vega-rl-hedging')\n",
    "\n",
    "from acme import specs\n",
    "from acme.tf import utils as tf2_utils\n",
    "from run import make_quantile_networks, make_iqn_networks, make_networks, load_agent\n",
    "from environment.utils import Utils\n",
    "from environment.Environment import TradingEnv\n",
    "\n",
    "# Set visual style\n",
    "#plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "589d2534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_critic_network(run_id=None, time=None, critic_type='qr-huber', spread=0.005, \n",
    "                       obj_func='cvar', threshold=0.95, vov=0.3, hedttm=30):\n",
    "    \"\"\"\n",
    "    Load the critic network from a trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    run_id : str\n",
    "        The run ID in format 'YYYYMMDD_HHMMSS'\n",
    "    time : str\n",
    "        Time component of the run ID if run_id is not provided\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    critic_network : tf.Module\n",
    "        The loaded critic network\n",
    "    observation_network : tf.Module\n",
    "        The loaded observation network\n",
    "    environment_spec : acme.specs.EnvironmentSpec\n",
    "        The environment specification\n",
    "    \"\"\"\n",
    "    if run_id is None and time is None:\n",
    "        # Get the most recent run\n",
    "        raise ValueError(\"Either run_id or time must be provided\")\n",
    "    elif time is not None:\n",
    "        # Use provided time component to construct run_id\n",
    "        date_part = run_id.split('_')[0] if run_id else '20250514'\n",
    "        run_id = f\"{date_part}_{time}\"\n",
    "    \n",
    "    # Construct path to logs\n",
    "    work_folder = f'spread={spread}_obj={obj_func}_threshold={threshold}_critic={critic_type}_v={vov}_hedttm={hedttm}'\n",
    "    base_path = f'/home/hedge/gamma-vega-rl-hedging/logs/run_{run_id}/RL/Huber/{work_folder}'\n",
    "    \n",
    "    print(f\"Loading model from: {base_path}\")\n",
    "    \n",
    "    # Create a dummy environment to get the specs\n",
    "    utils = Utils(n_episodes=1000, tenor=4, spread=spread)\n",
    "    environment_spec = specs.make_environment_spec(TradingEnv(utils=utils))\n",
    "    \n",
    "    # Create networks based on critic type\n",
    "    if critic_type == 'c51':\n",
    "        agent_networks = make_networks(action_spec=environment_spec.actions, max_time_steps=utils.num_period)\n",
    "    elif 'qr' in critic_type:\n",
    "        agent_networks = make_quantile_networks(action_spec=environment_spec.actions)\n",
    "    elif critic_type == 'iqn':\n",
    "        agent_networks = make_iqn_networks(action_spec=environment_spec.actions, cvar_th=threshold, max_time_steps=62)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown critic type: {critic_type}\")\n",
    "    \n",
    "    # Get critic and observation networks\n",
    "    critic_network = agent_networks['critic']\n",
    "    observation_network = agent_networks['observation']\n",
    "    \n",
    "    # Initialize network variables with a dummy pass\n",
    "    dummy_obs = tf.zeros([1, environment_spec.observations.shape[0]], dtype=tf.float32)\n",
    "    dummy_action = tf.zeros([1, environment_spec.actions.shape[0]], dtype=tf.float32)\n",
    "    \n",
    "    # First process the observation\n",
    "    processed_obs = observation_network(dummy_obs)\n",
    "    \n",
    "    # Then pass to critic\n",
    "    _ = critic_network(processed_obs, dummy_action)\n",
    "    \n",
    "    # Now load the trained weights\n",
    "    try:\n",
    "        load_agent(None, observation_network, base_path)\n",
    "        print(\"Observation network loaded successfully\")\n",
    "        \n",
    "        # For critic, we need to extract it from the snapshot\n",
    "        snapshot = tf.saved_model.load(f\"{base_path}/critic\")\n",
    "        \n",
    "        # Copy weights from snapshot to our critic network\n",
    "        for var, loaded_var in zip(critic_network.variables, snapshot.variables):\n",
    "            var.assign(loaded_var)\n",
    "        \n",
    "        print(\"Critic network loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading networks: {e}\")\n",
    "        # This error is expected if we don't have the critic saved separately\n",
    "        print(\"Will proceed with initialized networks which may not have trained weights\")\n",
    "    \n",
    "    return critic_network, observation_network, environment_spec, base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "cafc39d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episode_trajectories(base_path):\n",
    "    \"\"\"\n",
    "    Collect evaluation episode trajectories from logs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_path : str\n",
    "        Base path to the model directory\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with episode trajectories\n",
    "    \"\"\"\n",
    "    try:\n",
    "        eval_path = f\"{base_path}/logs/eval_env/logs.csv\"\n",
    "        df = pd.read_csv(eval_path)\n",
    "        print(f\"Loaded evaluation data: {len(df)} rows from {len(df['episode'].unique())} episodes\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading evaluation data: {e}\")\n",
    "        print(\"Will attempt to use training data instead\")\n",
    "        \n",
    "        try:\n",
    "            train_path = f\"{base_path}/logs/train_loop/logs.csv\"\n",
    "            df = pd.read_csv(train_path)\n",
    "            print(f\"Loaded training data: {len(df)} rows\")\n",
    "            return df\n",
    "        except Exception as e2:\n",
    "            print(f\"Error loading training data: {e2}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "064107a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model using the time variable from previous cells\n",
    "try:\n",
    "    critic_network, observation_network, environment_spec, base_path = load_critic_network(time=time)\n",
    "    trajectories_df = collect_episode_trajectories(base_path)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "a75dc1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_quantiles_from_critic(critic_network, observation_network, states, actions):\n",
    "    \"\"\"\n",
    "    Extract predicted quantiles from the critic network.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    critic_network : tf.Module\n",
    "        The critic network\n",
    "    observation_network : tf.Module\n",
    "        The observation processing network\n",
    "    states : np.ndarray\n",
    "        Batch of states\n",
    "    actions : np.ndarray\n",
    "        Batch of actions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    quantiles : np.ndarray\n",
    "        Predicted quantiles (batch_size, n_quantiles)\n",
    "    \"\"\"\n",
    "    # Convert inputs to tensors\n",
    "    states_tensor = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "    actions_tensor = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "    \n",
    "    # Process observations\n",
    "    processed_obs = observation_network(states_tensor)\n",
    "    \n",
    "    # Get critic output\n",
    "    critic_output = critic_network(processed_obs, actions_tensor)\n",
    "    \n",
    "    # For QR-DQN, the output should be the quantile values\n",
    "    # The exact format depends on the implementation\n",
    "    \n",
    "    # Check if critic output has a logits attribute (C51 distributional critic)\n",
    "    if hasattr(critic_output, 'logits'):\n",
    "        # For C51, we have logits that need to be converted to probabilities\n",
    "        probs = tf.nn.softmax(critic_output.logits, axis=-1)\n",
    "        # Get the support values\n",
    "        atoms = critic_output.atoms\n",
    "        # Calculate the expected value\n",
    "        expected_value = tf.reduce_sum(probs * atoms, axis=-1)\n",
    "        # For visualization purposes, let's create synthetic quantiles from the distribution\n",
    "        # This is a simple approach - for a proper implementation, you'd calculate the actual quantiles\n",
    "        cum_probs = tf.cumsum(probs, axis=-1)\n",
    "        quantile_indices = []\n",
    "        quantiles = np.linspace(0.05, 0.95, 10)  # 10 quantiles\n",
    "        for q in quantiles:\n",
    "            idx = tf.argmax((cum_probs > q).astype(tf.float32), axis=-1)\n",
    "            quantile_indices.append(idx)\n",
    "        quantile_indices = tf.stack(quantile_indices, axis=-1)\n",
    "        batch_indices = tf.range(tf.shape(quantile_indices)[0])\n",
    "        batch_indices = tf.tile(tf.expand_dims(batch_indices, -1), [1, tf.shape(quantile_indices)[1]])\n",
    "        indices = tf.stack([batch_indices, quantile_indices], axis=-1)\n",
    "        quantile_values = tf.gather_nd(atoms, indices)\n",
    "        return quantile_values.numpy(), expected_value.numpy()\n",
    "    \n",
    "    # For QR-DQN, we directly have the quantile values\n",
    "    elif hasattr(critic_output, 'values'):\n",
    "        # Return the quantile values and their mean\n",
    "        expected_value = tf.reduce_mean(critic_output.values, axis=-1)\n",
    "        return critic_output.values.numpy(), expected_value.numpy()\n",
    "    \n",
    "    # In case the structure is different, try a generic approach\n",
    "    else:\n",
    "        try:\n",
    "            # Assume the first attribute is the distribution\n",
    "            first_attr = list(critic_output.__dict__.keys())[0]\n",
    "            values = getattr(critic_output, first_attr)\n",
    "            if isinstance(values, tf.Tensor):\n",
    "                expected_value = tf.reduce_mean(values, axis=-1)\n",
    "                return values.numpy(), expected_value.numpy()\n",
    "            else:\n",
    "                print(f\"Unexpected critic output structure: {type(critic_output)}\")\n",
    "                return None, None\n",
    "        except:\n",
    "            print(f\"Failed to extract quantiles from critic output: {type(critic_output)}\")\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eded0d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_critic_predictions(critic_network, observation_network, trajectories_df, \n",
    "                                n_episodes=5, samples_per_episode=10):\n",
    "    \"\"\"\n",
    "    Visualize how well the critic predicts the actual return distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    critic_network : tf.Module\n",
    "        The critic network\n",
    "    observation_network : tf.Module\n",
    "        The observation processing network\n",
    "    trajectories_df : pd.DataFrame\n",
    "        DataFrame with episode trajectories\n",
    "    n_episodes : int, optional\n",
    "        Number of episodes to visualize\n",
    "    samples_per_episode : int, optional\n",
    "        Number of samples per episode to visualize\n",
    "    \"\"\"\n",
    "    if trajectories_df is None or len(trajectories_df) == 0:\n",
    "        print(\"No trajectory data available\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of unique episodes\n",
    "    unique_episodes = trajectories_df['episode'].unique()\n",
    "    if len(unique_episodes) < n_episodes:\n",
    "        n_episodes = len(unique_episodes)\n",
    "    \n",
    "    # Select episodes to visualize\n",
    "    selected_episodes = np.sort(np.random.choice(unique_episodes, size=n_episodes, replace=False))\n",
    "    \n",
    "    plt.figure(figsize=(16, n_episodes * 4))\n",
    "    \n",
    "    for i, episode in enumerate(selected_episodes):\n",
    "        # Get data for this episode\n",
    "        episode_data = trajectories_df[trajectories_df['episode'] == episode].copy()\n",
    "        \n",
    "        # Skip episodes without enough time steps\n",
    "        if len(episode_data) < 10:\n",
    "            continue\n",
    "        \n",
    "        # We'll sample state-action pairs from different time points in the episode\n",
    "        if len(episode_data) <= samples_per_episode:\n",
    "            sampled_indices = np.arange(len(episode_data))\n",
    "        else:\n",
    "            sampled_indices = np.sort(np.random.choice(\n",
    "                np.arange(len(episode_data)), \n",
    "                size=samples_per_episode, \n",
    "                replace=False\n",
    "            ))\n",
    "        \n",
    "        sampled_data = episode_data.iloc[sampled_indices].copy()\n",
    "        \n",
    "        # Extract states and actions\n",
    "        states = sampled_data[['rate_norm', 'hed_cost_norm', 'gamma_ratio', \n",
    "                               'gamma_unit_norm', 'iv_norm', 'ttm']].values\n",
    "        actions = sampled_data[['action_gamma']].values\n",
    "        \n",
    "        # Get critic predictions\n",
    "        quantiles, expected_values = extract_quantiles_from_critic(\n",
    "            critic_network, observation_network, states, actions\n",
    "        )\n",
    "        \n",
    "        if quantiles is None:\n",
    "            print(\"Failed to extract quantiles from critic\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate the actual returns from this point onwards for each sampled step\n",
    "        actual_returns = []\n",
    "        for idx in sampled_indices:\n",
    "            future_rewards = episode_data.iloc[idx:]['step_pnl'].values\n",
    "            # Apply discounting\n",
    "            discounted_return = 0\n",
    "            discount = 0.98  # Use the same discount factor as in training\n",
    "            for t, reward in enumerate(future_rewards):\n",
    "                discounted_return += discount**t * reward\n",
    "            actual_returns.append(discounted_return)\n",
    "        \n",
    "        # Create subplots for this episode\n",
    "        plt.subplot(n_episodes, 2, i*2 + 1)\n",
    "        \n",
    "        # Plot predicted quantiles vs actual returns\n",
    "        for j in range(len(sampled_indices)):\n",
    "            # Plot predicted quantiles as violin plot\n",
    "            if quantiles.ndim == 2:  # (batch_size, n_quantiles)\n",
    "                sns.violinplot(data=quantiles[j:j+1], inner='quartile', cut=0)\n",
    "            else:  # Handle different shapes\n",
    "                print(f\"Unexpected quantiles shape: {quantiles.shape}\")\n",
    "                if quantiles.ndim >= 3:  # Try to reshape\n",
    "                    reshaped = quantiles[j].reshape(-1)\n",
    "                    sns.violinplot(data=reshaped.reshape(1, -1), inner='quartile', cut=0)\n",
    "                \n",
    "            # Add actual return as a red X\n",
    "            plt.scatter(0, actual_returns[j], color='red', marker='x', s=100, zorder=10)\n",
    "            \n",
    "            # Add time step and expected value\n",
    "            plt.title(f\"Episode {episode} - Step {sampled_data.iloc[j]['t']}, \"\n",
    "                     f\"Expected Value: {expected_values[j]:.4f}, \"\n",
    "                     f\"Actual Return: {actual_returns[j]:.4f}\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Only show one violinplot per subplot\n",
    "            break\n",
    "        \n",
    "        # Second subplot: Compare all samples in the episode\n",
    "        plt.subplot(n_episodes, 2, i*2 + 2)\n",
    "        \n",
    "        # Create a dataframe for easy plotting\n",
    "        comparison_df = pd.DataFrame({\n",
    "            'Time Step': sampled_data['t'].values,\n",
    "            'Expected Value': expected_values,\n",
    "            'Actual Return': actual_returns,\n",
    "            'Error': expected_values - actual_returns\n",
    "        })\n",
    "        \n",
    "        # Plot expected vs actual return\n",
    "        plt.scatter(comparison_df['Time Step'], comparison_df['Expected Value'], \n",
    "                   label='Critic Prediction', marker='o', s=80)\n",
    "        plt.scatter(comparison_df['Time Step'], comparison_df['Actual Return'], \n",
    "                   label='Actual Return', marker='x', s=80)\n",
    "        \n",
    "        # Plot prediction error\n",
    "        plt.scatter(comparison_df['Time Step'], comparison_df['Error'], \n",
    "                   label='Prediction Error', marker='s', alpha=0.5)\n",
    "        \n",
    "        # Add zero line for error reference\n",
    "        plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        plt.title(f\"Episode {episode} - Critic Predictions vs Actual Returns\")\n",
    "        plt.xlabel(\"Time Step\")\n",
    "        plt.ylabel(\"Return\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the comparison data for further analysis\n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a2990347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize critic predictions for selected episodes\n",
    "if 'critic_network' in locals() and 'observation_network' in locals() and trajectories_df is not None:\n",
    "    try:\n",
    "        comparison_df = visualize_critic_predictions(\n",
    "            critic_network, observation_network, trajectories_df, \n",
    "            n_episodes=3, samples_per_episode=10\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing critic predictions: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Required components not loaded. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "57675402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prediction_accuracy(trajectories_df, n_episodes=20, max_horizon=30):\n",
    "    \"\"\"\n",
    "    Analyze how the prediction accuracy varies with the prediction horizon.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    trajectories_df : pd.DataFrame\n",
    "        DataFrame with episode trajectories\n",
    "    n_episodes : int, optional\n",
    "        Number of episodes to analyze\n",
    "    max_horizon : int, optional\n",
    "        Maximum prediction horizon to consider\n",
    "    \"\"\"\n",
    "    if trajectories_df is None or len(trajectories_df) == 0:\n",
    "        print(\"No trajectory data available\")\n",
    "        return\n",
    "    \n",
    "    # Get a list of unique episodes\n",
    "    unique_episodes = trajectories_df['episode'].unique()\n",
    "    if len(unique_episodes) < n_episodes:\n",
    "        n_episodes = len(unique_episodes)\n",
    "    \n",
    "    # Select episodes to analyze\n",
    "    selected_episodes = np.sort(np.random.choice(unique_episodes, size=n_episodes, replace=False))\n",
    "    \n",
    "    # Prepare data for horizon analysis\n",
    "    horizons = list(range(1, max_horizon + 1))\n",
    "    mse_by_horizon = {h: [] for h in horizons}\n",
    "    mae_by_horizon = {h: [] for h in horizons}\n",
    "    \n",
    "    # For each episode, compute prediction error at different horizons\n",
    "    for episode in selected_episodes:\n",
    "        episode_data = trajectories_df[trajectories_df['episode'] == episode].copy()\n",
    "        \n",
    "        # Skip episodes without enough time steps\n",
    "        if len(episode_data) <= max_horizon:\n",
    "            continue\n",
    "        \n",
    "        # For each step in the episode (up to max_horizon from the end)\n",
    "        for t in range(len(episode_data) - max_horizon):\n",
    "            # Get the actual returns for each horizon\n",
    "            for horizon in horizons:\n",
    "                if t + horizon < len(episode_data):\n",
    "                    # Calculate discounted return for this horizon\n",
    "                    future_rewards = episode_data.iloc[t:t+horizon]['step_pnl'].values\n",
    "                    discount = 0.98  # Use the same discount as in training\n",
    "                    discounted_return = 0\n",
    "                    for h, reward in enumerate(future_rewards):\n",
    "                        discounted_return += discount**h * reward\n",
    "                    \n",
    "                    # For now, use a simple heuristic as the \"predicted\" value\n",
    "                    # In a real analysis, you'd use the critic's prediction\n",
    "                    # But we need to compute that for each state-action pair\n",
    "                    predicted_return = episode_data.iloc[t]['step_pnl'] * horizon * 0.8\n",
    "                    \n",
    "                    # Compute error metrics\n",
    "                    mse = (predicted_return - discounted_return)**2\n",
    "                    mae = abs(predicted_return - discounted_return)\n",
    "                    \n",
    "                    mse_by_horizon[horizon].append(mse)\n",
    "                    mae_by_horizon[horizon].append(mae)\n",
    "    \n",
    "    # Compute average error metrics by horizon\n",
    "    avg_mse = [np.mean(mse_by_horizon[h]) for h in horizons]\n",
    "    avg_mae = [np.mean(mae_by_horizon[h]) for h in horizons]\n",
    "    \n",
    "    # Plot error metrics by horizon\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(horizons, avg_mse, marker='o', linewidth=2)\n",
    "    plt.title('Mean Squared Error by Prediction Horizon')\n",
    "    plt.xlabel('Prediction Horizon (Steps)')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(horizons, avg_mae, marker='o', linewidth=2)\n",
    "    plt.title('Mean Absolute Error by Prediction Horizon')\n",
    "    plt.xlabel('Prediction Horizon (Steps)')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {'mse': avg_mse, 'mae': avg_mae, 'horizons': horizons}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "a5f20323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction accuracy by horizon\n",
    "if trajectories_df is not None:\n",
    "    try:\n",
    "        horizon_metrics = analyze_prediction_accuracy(\n",
    "            trajectories_df, n_episodes=20, max_horizon=30\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing prediction accuracy: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Trajectory data not loaded. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "6c23ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_return_distribution_by_state(trajectories_df, state_variable='gamma_ratio', \n",
    "                                          n_bins=5, min_samples=50):\n",
    "    \"\"\"\n",
    "    Visualize how the return distribution varies by state variable.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    trajectories_df : pd.DataFrame\n",
    "        DataFrame with episode trajectories\n",
    "    state_variable : str, optional\n",
    "        State variable to analyze\n",
    "    n_bins : int, optional\n",
    "        Number of bins for the state variable\n",
    "    min_samples : int, optional\n",
    "        Minimum number of samples required in each bin\n",
    "    \"\"\"\n",
    "    if trajectories_df is None or len(trajectories_df) == 0:\n",
    "        print(\"No trajectory data available\")\n",
    "        return\n",
    "    \n",
    "    if state_variable not in trajectories_df.columns:\n",
    "        print(f\"State variable {state_variable} not found in data\")\n",
    "        return\n",
    "    \n",
    "    # Calculate the return from each state\n",
    "    # For simplicity, we'll use the remainder of the episode as the \"return\"\n",
    "    episode_groups = trajectories_df.groupby('episode')\n",
    "    all_returns = []\n",
    "    \n",
    "    for episode, group in episode_groups:\n",
    "        for i in range(len(group)):\n",
    "            # Current state\n",
    "            state_value = group.iloc[i][state_variable]\n",
    "            \n",
    "            # Future rewards\n",
    "            future_rewards = group.iloc[i:]['step_pnl'].values\n",
    "            \n",
    "            # Apply discounting\n",
    "            discounted_return = 0\n",
    "            discount = 0.98  # Use the same discount factor as in training\n",
    "            for t, reward in enumerate(future_rewards):\n",
    "                discounted_return += discount**t * reward\n",
    "            \n",
    "            all_returns.append({\n",
    "                'episode': episode,\n",
    "                'step': i,\n",
    "                state_variable: state_value,\n",
    "                'return': discounted_return\n",
    "            })\n",
    "    \n",
    "    returns_df = pd.DataFrame(all_returns)\n",
    "    \n",
    "    # Create bins for the state variable\n",
    "    bins = np.linspace(returns_df[state_variable].min(), \n",
    "                      returns_df[state_variable].max(),\n",
    "                      n_bins + 1)\n",
    "    \n",
    "    # Assign each sample to a bin\n",
    "    returns_df['bin'] = pd.cut(returns_df[state_variable], bins)\n",
    "    \n",
    "    # Ensure each bin has enough samples\n",
    "    bin_counts = returns_df['bin'].value_counts()\n",
    "    valid_bins = bin_counts[bin_counts >= min_samples].index\n",
    "    \n",
    "    if len(valid_bins) == 0:\n",
    "        print(f\"No bins have at least {min_samples} samples\")\n",
    "        return\n",
    "    \n",
    "    # Filter for valid bins\n",
    "    returns_df = returns_df[returns_df['bin'].isin(valid_bins)]\n",
    "    \n",
    "    # Plot return distributions by bin\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Box plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(x='bin', y='return', data=returns_df)\n",
    "    plt.title(f'Return Distribution by {state_variable}')\n",
    "    plt.xlabel(state_variable)\n",
    "    plt.ylabel('Return')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Violin plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.violinplot(x='bin', y='return', data=returns_df)\n",
    "    plt.title(f'Return Distribution by {state_variable}')\n",
    "    plt.xlabel(state_variable)\n",
    "    plt.ylabel('Return')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate statistics for each bin\n",
    "    stats = returns_df.groupby('bin')['return'].agg([\n",
    "        'mean', 'std', 'min', 'max',\n",
    "        lambda x: np.percentile(x, 5),  # 5th percentile\n",
    "        lambda x: np.percentile(x, 95)  # 95th percentile\n",
    "    ]).reset_index()\n",
    "    \n",
    "    stats = stats.rename(columns={\n",
    "        '<lambda_0>': 'percentile_05',\n",
    "        '<lambda_1>': 'percentile_95'\n",
    "    })\n",
    "    \n",
    "    print(\"Return statistics by state variable bin:\")\n",
    "    print(stats)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "8d0d47b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize return distribution by key state variables\n",
    "if trajectories_df is not None:\n",
    "    try:\n",
    "        # Analyze by gamma_ratio\n",
    "        gamma_stats = visualize_return_distribution_by_state(\n",
    "            trajectories_df, state_variable='gamma_ratio', n_bins=5\n",
    "        )\n",
    "        \n",
    "        # Analyze by iv_norm (normalized implied volatility)\n",
    "        iv_stats = visualize_return_distribution_by_state(\n",
    "            trajectories_df, state_variable='iv_norm', n_bins=5\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing return distributions: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Trajectory data not loaded. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b98578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
