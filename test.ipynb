{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "from scipy import interpolate\n",
    "from functools import partial\n",
    "from scipy.stats import norm\n",
    "import ipympl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def doust_corr(beta, n):\n",
    "    '''\n",
    "    create nxn doust correlation with beta decay exponential\n",
    "    n = # of semi-annual expiries\n",
    "    '''\n",
    "    tau = np.arange(0, n+1)/2 # start from spot\n",
    "    a = np.exp(- beta / np.arange(1, len(tau[:-1])+1) )\n",
    "    doust = np.zeros((n, n))\n",
    "    dim = doust.shape\n",
    "    for i in range(doust.shape[0]):\n",
    "        for j in range(doust.shape[1]):\n",
    "            if i == j:\n",
    "                doust[i, j] = 1\n",
    "            elif i > j:\n",
    "                doust[i, j] = np.prod(a[j:i])\n",
    "    #reflect\n",
    "    doust[np.triu_indices(dim[0], 1)] = doust.T[np.triu_indices(dim[0], 1)]\n",
    "    return(doust)\n",
    "\n",
    "\n",
    "def interpolate_correlation_matrix(matrix: np.ndarray, resolution: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Interpolates a correlation matrix using bilinear interpolation.\n",
    "\n",
    "    Args:\n",
    "        matrix (np.ndarray): The input correlation matrix (must be square).\n",
    "        resolution (int): The resolution factor. For a 4x4 and resolution=2, output will be 7x7.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Interpolated correlation matrix.\n",
    "    \"\"\"\n",
    "    if matrix.shape[0] != matrix.shape[1]:\n",
    "        raise ValueError(\"Input must be a square matrix.\")\n",
    "\n",
    "    # Compute zoom factor: new_size = original_size + (original_size - 1) * (resolution - 1)\n",
    "    zoom_factor = resolution\n",
    "\n",
    "    # Use order=1 for bilinear interpolation\n",
    "    interpolated = zoom(matrix, zoom=zoom_factor, order=1)\n",
    "\n",
    "    # Adjust shape to match expected output: new size = original + (n-1)*(res-1)\n",
    "    target_size = matrix.shape[0] + (matrix.shape[0] - 1) * (resolution - 1)\n",
    "    interpolated = interpolated[:target_size, :target_size]\n",
    "\n",
    "    return interpolated\n",
    "\n",
    "\n",
    "\n",
    "def get_instant_vol_func(tau , params):\n",
    "    '''\n",
    "    Return the instantaneous volatility ,\n",
    "    computed in terms of the parametric\n",
    "    form proposed by Rebonato , at a given time t.\n",
    "    @var t: time at which we want to compute the\n",
    "    instantaneous volatility (in years)\n",
    "    @var expiry: caplet expiry (in years)\n",
    "    @var a: parameter a of Rebonato ’s instant. vol. function\n",
    "    @var b: parameter b of Rebonato ’s instant. vol. function\n",
    "    @var c: parameter c of Rebonato ’s instant. vol. function\n",
    "    @var d: parameter d of Rebonato ’s instant. vol. function\n",
    "    \n",
    "    #g(T - t) & h(T - t)\n",
    "    '''\n",
    "    tau = np.maximum(tau, 0)\n",
    "    a,b,c,d = params\n",
    "    instantaneous_vol = (a + b * tau) * np.exp(-c * tau) + d\n",
    "    return instantaneous_vol\n",
    "\n",
    "\n",
    "def build_phi_matrix(T, phi_diag, lambda3, lambda4, add_spot=True):\n",
    "    n = len(T)\n",
    "    phi = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            Ti, Tj = T[i], T[j]\n",
    "            phi_ii, phi_jj = phi_diag[i], phi_diag[j]\n",
    "            A = np.sign(phi_ii) * np.sqrt(abs(phi_ii * phi_jj))\n",
    "            decay = np.exp(-lambda3 * max(Ti - Tj, 0) - lambda4 * max(Tj - Ti, 0))\n",
    "            phi[i, j] = A * decay\n",
    "\n",
    "    return phi\n",
    "\n",
    "\n",
    "def black_price(F, K, sigma, T, r=0.0, option_type=\"call\"):\n",
    "    \"\"\"\n",
    "    Black's formula for European options on forwards.\n",
    "\n",
    "    Args:\n",
    "        F (float): Forward rate\n",
    "        K (float): Strike\n",
    "        sigma (float): Implied volatility\n",
    "        T (float): Time to maturity\n",
    "        r (float): Discount rate (e.g. risk-free rate)\n",
    "        option_type (str): 'call' or 'put'\n",
    "\n",
    "    Returns:\n",
    "        float: Present value of the option\n",
    "    \"\"\"\n",
    "    print(\"T AND SIGMA\",T, sigma)\n",
    "    if np.isclose(T,0) or np.isclose(sigma,0):\n",
    "        print(\"WE AT MATURITY\")\n",
    "        intrinsic = max(F - K, 0) if option_type == \"call\" else max(K - F, 0)\n",
    "        return np.exp(-r * T) * intrinsic\n",
    "\n",
    "    d1 = (np.log(F / K) + 0.5 * sigma ** 2 * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "\n",
    "    if option_type == \"call\":\n",
    "        price = np.exp(-r * T) * (F * norm.cdf(d1) - K * norm.cdf(d2))\n",
    "    elif option_type == \"put\":\n",
    "        price = np.exp(-r * T) * (K * norm.cdf(-d2) - F * norm.cdf(-d1))\n",
    "    else:\n",
    "        raise ValueError(\"option_type must be 'call' or 'put'\")\n",
    "\n",
    "    return price\n",
    "\n",
    "def pairwise_outer(arr):\n",
    "    \"\"\"\n",
    "    Given an array of shape (..., d), return an array of shape (..., d, d),\n",
    "    where each (...)-indexed vector is expanded to an outer product with itself.\n",
    "\n",
    "    Parameters:\n",
    "    - arr: np.ndarray, shape (..., d)\n",
    "\n",
    "    Returns:\n",
    "    - out: np.ndarray, shape (..., d, d)\n",
    "    \"\"\"\n",
    "    return arr[..., :, None] * arr[..., None, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOADING AND CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_params = os.path.join(os.getcwd(), \"parameters\")\n",
    "forwards = load_object(path_params+\"/spot_forwards.pkl\")\n",
    "s0_exp = load_object(path_params+\"/vol_initial_correction.pkl\")\n",
    "epsilon_exp = load_object(path_params+\"/volvol_initial_correction.pkl\")\n",
    "doust_fwd_fwd = load_object(path_params+\"/fwdfwd_corr.pkl\")\n",
    "doust_vol_vol = load_object(path_params+\"/volvol_corr.pkl\")\n",
    "corr_fwd_vol = load_object(path_params+\"/fwdvol_corr.pkl\")\n",
    "params_g = load_object(path_params+\"/vol_params_g.pkl\")\n",
    "params_h= load_object(path_params+\"/volvol_params_h.pkl\")\n",
    "spots= load_object(path_params+\"/spot_rates.pkl\")\n",
    "params_g = np.array([-0.00557585, -0.00864318,  0.89466108,  0.00755986])\n",
    "params_h = np.array([1.42258187e-08, 3.01935702e01, 4.57201647e00, 4.05843346e-12,])\n",
    "epsilon_exp = np.concatenate([epsilon_exp[[0]],epsilon_exp])\n",
    "s0_exp = np.concatenate([s0_exp[[0]], s0_exp])\n",
    "\n",
    "\n",
    "\n",
    "rho_mat_6m = doust_fwd_fwd[:19, :19]\n",
    "theta_mat_6m = doust_vol_vol[:19, :19] #TODO: check if this is correct, or it should remove the first row and column instead\n",
    "phi_mat_diag = corr_fwd_vol\n",
    "fwd_tenors = np.arange(1,10.5,0.5)\n",
    "# self defined\n",
    "params_g = np.array([0.005, 0.04, 1, 0.001])\n",
    "params_h = np.array([0.001, 3, 3, 0.01])\n",
    "s0_exp = np.ones_like(s0_exp)\n",
    "\n",
    "\n",
    "# Create new matrices \n",
    "\n",
    "\n",
    "beta_6m = 0.20696204\n",
    "beta_0m = 0.25697769\n",
    "beta_theta_0m = 0.1556888\n",
    "beta_theta_6m = 0.12135651\n",
    "n=20\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create phi \n",
    "\n",
    "\n",
    "\n",
    "# interpolate phi diag\n",
    "phi_diag = np.diag(corr_fwd_vol)\n",
    "\n",
    "phi_diag = np.concatenate([phi_diag[[0]],phi_diag])\n",
    "T_phi = np.arange(0,10, 0.5)\n",
    "\n",
    "# SHOULD BE IN LMM CLASS\n",
    "\n",
    "rho_mat_0m = doust_corr(beta_0m, n)\n",
    "theta_mat_0m = doust_corr(beta_theta_0m, n)\n",
    "phi_mat_0m = build_phi_matrix(T_phi, phi_diag, 0.0087931, 0.051319)\n",
    "\n",
    "\n",
    "# create rates \n",
    "path = os.path.join(os.getcwd(), \"raw_dataset\")\n",
    "df_cap = pd.read_excel(path+\"/caplet_raw.xlsx\", sheet_name = 2, header = 0)\n",
    "df_raw_spot = pd.read_csv(path+\"/spot.csv\")\n",
    "df_raw_spot[\"Tenor\"] = np.array([1/12, 2/12, 3/12, 0.5, 0.75, 1, 2, 3, 4, 5, 7, 9, 10, 12, 15, 20, 30, 50])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# selfSABR RELATED FUNCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_df_init(df_fwd, df_raw_spot, resolution, tau=0.5):\n",
    "\n",
    "    # Get the LIBOR 6-month spot rate\n",
    "    libor_6m_spot = df_raw_spot.loc[df_raw_spot[\"Tenor\"] == 0.5, \"Spot\"].iloc[0] # floating point comparison only safe due to 0.5 bein representable with 2**-1\n",
    "\n",
    "    # Create a dataframe with the initial values for the spot rate\n",
    "    spot_row = pd.DataFrame({\n",
    "        'Fixing': [0.0],\n",
    "        'Reset Rate': [libor_6m_spot],  # Convert back to percentage\n",
    "        'Maturity': [0.5]\n",
    "    })\n",
    "\n",
    "    # Initialize df_init with df_cap data\n",
    "    df_full = df_fwd[['Fixing', 'Reset Rate', 'Maturity']].copy()\n",
    "\n",
    "    # Concatenate with the spot rate row and reset index\n",
    "    df_full = pd.concat([spot_row, df_full], ignore_index=True)\n",
    "    df_full['Reset Rate'] = df_full['Reset Rate'] / 100  # Convert to percentage\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    #                               TIME INDEXING\n",
    "    # =============================================================================\n",
    "    ts_fwd_expiry = df_full['Fixing'].values\n",
    "\n",
    "    dt = tau / resolution\n",
    "    ids_fwd_interp = (ts_fwd_expiry / dt).astype(int) # divide by dt to get indices in the new time unit\n",
    "    \n",
    "    \n",
    "    n_fwd = len(ts_fwd_expiry)-1 # exclude period covering the last forward rate tenor\n",
    "    ts_fwd_interp= np.linspace(0, n_fwd*tau, int(n_fwd * resolution +1))\n",
    "    #print(f\"{ts_fwd_interp=}\")\n",
    "    assert np.all(np.isin(ts_fwd_expiry, ts_fwd_interp)), \"Not all forward expirys are in the time grid\"\n",
    "    # =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    #           Create the zcb interpolated curve\n",
    "    # =============================================================================\n",
    "\n",
    "\n",
    "    fwd_canon = df_full['Reset Rate'].values\n",
    "    discount_factors = 1 / (1 + fwd_canon[:-1] * 0.5) # leave out last as we dont use zcb prices after the last forward rate\n",
    "    zcb_from_fwd = np.concatenate(([1], np.cumprod(discount_factors)))\n",
    "\n",
    "    zcb_cs = interpolate.CubicSpline(ts_fwd_expiry, zcb_from_fwd)\n",
    "    zcb_interp = zcb_cs(ts_fwd_interp)\n",
    "\n",
    "    # =============================================================================\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    #          Construct dataframe with tenors, zcb and forward rates\n",
    "    # =============================================================================\n",
    "    df = pd.DataFrame({'Tenor': ts_fwd_interp, 'zcb': zcb_interp, 'Forward': np.nan, 's0': np.nan})\n",
    "    df.loc[ids_fwd_interp, 'Forward'] = fwd_canon\n",
    "    df.loc[ids_fwd_interp, 'k0'] = s0_exp\n",
    "    # add column with backfilled forward indices, such that the value in this column is 0 from 0 to 5, 1 from 6 to 11, 2 from 12 to 17, etc.    \n",
    "    df['i_s'] = (np.arange(len(df)) // resolution)*resolution\n",
    "    df['i_sp1'] = (np.arange(len(df)) // resolution+1)*resolution\n",
    "    df['mod_accrual'] = tau - (df['Tenor'] % tau)\n",
    "    df_temp = df.merge(\n",
    "    df[['zcb', 'Forward']],\n",
    "    left_on='i_s',     # Column with pointers to index\n",
    "    right_index=True,      # Use index from right DataFrame\n",
    "    how='left',            # Keep all rows from original\n",
    "    suffixes=('', '_i_s')  # Add suffix to avoid column name conflicts\n",
    "    )\n",
    "    df_temp = df_temp.merge(\n",
    "    df[['zcb']],\n",
    "    left_on='i_sp1',     # Column with pointers to index\n",
    "    right_index=True,      # Use index from right DataFrame\n",
    "    how='left',            # Keep all rows from original\n",
    "    suffixes=('', '_i_sp1')  # Add suffix to avoid column name conflicts\n",
    "    )\n",
    "    df['gamma']= (df_temp['zcb'] / df_temp['zcb_i_sp1'] -1) / df_temp['mod_accrual'] /df_temp['Forward_i_s']\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    return df\n",
    "\n",
    "\n",
    "def interp_func_fac(df_init, resolution=2, tau=0.5, beta=0.5, rho_mat=None, g_func=None, interp_vol = False):\n",
    "    df = df_init\n",
    "    #fwd = df['Forward'].values # only used for test, not to be uncommented\n",
    "\n",
    "    i_s = df['i_s'].values[:-resolution]\n",
    "    i_sp1 = df['i_sp1'].values[:-resolution]\n",
    "    i_e = i_sp1\n",
    "\n",
    "    s = np.arange(len(df)-resolution)\n",
    "    #print(\"len s\",len(s))\n",
    "    e = s + resolution\n",
    "\n",
    "    theta = (df['mod_accrual'].values)\n",
    "    gamma = (df['gamma'].values)\n",
    "    gamma_theta = gamma * theta\n",
    "\n",
    "    #print(f\"{len(i_s)}, {len(i_e)}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def get_interp_rates(fwd):\n",
    "        p_s_e = (1 + fwd[i_e] * gamma_theta[e]) / (1 + fwd[i_s] * gamma_theta[s]) * 1/(1+fwd[i_e]*tau)\n",
    "        f_s_e = (1 / p_s_e - 1) / tau \n",
    "        return f_s_e\n",
    "    \n",
    "    if interp_vol:\n",
    "        rho_mat_interpolated = interpolate_correlation_matrix(rho_mat, resolution)\n",
    "        fwd = df['Forward'].values\n",
    "        f1 = fwd[i_s]**beta      \n",
    "        f2 = fwd[i_e]**beta\n",
    "        w1 = gamma_theta[s] / tau\n",
    "        w2 = (tau - gamma_theta[e]) / tau\n",
    "        f_interp = get_interp_rates(fwd)**beta\n",
    "        term1 = (w1**2) * (f1**2)/f_interp[s]**2\n",
    "        term2 = (w2**2) * (f2**2)/f_interp[s]**2\n",
    "        rho = rho_mat_interpolated[i_s, i_e]  \n",
    "        cross = 2 * w1 * w2 * f1 * f2 * rho / f_interp[s]**2\n",
    "        tenors = df['Tenor'].values\n",
    "        ttm_mat = tenors[None,:]-tenors[:,None]\n",
    "        g_mat = g_func(ttm_mat)\n",
    "        def get_interp_vol(s_arr,t_idx):\n",
    "            \n",
    "            s1 = s_arr[i_s]\n",
    "            s2 = s_arr[i_e]\n",
    "            # Compute numerator vectorized\n",
    "            term1_s = term1 * (s1**2)\n",
    "            term2_s = term2 * (s2**2)\n",
    "            cross_s = cross * s1 * s2 \n",
    "            sigma_sum = term1_s + term2_s + cross_s\n",
    "            s_interp = np.sqrt(sigma_sum)\n",
    "            return s_interp / g_mat[t_idx, s]\n",
    "\n",
    "\n",
    "        return get_interp_rates, get_interp_vol\n",
    "    \n",
    "    return get_interp_rates\n",
    "\n",
    "\n",
    "\n",
    "def get_swap_matrix(f_sim, T_idxs, resolution, tau, tenor, df, expiry=1, beta=0.5, B=0.5):\n",
    "    \"\"\"\n",
    "    Compute time-evolving swap rates from a simulated forward path for a set of swap expiries.\n",
    "\n",
    "    Parameters:\n",
    "    - f_sim: np.ndarray, shape (n_steps, n_forwards)\n",
    "        Simulated forward curve matrix (lower-triangular in time).\n",
    "    - T_idxs: list or np.ndarray of int\n",
    "        Forward indices (expiry) at which each swap starts.\n",
    "    - resolution: int\n",
    "        Number of simulation steps per accrual period (tau).\n",
    "    - tau: float\n",
    "        Accrual period of the swap (e.g., 0.5 for semiannual).\n",
    "    - tenor: float\n",
    "        Total length of the swap in years (e.g., 1.0 for a 1y swap).\n",
    "    - df: pd.DataFrame\n",
    "        DataFrame containing initial zero-coupon bond prices in column 'zcb'.\n",
    "    - expiry: float, optional\n",
    "        If provided, the maximum length of a simulated swap path will be limited to this value (in year units).\n",
    "    Returns:\n",
    "    - swap_paths: np.ndarray, shape (n_valid_steps, n_swaps)\n",
    "        Matrix of swap rates over time for each swap expiry.\n",
    "    - valid_steps: np.ndarray\n",
    "        Array of time steps for which all required forward rates exist.\n",
    "    - used_T_idxs: np.ndarray\n",
    "        Final T_idxs that were valid and included in the result.\n",
    "    \"\"\"\n",
    "    f_sim = np.asarray(f_sim)\n",
    "    T_idxs = np.asarray(T_idxs)\n",
    "    zcbs = df['zcb'].values\n",
    "\n",
    "    n_steps, n_forwards = f_sim.shape\n",
    "    n_payments = int(tenor / tau)\n",
    "    swap_len = n_payments  # number of forward rates needed\n",
    "\n",
    "    # Compute max usable T_idx based on number of forward rates\n",
    "    max_T_idx = n_forwards - swap_len * resolution\n",
    "    T_idxs = T_idxs#[T_idxs <= max_T_idx]\n",
    "    if len(T_idxs) == 0:\n",
    "        raise ValueError(\"No valid T_idxs remain after filtering based on swap length and resolution.\")\n",
    "\n",
    "    # Compute all valid time steps\n",
    "    t_end = n_steps - swap_len * resolution\n",
    "    valid_steps = np.arange(0, t_end)\n",
    "\n",
    "    # Compute forward rate indices for each swap\n",
    "    forward_offsets = np.arange(n_payments) * resolution\n",
    "    col_indices = T_idxs[:, None] + forward_offsets[None, :]\n",
    "    # Check bounds\n",
    "    if np.any(col_indices >= n_forwards):\n",
    "        raise IndexError(\"Computed forward indices exceed available f_sim columns.\")\n",
    "\n",
    "    # Compute ZCB indices needed for annuity weights\n",
    "    zcb_offsets = np.arange(1,n_payments+1) * resolution\n",
    "    zcb_indices = T_idxs[:, None] + zcb_offsets[None, :]\n",
    "    if np.any(zcb_indices >= len(zcbs)):\n",
    "        raise IndexError(\"Computed ZCB indices exceed available zcb entries.\")\n",
    "    \n",
    "\n",
    "    # Compute frozen swap weights\n",
    "    zcb_sets = np.stack([zcbs[idxs] for idxs in zcb_indices])\n",
    "    swap_weights = zcb_sets * tau\n",
    "    swap_weights = swap_weights / swap_weights.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # Gather simulated forward rate and volatility slices\n",
    "    f_curves = f_sim[valid_steps]\n",
    "    fwd_subsets = np.stack([f_curves[:, idxs] for idxs in col_indices], axis=1)\n",
    "\n",
    "    #print(fwd_subsets.shape)\n",
    "    # Compute weighted sum (dot product): (n_valid_steps, n_swaps)\n",
    "    swap_paths = np.einsum('tsp,sp->ts', fwd_subsets, swap_weights)\n",
    "    swap_paths[np.triu_indices_from(swap_paths, k=int(expiry*resolution/tau+1))] = np.nan  # Set upper triangle to NaN\n",
    "\n",
    "    # Compute W: (n_valid_steps, n_swaps, n_payments)\n",
    "    swaps_expanded = swap_paths[:, :, None]  # (n_valid_steps, n_swaps, 1)\n",
    "    W = swap_weights[None, :, :] * (fwd_subsets**beta) / (swaps_expanded**B)  # default: beta=0.5, B=0.5\n",
    "    #print(swap_weights)\n",
    "    #W = np.nan_to_num(W)\n",
    "    return swap_paths, W\n",
    "\n",
    "\n",
    "def make_swap_indexer(n_steps, T_idxs, resolution, tau, tenor, return_indices=False):\n",
    "    \"\"\"\n",
    "    this is a function factory to create an indexer that given a matrix of similar structure to f_sim\n",
    "    will return an indexer function that takes a matrix of simulated values connected to a set of values relevant to the swap\n",
    "\n",
    "    Parameters:\n",
    "    - n_steps: int\n",
    "        Number of simulation steps.\n",
    "    - T_idxs: np.ndarray\n",
    "        Array of forward indices where swaps start (e.g. expiries).\n",
    "    - resolution: int\n",
    "        Number of simulation steps per accrual period (tau).\n",
    "    - tau: float\n",
    "        Accrual period of the swap (e.g., 0.5 for semiannual).\n",
    "    - tenor: float\n",
    "        Total length of the swap in years (e.g., 1.0 for a 1y swap).\n",
    "    - return_indices: bool, optional\n",
    "        If True, return the valid steps and column indices used for indexing.\n",
    "    Returns:\n",
    "    - indexer: callable\n",
    "        Function to index into the matrix of simulated forward rates.\n",
    "    - (valid_steps, col_indices): tuple of np.ndarray\n",
    "        If return_indices is True, returns the valid steps and column indices used for indexing.\n",
    "    \"\"\"\n",
    "    n_payments = int(tenor / tau)\n",
    "    swap_len = n_payments\n",
    "    t_end = n_steps - swap_len * resolution\n",
    "    valid_steps = np.arange(0, t_end)\n",
    "    offsets = np.arange(n_payments) * resolution\n",
    "    col_indices = T_idxs[:, None] + offsets[None, :]\n",
    "    col_indices = np.broadcast_to(col_indices, (len(valid_steps), *col_indices.shape))\n",
    "\n",
    "    def indexer(mat):\n",
    "        mat_short = mat[valid_steps]\n",
    "        mat_short = mat_short[:, None, :]\n",
    "        return np.take_along_axis(mat_short, col_indices, axis=2) # shape (n_valid_steps, n_swaps, n_payments)\n",
    "    if return_indices:\n",
    "        return indexer, (valid_steps, col_indices)\n",
    "    return indexer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_swap_correlation_tensor(rho, T_idxs, resolution, tau, tenor, n_valid_steps=None):\n",
    "    \"\"\"\n",
    "    Build a (n_valid_steps, n_swaps, n_payments, n_payments) tensor of forward correlations for each swap.\n",
    "\n",
    "    Parameters:\n",
    "    - rho: np.ndarray, shape (n_forwards, n_forwards)\n",
    "        Correlation matrix between forward rates.\n",
    "    - T_idxs: np.ndarray, shape (n_swaps,)\n",
    "        Start indices for each swap.\n",
    "    - resolution: int\n",
    "        Number of forward steps per tau.\n",
    "    - tau: float\n",
    "        Accrual period (in years).\n",
    "    - tenor: float\n",
    "        Swap tenor (in years).\n",
    "    - n_valid_steps: int\n",
    "        Number of time steps to tile over.\n",
    "\n",
    "    Returns:\n",
    "    - rho_tensor: np.ndarray, shape (n_valid_steps, n_swaps, n_payments, n_payments)\n",
    "    \"\"\"\n",
    "    if not n_valid_steps:\n",
    "        n_valid_steps = len(T_idxs)\n",
    "    n_swaps = len(T_idxs)\n",
    "    n_payments = int(tenor / tau)\n",
    "    rho_subs = np.empty((n_swaps, n_payments, n_payments))\n",
    "\n",
    "    for i, T_idx in enumerate(T_idxs):\n",
    "        indices = T_idx + np.arange(n_payments) * resolution\n",
    "        rho_subs[i] = rho[np.ix_(indices, indices)]\n",
    "\n",
    "    # Tile over time steps\n",
    "    rho_tensor = np.tile(rho_subs[None, :, :, :], (n_valid_steps, 1, 1, 1))\n",
    "    assert rho_tensor.shape[0] == rho_tensor.shape[1], f\"Shape mismatch: {rho_tensor.shape[0]} != {rho_tensor.shape[1]}\"\n",
    "    #print(f\"rho_tensor shape: {rho_tensor.shape}\")\n",
    "    return rho_tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batched stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_swap_correlation_tensor_batched(rho, T_idxs, resolution, tau, tenor, n_valid_steps=None):\n",
    "    \"\"\"\n",
    "    Batched version of build_swap_correlation_tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - rho: np.ndarray, shape (S, F, F)\n",
    "        Correlation matrices for each simulation.\n",
    "    - T_idxs: np.ndarray, shape (n_swaps,)\n",
    "        Forward start indices of each swap.\n",
    "    - resolution: int\n",
    "        Steps per tau.\n",
    "    - tau: float\n",
    "        Accrual period.\n",
    "    - tenor: float\n",
    "        Swap length in years.\n",
    "    - n_valid_steps: int, optional\n",
    "        Number of valid simulation steps.\n",
    "\n",
    "    Returns:\n",
    "    - rho_tensor: np.ndarray, shape (S, n_valid_steps, n_swaps, n_payments, n_payments)\n",
    "    \"\"\"\n",
    "    S, F, _ = rho.shape\n",
    "    n_swaps = len(T_idxs)\n",
    "    n_payments = int(tenor / tau)\n",
    "\n",
    "    if n_valid_steps is None:\n",
    "        n_valid_steps = len(T_idxs)\n",
    "\n",
    "    rho_subs = np.empty((S, n_swaps, n_payments, n_payments))\n",
    "\n",
    "    for j, T_idx in enumerate(T_idxs):\n",
    "        indices = T_idx + np.arange(n_payments) * resolution  # (n_payments,)\n",
    "        # Batched slice: (S, n_payments, n_payments)\n",
    "        rho_subs[:, j] = np.take(rho, indices[:, None], axis=1)[..., indices]\n",
    "\n",
    "    # Now tile over valid steps: (S, n_valid_steps, n_swaps, n_payments, n_payments)\n",
    "    rho_tensor = np.tile(rho_subs[:, None, :, :, :], (1, n_valid_steps, 1, 1, 1))\n",
    "\n",
    "    return rho_tensor\n",
    "\n",
    "\n",
    "def make_swap_indexer_batched(n_steps, T_idxs, resolution, tau, tenor, return_indices=False):\n",
    "    \"\"\"\n",
    "    Batched version of swap indexer — works on input shaped (n_sims, n_steps, n_forwards).\n",
    "\n",
    "    Returns an indexer that extracts the floating leg reset values for all simulations.\n",
    "    \"\"\"\n",
    "    n_payments = int(tenor / tau)\n",
    "    swap_len = n_payments\n",
    "    t_end = n_steps - swap_len * resolution\n",
    "    valid_steps = np.arange(0, t_end)\n",
    "\n",
    "    # (n_swaps, n_payments)\n",
    "    offsets = np.arange(n_payments) * resolution\n",
    "    col_indices = T_idxs[:, None] + offsets[None, :]\n",
    "    \n",
    "    # (n_valid_steps, n_swaps, n_payments)\n",
    "    col_indices = np.broadcast_to(col_indices, (len(valid_steps), *col_indices.shape))\n",
    "\n",
    "    def indexer(mat):\n",
    "        \"\"\"\n",
    "        mat: np.ndarray of shape (n_sims, n_steps, n_forwards)\n",
    "        returns: (n_sims, n_valid_steps, n_swaps, n_payments)\n",
    "        \"\"\"\n",
    "        n_sims = mat.shape[0]\n",
    "\n",
    "        # (n_valid_steps, n_steps) → pick rows from each sim\n",
    "        mat_short = mat[:, valid_steps, :]  # shape: (n_sims, n_valid_steps, n_forwards)\n",
    "\n",
    "        # Expand for swap index structure\n",
    "        # We want: (n_sims, n_valid_steps, n_swaps, n_payments)\n",
    "        # We can do this using np.take_along_axis\n",
    "\n",
    "        # col_indices: (n_valid_steps, n_swaps, n_payments)\n",
    "        # Broadcast to (1, n_valid_steps, n_swaps, n_payments)\n",
    "        col_indices_batched = np.broadcast_to(col_indices, (n_sims, *col_indices.shape))\n",
    "\n",
    "        # mat_short: (n_sims, n_valid_steps, n_forwards) → expand axis\n",
    "        mat_short_expanded = mat_short[:, :, None, :]  # (n_sims, n_valid_steps, 1, n_forwards)\n",
    "\n",
    "        return np.take_along_axis(mat_short_expanded, col_indices_batched, axis=3)\n",
    "\n",
    "    if return_indices:\n",
    "        return indexer, (valid_steps, col_indices)\n",
    "    return indexer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_swap_matrix_batched(f_sim, T_idxs, resolution, tau, tenor, df, expiry=1, beta=0.5, B=0.5):\n",
    "    \"\"\"\n",
    "    Batched version of get_swap_matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - f_sim: np.ndarray, shape (n_sims, n_steps, n_forwards)\n",
    "    - T_idxs: np.ndarray of shape (n_swaps,)\n",
    "    - df: pd.DataFrame with column 'zcb'\n",
    "    Returns:\n",
    "    - swap_paths: (n_sims, n_valid_steps, n_swaps)\n",
    "    - W: (n_sims, n_valid_steps, n_swaps, n_payments)\n",
    "    \"\"\"\n",
    "    f_sim = np.asarray(f_sim)\n",
    "    T_idxs = np.asarray(T_idxs)\n",
    "    zcbs = df['zcb'].values\n",
    "\n",
    "    S, n_steps, n_forwards = f_sim.shape\n",
    "    n_payments = int(tenor / tau)\n",
    "    swap_len = n_payments\n",
    "    max_T_idx = n_forwards - swap_len * resolution\n",
    "\n",
    "    if len(T_idxs) == 0:\n",
    "        raise ValueError(\"No valid T_idxs remain after filtering.\")\n",
    "\n",
    "    t_end = n_steps - swap_len * resolution\n",
    "    valid_steps = np.arange(0, t_end)\n",
    "\n",
    "    # (n_swaps, n_payments)\n",
    "    forward_offsets = np.arange(n_payments) * resolution\n",
    "    col_indices = T_idxs[:, None] + forward_offsets[None, :]\n",
    "\n",
    "    if np.any(col_indices >= n_forwards):\n",
    "        raise IndexError(\"Forward indices exceed f_sim dimension.\")\n",
    "\n",
    "    # ZCB weights\n",
    "    zcb_offsets = np.arange(1, n_payments + 1) * resolution\n",
    "    zcb_indices = T_idxs[:, None] + zcb_offsets[None, :]\n",
    "\n",
    "    if np.any(zcb_indices >= len(zcbs)):\n",
    "        raise IndexError(\"ZCB indices exceed available entries.\")\n",
    "\n",
    "    # Frozen swap weights: (n_swaps, n_payments)\n",
    "    zcb_sets = np.stack([zcbs[idxs] for idxs in zcb_indices])\n",
    "    swap_weights = zcb_sets * tau\n",
    "    swap_weights = swap_weights / swap_weights.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # Grab forward subsets\n",
    "    f_curves = f_sim[:, valid_steps, :]  # (S, T', F)\n",
    "    fwd_subsets = np.stack(\n",
    "        [np.take(f_curves, idxs, axis=2) for idxs in col_indices], axis=2\n",
    "    )  # shape: (S, T', n_swaps, n_payments)\n",
    "\n",
    "    # Compute weighted swap rates: einsum over payments axis\n",
    "    swap_paths = np.einsum(\"stnp,sp->stn\", fwd_subsets, swap_weights)\n",
    "\n",
    "    # Mask future expiry values\n",
    "    expiry_cutoff = int(expiry * resolution / tau) + 1\n",
    "    for i in range(swap_paths.shape[1]):\n",
    "        if i > expiry_cutoff:\n",
    "            swap_paths[:, i, :] = np.nan\n",
    "\n",
    "    # Compute W\n",
    "    swaps_expanded = swap_paths[..., None]  # (S, T', n_swaps, 1)\n",
    "    weights_expanded = swap_weights[None, None, :, :]  # (1, 1, n_swaps, n_payments)\n",
    "    W = weights_expanded * (fwd_subsets ** beta) / (swaps_expanded ** B)  # shape: (S, T', n_swaps, n_payments)\n",
    "\n",
    "    return swap_paths, W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOOD FOR DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[199], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Rewriting the function with corrected forward index logic (based on tenor, tau, resolution)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Parameters from user-provided setup\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mtau \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolution \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      7\u001b[0m tenor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# Rewriting the function with corrected forward index logic (based on tenor, tau, resolution)\n",
    "\n",
    "\n",
    "# Parameters from user-provided setup\n",
    "self.tau = 0.5\n",
    "self.resolution = 2\n",
    "tenor = 1\n",
    "\n",
    "times = np.arange(0, 8.0001, self.tau / self.resolution)\n",
    "#print(times.shape)\n",
    "n_steps = len(times)\n",
    "f_sim = np.tile(np.linspace(0.01, 0.04, n_steps), (n_steps, 1))\n",
    "f_sim[np.tril_indices_from(f_sim, k=-1)] = np.nan\n",
    "df = pd.DataFrame({'zcb': np.exp(-0.02 * times)})\n",
    "T_idxs = np.arange(10)\n",
    "\n",
    "\n",
    "# Run the final version\n",
    "swap_paths, W = get_swap_matrix(\n",
    "    f_sim, T_idxs, self.resolution, self.tau, tenor, df, beta=0.5, B=0.5\n",
    ")\n",
    "#print(swap_paths.shape)\n",
    "# Display result\n",
    "col_names = [f\"Swap_{times[i]:.2f}y\" for i in T_idxs]\n",
    "df_result = pd.DataFrame(swap_paths, index=np.arange(len(swap_paths)), columns=col_names)\n",
    "df_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class LMMSABR():\n",
    "#     def __init__(self,df_raw_spot, df_fwd, rho_mat, theta_mat, phi_mat, g_params, h_params, volvol_0_exp= None, k0_exp =None, tau=0.5, tenor=1, resolution=2, max_expiry=9.5):\n",
    "#         self.rho_mat = rho_mat\n",
    "#         self.theta_mat = theta_mat\n",
    "#         self.phi_mat = phi_mat\n",
    "#         self.g = partial(get_instant_vol_func, params=g_params)\n",
    "#         self.h = partial(get_instant_vol_func, params=h_params)\n",
    "#         self.epsilon_exp = volvol_0_exp\n",
    "#         self.k0_exp = k0_exp\n",
    "#         self.df_raw_spot = df_raw_spot\n",
    "#         self.df_fwd = df_fwd\n",
    "#         self.max_expiry = max_expiry\n",
    "        \n",
    "#         self.tau = tau\n",
    "#         self.tenor = tenor\n",
    "#         self.resolution = resolution\n",
    "#         self.dt = self.tau / self.resolution\n",
    "#         self.dt_sqrt = math.sqrt(self.dt)\n",
    "    \n",
    "#     def _prepare_initial_conditions(self):\n",
    "#         self.df_init = create_df_init(self.df_fwd, self.df_raw_spot, resolution=self.resolution, tau=self.tau).query(f\"Tenor <= {self.max_expiry+1e-6}\")\n",
    "#         self.ids_fwd_canon = self.df_init['Forward'].dropna().index.values\n",
    "        \n",
    "#         self.num_forwards = self.df_init['Forward'].dropna().shape[0]\n",
    "#         self.t_arr = self.df_init['Tenor'].values\n",
    "        \n",
    "#         # f_sim dimensions\n",
    "#         self.f_0 = self.df_init['Forward'].values\n",
    "#         self.n_steps = len(self.t_arr)\n",
    "\n",
    "#         self.ttm_mat = self.t_arr[None,:]-self.t_arr[:,None] # each row is a time increment, and each column represents the time to maturity of a forward rate\n",
    "#         # precompute h and g\n",
    "#         self.h_mat = self.h(self.ttm_mat[1:, self.ids_fwd_canon])\n",
    "#         self.g_mat = self.g(self.ttm_mat[:, self.ids_fwd_canon])\n",
    "\n",
    "#     def _create_interp_funcs(self):\n",
    "#         self.interp_func, self.interp_vol_func = interp_func_fac(\n",
    "#             self.df_init,\n",
    "#             resolution=self.resolution,\n",
    "#             tau=self.tau,\n",
    "#             rho_mat=self.rho_mat,\n",
    "#             g_func=self.g,\n",
    "#             interp_vol=True,\n",
    "#         )\n",
    "\n",
    "#     def _precompute_volatility_matrix(self):\n",
    "#         pass\n",
    "\n",
    "#     def simulate(self, tenor=1):\n",
    "#         self._prepare_initial_conditions()\n",
    "#         self._create_interp_funcs()\n",
    "\n",
    "#         dW_s = np.random.multivariate_normal(np.zeros(self.num_forwards), self.theta_mat[:self.num_forwards,:self.num_forwards], self.n_steps-1) * self.dt_sqrt\n",
    "#         k_mat = np.concatenate([self.k0_exp[:self.num_forwards].reshape(1,-1), (self.k0_exp[:self.num_forwards] * np.cumprod(1 + self.epsilon_exp[:self.num_forwards].reshape(1,-1) * dW_s * self.h_mat, axis=0))])\n",
    "#         s_mat = self.g_mat * k_mat\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "#         f_sim = np.zeros(self.ttm_mat.shape)*np.nan # each simulation, we must have a spot rate\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "#         # precompute random draws\n",
    "#         dZ_f = np.random.multivariate_normal(np.zeros(self.num_forwards), self.rho_mat[:self.num_forwards,:self.num_forwards], self.n_steps-1) * self.dt_sqrt\n",
    "\n",
    "\n",
    "#         self.s_mat = s_mat\n",
    "\n",
    "        \n",
    "#         k_mat_full_res = np.zeros((self.n_steps, self.n_steps))*np.nan\n",
    "#         s_mat_full_res = np.zeros((self.n_steps, self.n_steps))*np.nan\n",
    "        \n",
    "#         s_mat_full_res[:, self.ids_fwd_canon] = k_mat * self.g_mat\n",
    "\n",
    "#         for i in range(self.n_steps):\n",
    "#             k_mat_full_res[i,:-self.resolution] = self.interp_vol_func(s_mat_full_res[i], t_idx=i)\n",
    "#             if i >= self.resolution:\n",
    "#                 k_mat_full_res[i, :(i-self.resolution)] = np.nan\n",
    "        \n",
    "#         self.k_mat_full_res = k_mat_full_res\n",
    "#         self.k_mat = k_mat\n",
    "\n",
    "#         drift_correction = np.zeros(len(f_sim[0]))\n",
    "#         drift_shared = np.zeros(len(f_sim[0]))\n",
    "#         ids_fwd_canon_rev = self.ids_fwd_canon[::-1]\n",
    "\n",
    "\n",
    "        \n",
    "#         ids_fwd_canon_short_rev = ids_fwd_canon_rev//self.resolution\n",
    "#         non_canon_indices = np.setdiff1d(np.arange(len(f_sim[0]))[:-self.resolution], ids_fwd_canon_rev)\n",
    "#         non_canon_idx = non_canon_indices\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "#         f_sim = f_sim * np.nan\n",
    "#         f_sim[0] = self.f_0*4\n",
    "#         f_sim[0, non_canon_idx] = self.interp_func(f_sim[0])[non_canon_idx]\n",
    "\n",
    "#         for t in range(1, self.n_steps-1):\n",
    "#             drift_correction.fill(0)\n",
    "#             drift_shared.fill(0)\n",
    "#             # next loop runs from longest to shortest tenor\n",
    "#             for canon_short_idx, canon_idx in zip(ids_fwd_canon_short_rev, ids_fwd_canon_rev):\n",
    "                \n",
    "#                 if self.ttm_mat[t, canon_idx] +self.tau+1e-8>= 0:     # TODO <------------ THIS IS IMPORTANT\n",
    "#                     s_t, dZ_f_t,  f_t = s_mat[t-1, canon_short_idx], dZ_f[t-1,canon_short_idx], f_sim[t-1,canon_idx]\n",
    "#                     f_beta_t = f_t**0.5\n",
    "                    \n",
    "#                     drift_f = (-self.g_mat[t, canon_short_idx] * k_mat[t, canon_short_idx] * f_beta_t * drift_shared[canon_short_idx])\n",
    "#                     df_t =  drift_f + f_beta_t*s_t*dZ_f_t\n",
    "#                     f_t += df_t\n",
    "                    \n",
    "#                     f_t_new =  f_t + df_t if f_t + df_t > 0 else 0  # zero absorbing boundary, interest rates cannot be negative due to arbitrage\n",
    "#                     f_sim[t,canon_idx] = f_t_new\n",
    "\n",
    "#                     if canon_short_idx > 0:\n",
    "#                         drift_correction[canon_short_idx-1] = self.rho_mat[canon_short_idx-1, canon_short_idx] * self.tau * self.g_mat[t,canon_short_idx] * k_mat[t, canon_short_idx] * f_beta_t / (1 + self.tau * f_t)\n",
    "#                         drift_shared[canon_short_idx-1] = np.sum(drift_correction[canon_short_idx-1:])\n",
    "            \n",
    "\n",
    "                \n",
    "#             f_sim[t, non_canon_idx] = self.interp_func(f_sim[t])[non_canon_idx]\n",
    "\n",
    "\n",
    "            \n",
    "#             self.f_sim = f_sim\n",
    "#         T_idxs = np.arange(len(f_sim)-int(tenor/self.tau*self.resolution))\n",
    "#         swap_sim, W = get_swap_matrix(f_sim, T_idxs=T_idxs, resolution=self.resolution, tau=self.tau, tenor=self.tenor, df=self.df_init)    \n",
    "#         self.W = W\n",
    "#         self.swap_sim = swap_sim\n",
    "#         assert self.k_mat_full_res.shape == self.f_sim.shape, f\"Shape mismatch: {self.k_mat_full_res.shape} != {self.f_sim.shape}\"\n",
    "#         return f_sim, swap_sim, k_mat_full_res\n",
    "    \n",
    "\n",
    "\n",
    "#     def plot(self):\n",
    "#         fig = plt.figure(figsize=(10, 8))\n",
    "#         ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "#         # Create mesh grid for x (time) and y (tenors)\n",
    "#         X = np.arange(self.n_steps)*self.dt\n",
    "#         Y = np.arange(self.n_steps-self.resolution)*self.dt\n",
    "#         X, Y = np.meshgrid(X, Y)\n",
    "        \n",
    "#         # Transpose f_sim to match the grid shape\n",
    "#         Z = self.f_sim[:,:-self.resolution].T\n",
    "\n",
    "#         # Create surface plot\n",
    "#         surf = ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')\n",
    "        \n",
    "#         # Add labels and colorbar\n",
    "#         ax.set_xlabel('Time Steps')\n",
    "#         ax.set_ylabel('Tenors')\n",
    "#         ax.set_zlabel('Forward Rates')\n",
    "#         fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "        \n",
    "#         plt.show()\n",
    "#     def plot_swap(self):\n",
    "#         fig = plt.figure(figsize=(10, 8))\n",
    "#         ax = fig.add_subplot(111, projection='3d')\n",
    "#         # Create mesh grid the shape of self.swap_sim\n",
    "#         X = np.arange(self.swap_sim.shape[0])*self.dt\n",
    "#         Y = np.arange(self.swap_sim.shape[1])\n",
    "#         X, Y = np.meshgrid(X, Y)\n",
    "#         Z = self.swap_sim.T\n",
    "#         # Create surface plot\n",
    "#         surf = ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')\n",
    "#         # Add labels and colorbar\n",
    "#         ax.set_xlabel('Time Steps')\n",
    "#         ax.set_ylabel('Swap Expiry')\n",
    "#         ax.set_zlabel('Swap Rates')\n",
    "#         fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "#         # angle so we look down from above\n",
    "#         ax.view_init(elev=45, azim=210)\n",
    "        \n",
    "#         plt.show()\n",
    "# lmm = LMMSABR(df_raw_spot, df_cap, rho_mat_0m, theta_mat_0m, phi_mat, params_g, params_h, epsilon_exp, k0_exp=s0_exp)\n",
    "# f_sim, swap_sim, k_mat_full_res = lmm.simulate()\n",
    "# print(f_sim)\n",
    "# print(lmm.swap_sim.shape, lmm.f_sim.shape, lmm.k_mat_full_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.    , 0.0106, 0.0105, 0.0105, 0.0105, 0.0106, 0.0106, 0.0107, 0.0108, 0.0109, 0.011 , 0.0111, 0.0113, 0.0114, 0.0116, 0.0118, 0.012 , 0.0122, 0.0124, 0.0126, 0.0128, 0.0131, 0.0133, 0.0136, 0.0138, 0.0141, 0.0144, 0.0144, 0.0144, 0.0144, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0142, 0.0143, 0.0143, 0.0143, 0.0143, 0.0144, 0.0144, 0.0145, 0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.015 , 0.0151, 0.0152, 0.0154, 0.0156],\n",
       "        [   nan, 0.    , 0.0105, 0.0104, 0.0104, 0.0105, 0.0105, 0.0106, 0.0107, 0.0108, 0.0109, 0.0111, 0.0112, 0.0114, 0.0115, 0.0117, 0.0119, 0.0121, 0.0123, 0.0126, 0.0128, 0.013 , 0.0133, 0.0135, 0.0138, 0.0141, 0.0144, 0.0144, 0.0144, 0.0144, 0.0143, 0.0143, 0.0143, 0.0143, 0.0142, 0.0142, 0.0143, 0.0143, 0.0143, 0.0143, 0.0144, 0.0144, 0.0145, 0.0146, 0.0146, 0.0147, 0.0148, 0.0149, 0.0151, 0.0152, 0.0153, 0.0155, 0.0157],\n",
       "        [   nan,    nan, 0.    , 0.0107, 0.0107, 0.0107, 0.0108, 0.0108, 0.0109, 0.011 , 0.0112, 0.0113, 0.0114, 0.0116, 0.0118, 0.0119, 0.0121, 0.0123, 0.0126, 0.0128, 0.013 , 0.0133, 0.0135, 0.0138, 0.014 , 0.0143, 0.0146, 0.0146, 0.0146, 0.0146, 0.0145, 0.0145, 0.0145, 0.0144, 0.0144, 0.0144, 0.0144, 0.0144, 0.0145, 0.0145, 0.0145, 0.0146, 0.0146, 0.0147, 0.0148, 0.0149, 0.015 , 0.0151, 0.0152, 0.0153, 0.0155, 0.0156, 0.0158],\n",
       "        [   nan,    nan,    nan, 0.    , 0.0108, 0.0107, 0.0108, 0.0108, 0.0109, 0.011 , 0.0111, 0.0112, 0.0114, 0.0115, 0.0117, 0.0119, 0.0121, 0.0123, 0.0125, 0.0127, 0.0129, 0.0132, 0.0134, 0.0137, 0.0139, 0.0142, 0.0145, 0.0145, 0.0145, 0.0145, 0.0144, 0.0144, 0.0144, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0144, 0.0144, 0.0144, 0.0145, 0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.015 , 0.0151, 0.0152, 0.0154, 0.0155, 0.0157],\n",
       "        [   nan,    nan,    nan,    nan, 0.    , 0.0103, 0.0103, 0.0103, 0.0104, 0.0105, 0.0105, 0.0107, 0.0108, 0.0109, 0.0111, 0.0112, 0.0114, 0.0116, 0.0118, 0.012 , 0.0122, 0.0124, 0.0126, 0.0129, 0.0131, 0.0134, 0.0137, 0.0137, 0.0137, 0.0137, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0137, 0.0137, 0.0137, 0.0138, 0.0139, 0.0139, 0.014 , 0.0141, 0.0142, 0.0143, 0.0144, 0.0146, 0.0147, 0.0149, 0.015 , 0.0152],\n",
       "        [   nan,    nan,    nan,    nan,    nan, 0.    , 0.0101, 0.0101, 0.0102, 0.0102, 0.0103, 0.0104, 0.0105, 0.0107, 0.0108, 0.011 , 0.0111, 0.0113, 0.0115, 0.0117, 0.0119, 0.0121, 0.0124, 0.0126, 0.0128, 0.0131, 0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0133, 0.0133, 0.0133, 0.0133, 0.0134, 0.0134, 0.0134, 0.0135, 0.0135, 0.0136, 0.0136, 0.0137, 0.0138, 0.0139, 0.014 , 0.0141, 0.0142, 0.0144, 0.0145, 0.0147, 0.0149, 0.015 ],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0104, 0.0104, 0.0105, 0.0106, 0.0107, 0.0108, 0.0109, 0.011 , 0.0112, 0.0114, 0.0115, 0.0117, 0.0119, 0.0122, 0.0124, 0.0126, 0.0128, 0.0131, 0.0133, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0137, 0.0137, 0.0138, 0.0138, 0.0139, 0.014 , 0.0141, 0.0142, 0.0143, 0.0144, 0.0145, 0.0147, 0.0148, 0.015 , 0.0152],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0107, 0.0107, 0.0108, 0.0108, 0.0109, 0.0111, 0.0112, 0.0113, 0.0115, 0.0117, 0.0119, 0.0121, 0.0123, 0.0125, 0.0127, 0.013 , 0.0132, 0.0135, 0.0137, 0.0137, 0.0137, 0.0137, 0.0137, 0.0137, 0.0136, 0.0136, 0.0136, 0.0136, 0.0137, 0.0137, 0.0137, 0.0138, 0.0138, 0.0139, 0.014 , 0.014 , 0.0141, 0.0142, 0.0143, 0.0145, 0.0146, 0.0147, 0.0149, 0.0151, 0.0152],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0113, 0.0114, 0.0114, 0.0115, 0.0116, 0.0118, 0.0119, 0.0121, 0.0122, 0.0124, 0.0126, 0.0129, 0.0131, 0.0133, 0.0136, 0.0138, 0.0141, 0.0144, 0.0144, 0.0144, 0.0143, 0.0143, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0143, 0.0143, 0.0144, 0.0144, 0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.015 , 0.0152, 0.0153, 0.0155, 0.0156],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.011 , 0.0111, 0.0111, 0.0112, 0.0113, 0.0115, 0.0116, 0.0118, 0.0119, 0.0121, 0.0123, 0.0125, 0.0128, 0.013 , 0.0132, 0.0135, 0.0137, 0.0138, 0.0137, 0.0137, 0.0137, 0.0137, 0.0137, 0.0136, 0.0136, 0.0137, 0.0137, 0.0137, 0.0137, 0.0138, 0.0138, 0.0139, 0.014 , 0.0141, 0.0142, 0.0143, 0.0144, 0.0145, 0.0146, 0.0148, 0.0149, 0.0151, 0.0153],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0113, 0.0113, 0.0114, 0.0115, 0.0116, 0.0118, 0.0119, 0.0121, 0.0123, 0.0125, 0.0127, 0.0129, 0.0131, 0.0133, 0.0136, 0.0138, 0.0139, 0.0139, 0.0138, 0.0138, 0.0138, 0.0137, 0.0137, 0.0137, 0.0138, 0.0138, 0.0138, 0.0138, 0.0139, 0.0139, 0.014 , 0.0141, 0.0141, 0.0142, 0.0143, 0.0145, 0.0146, 0.0147, 0.0149, 0.015 , 0.0152, 0.0154],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0114, 0.0115, 0.0116, 0.0117, 0.0118, 0.0119, 0.0121, 0.0123, 0.0125, 0.0127, 0.0129, 0.0131, 0.0134, 0.0136, 0.0139, 0.0139, 0.0139, 0.0138, 0.0138, 0.0138, 0.0138, 0.0138, 0.0138, 0.0138, 0.0138, 0.0138, 0.0139, 0.0139, 0.014 , 0.014 , 0.0141, 0.0142, 0.0143, 0.0144, 0.0145, 0.0146, 0.0148, 0.0149, 0.0151, 0.0152, 0.0154],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0114, 0.0114, 0.0115, 0.0116, 0.0118, 0.0119, 0.0121, 0.0123, 0.0124, 0.0127, 0.0129, 0.0131, 0.0133, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0135, 0.0135, 0.0135, 0.0135, 0.0135, 0.0136, 0.0136, 0.0136, 0.0137, 0.0137, 0.0138, 0.0139, 0.014 , 0.0141, 0.0142, 0.0143, 0.0144, 0.0146, 0.0147, 0.0149, 0.0151, 0.0153],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0116, 0.0117, 0.0118, 0.0119, 0.012 , 0.0122, 0.0123, 0.0125, 0.0127, 0.0129, 0.0131, 0.0134, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0135, 0.0135, 0.0135, 0.0135, 0.0135, 0.0136, 0.0136, 0.0136, 0.0137, 0.0137, 0.0138, 0.0139, 0.014 , 0.0141, 0.0142, 0.0143, 0.0144, 0.0146, 0.0147, 0.0149, 0.0151, 0.0153],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0118, 0.0118, 0.0119, 0.012 , 0.0122, 0.0123, 0.0125, 0.0127, 0.0129, 0.0132, 0.0134, 0.0136, 0.0136, 0.0136, 0.0136, 0.0136, 0.0135, 0.0135, 0.0135, 0.0135, 0.0135, 0.0136, 0.0136, 0.0136, 0.0137, 0.0137, 0.0138, 0.0139, 0.014 , 0.0141, 0.0142, 0.0143, 0.0144, 0.0146, 0.0147, 0.0149, 0.0151, 0.0153],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0112, 0.0112, 0.0113, 0.0114, 0.0116, 0.0117, 0.0119, 0.0121, 0.0123, 0.0125, 0.0128, 0.0128, 0.0128, 0.0128, 0.0127, 0.0127, 0.0127, 0.0127, 0.0128, 0.0128, 0.0128, 0.0129, 0.0129, 0.013 , 0.0131, 0.0131, 0.0132, 0.0133, 0.0134, 0.0136, 0.0137, 0.0138, 0.014 , 0.0141, 0.0143, 0.0145, 0.0147],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0107, 0.0107, 0.0108, 0.0109, 0.0111, 0.0112, 0.0114, 0.0116, 0.0118, 0.012 , 0.012 , 0.012 , 0.012 , 0.012 , 0.012 , 0.012 , 0.012 , 0.0121, 0.0121, 0.0121, 0.0122, 0.0123, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0129, 0.013 , 0.0131, 0.0133, 0.0134, 0.0136, 0.0138, 0.014 , 0.0142],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0113, 0.0114, 0.0115, 0.0116, 0.0118, 0.012 , 0.0121, 0.0123, 0.0126, 0.0126, 0.0126, 0.0126, 0.0126, 0.0126, 0.0126, 0.0126, 0.0126, 0.0127, 0.0127, 0.0128, 0.0128, 0.0129, 0.013 , 0.0131, 0.0132, 0.0133, 0.0134, 0.0136, 0.0137, 0.0138, 0.014 , 0.0142, 0.0144, 0.0146, 0.0148],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0105, 0.0106, 0.0107, 0.0108, 0.0109, 0.0111, 0.0113, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0116, 0.0116, 0.0117, 0.0117, 0.0118, 0.0119, 0.012 , 0.012 , 0.0121, 0.0123, 0.0124, 0.0125, 0.0127, 0.0128, 0.013 , 0.0131, 0.0133, 0.0135, 0.0137, 0.0139],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0109, 0.011 , 0.0111, 0.0112, 0.0114, 0.0116, 0.0117, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0119, 0.0119, 0.012 , 0.012 , 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0128, 0.0129, 0.013 , 0.0132, 0.0134, 0.0136, 0.0137, 0.0139, 0.0142, 0.0144],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0114, 0.0115, 0.0116, 0.0118, 0.0119, 0.0121, 0.0121, 0.0122, 0.0122, 0.0122, 0.0122, 0.0122, 0.0122, 0.0123, 0.0124, 0.0124, 0.0125, 0.0126, 0.0127, 0.0128, 0.0129, 0.013 , 0.0132, 0.0133, 0.0135, 0.0136, 0.0138, 0.014 , 0.0142, 0.0144, 0.0146, 0.0149],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0115, 0.0116, 0.0117, 0.0118, 0.012 , 0.012 , 0.012 , 0.0121, 0.0121, 0.0121, 0.0121, 0.0121, 0.0122, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0128, 0.0129, 0.013 , 0.0132, 0.0133, 0.0135, 0.0137, 0.0139, 0.0141, 0.0143, 0.0145, 0.0148],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0118, 0.012 , 0.0121, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0124, 0.0125, 0.0125, 0.0126, 0.0127, 0.0128, 0.0129, 0.013 , 0.0131, 0.0133, 0.0134, 0.0136, 0.0138, 0.014 , 0.0142, 0.0144, 0.0146, 0.0148, 0.0151],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0124, 0.0126, 0.0127, 0.0127, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0129, 0.013 , 0.013 , 0.0131, 0.0132, 0.0133, 0.0134, 0.0136, 0.0137, 0.0139, 0.014 , 0.0142, 0.0144, 0.0146, 0.0148, 0.015 , 0.0153, 0.0155, 0.0158],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0123, 0.0124, 0.0124, 0.0125, 0.0125, 0.0124, 0.0125, 0.0125, 0.0125, 0.0126, 0.0126, 0.0127, 0.0128, 0.0129, 0.013 , 0.0131, 0.0133, 0.0134, 0.0136, 0.0137, 0.0139, 0.0141, 0.0143, 0.0145, 0.0147, 0.0149, 0.0152, 0.0154],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0124, 0.0124, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0126, 0.0127, 0.0128, 0.0129, 0.013 , 0.0131, 0.0132, 0.0133, 0.0135, 0.0136, 0.0138, 0.014 , 0.0142, 0.0144, 0.0146, 0.0148, 0.0151, 0.0153, 0.0156],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0122, 0.0122, 0.0122, 0.0122, 0.0122, 0.0122, 0.0123, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0128, 0.013 , 0.0131, 0.0133, 0.0134, 0.0136, 0.0138, 0.014 , 0.0142, 0.0144, 0.0147, 0.0149, 0.0152, 0.0154],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0122, 0.0122, 0.0122, 0.0122, 0.0122, 0.0123, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0128, 0.0129, 0.0131, 0.0132, 0.0134, 0.0136, 0.0138, 0.014 , 0.0142, 0.0144, 0.0146, 0.0149, 0.0151, 0.0154],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0113, 0.0113, 0.0112, 0.0112, 0.0113, 0.0113, 0.0114, 0.0114, 0.0115, 0.0116, 0.0117, 0.0119, 0.012 , 0.0121, 0.0123, 0.0124, 0.0126, 0.0128, 0.013 , 0.0132, 0.0134, 0.0136, 0.0139, 0.0141],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0109, 0.0108, 0.0108, 0.0108, 0.0108, 0.0109, 0.0109, 0.011 , 0.0111, 0.0112, 0.0113, 0.0114, 0.0116, 0.0117, 0.0119, 0.012 , 0.0122, 0.0124, 0.0126, 0.0128, 0.013 , 0.0133, 0.0135],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0107, 0.0107, 0.0107, 0.0107, 0.0107, 0.0108, 0.0108, 0.0109, 0.011 , 0.0111, 0.0112, 0.0113, 0.0115, 0.0116, 0.0118, 0.0119, 0.0121, 0.0123, 0.0125, 0.0127, 0.0129, 0.0132],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0104, 0.0103, 0.0103, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106, 0.0107, 0.0108, 0.0109, 0.011 , 0.0112, 0.0113, 0.0115, 0.0116, 0.0118, 0.012 , 0.0122, 0.0124, 0.0127],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0106, 0.0106, 0.0105, 0.0106, 0.0106, 0.0107, 0.0107, 0.0108, 0.0109, 0.011 , 0.0112, 0.0113, 0.0115, 0.0116, 0.0118, 0.012 , 0.0122, 0.0124, 0.0126, 0.0128],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0105, 0.0104, 0.0104, 0.0105, 0.0105, 0.0106, 0.0106, 0.0107, 0.0108, 0.011 , 0.0111, 0.0112, 0.0114, 0.0116, 0.0117, 0.0119, 0.0121, 0.0123, 0.0126],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0098, 0.0097, 0.0097, 0.0097, 0.0098, 0.0098, 0.0099, 0.01  , 0.0101, 0.0102, 0.0103, 0.0105, 0.0106, 0.0108, 0.0109, 0.0111, 0.0113, 0.0115],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0098, 0.0097, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099, 0.01  , 0.0101, 0.0103, 0.0104, 0.0105, 0.0107, 0.0108, 0.011 , 0.0112, 0.0114],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0104, 0.0104, 0.0104, 0.0104, 0.0105, 0.0106, 0.0107, 0.0108, 0.0109, 0.011 , 0.0112, 0.0113, 0.0115, 0.0117, 0.0119, 0.0121],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0103, 0.0102, 0.0103, 0.0103, 0.0104, 0.0105, 0.0106, 0.0107, 0.0108, 0.0109, 0.0111, 0.0113, 0.0115, 0.0117, 0.0119],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0103, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106, 0.0107, 0.0108, 0.0109, 0.0111, 0.0112, 0.0114, 0.0116, 0.0118],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0102, 0.0102, 0.0102, 0.0103, 0.0103, 0.0104, 0.0105, 0.0107, 0.0108, 0.011 , 0.0111, 0.0113, 0.0115],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0107, 0.0106, 0.0107, 0.0107, 0.0108, 0.0109, 0.0111, 0.0112, 0.0114, 0.0115, 0.0117, 0.0119],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.011 , 0.011 , 0.011 , 0.0111, 0.0112, 0.0113, 0.0115, 0.0116, 0.0118, 0.012 , 0.0122],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0117, 0.0117, 0.0118, 0.0119, 0.012 , 0.0121, 0.0123, 0.0124, 0.0126, 0.0128],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0117, 0.0117, 0.0118, 0.0118, 0.012 , 0.0121, 0.0123, 0.0124, 0.0126],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0115, 0.0115, 0.0116, 0.0117, 0.0118, 0.0119, 0.0121, 0.0123],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0124, 0.0124, 0.0125, 0.0127, 0.0128, 0.013 , 0.0132],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0126, 0.0127, 0.0128, 0.0129, 0.0131, 0.0133],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0113, 0.0113, 0.0114, 0.0115, 0.0116],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0111, 0.0111, 0.0112, 0.0113],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0104, 0.0105, 0.0106],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0099, 0.0099],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.01  ],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    ]]),\n",
       " array([[0.    , 0.2692, 0.2736, 0.2759, 0.2773, 0.2782, 0.2786, 0.2787, 0.2786, 0.2782, 0.2775, 0.2767, 0.2758, 0.2746, 0.2733, 0.2719, 0.2704, 0.2688, 0.267 , 0.2652, 0.2633, 0.2613, 0.2592, 0.2571, 0.255 , 0.2528, 0.2505, 0.2479, 0.245 , 0.2421, 0.2392, 0.2364, 0.2336, 0.2307, 0.2279, 0.2252, 0.2224, 0.2196, 0.2169, 0.2142, 0.2115, 0.2089, 0.2062, 0.2036, 0.201 , 0.1984, 0.1959, 0.1933, 0.1908, 0.1883, 0.1858, 0.1833, 0.1809],\n",
       "        [   nan, 0.    , 0.265 , 0.2698, 0.2729, 0.2749, 0.2763, 0.2772, 0.2777, 0.2779, 0.2777, 0.2772, 0.2766, 0.2757, 0.2746, 0.2733, 0.2719, 0.2704, 0.2687, 0.2669, 0.2649, 0.2629, 0.2608, 0.2587, 0.2565, 0.2542, 0.2519, 0.2493, 0.2465, 0.2436, 0.2409, 0.2381, 0.2354, 0.2326, 0.2299, 0.2272, 0.2245, 0.2218, 0.2191, 0.2165, 0.2138, 0.2112, 0.2086, 0.2061, 0.2035, 0.2009, 0.1984, 0.1959, 0.1934, 0.1909, 0.1884, 0.186 , 0.1835],\n",
       "        [   nan,    nan, 0.    , 0.2629, 0.268 , 0.2713, 0.2736, 0.2753, 0.2764, 0.277 , 0.2773, 0.2773, 0.2769, 0.2764, 0.2756, 0.2746, 0.2734, 0.272 , 0.2705, 0.2689, 0.2671, 0.2652, 0.2632, 0.2611, 0.259 , 0.2567, 0.2545, 0.252 , 0.2492, 0.2464, 0.2437, 0.241 , 0.2382, 0.2355, 0.2328, 0.2301, 0.2274, 0.2247, 0.222 , 0.2193, 0.2167, 0.2141, 0.2114, 0.2088, 0.2063, 0.2037, 0.2011, 0.1986, 0.196 , 0.1935, 0.191 , 0.1885, 0.186 ],\n",
       "        [   nan,    nan,    nan, 0.    , 0.2584, 0.2643, 0.2682, 0.271 , 0.2731, 0.2746, 0.2756, 0.2762, 0.2764, 0.2764, 0.276 , 0.2754, 0.2745, 0.2735, 0.2722, 0.2708, 0.2692, 0.2675, 0.2657, 0.2637, 0.2617, 0.2596, 0.2574, 0.2549, 0.2522, 0.2495, 0.2468, 0.244 , 0.2413, 0.2386, 0.2358, 0.2331, 0.2303, 0.2276, 0.2249, 0.2222, 0.2195, 0.2168, 0.2141, 0.2114, 0.2088, 0.2061, 0.2035, 0.2009, 0.1983, 0.1957, 0.1931, 0.1905, 0.188 ],\n",
       "        [   nan,    nan,    nan,    nan, 0.    , 0.2541, 0.2608, 0.2654, 0.2688, 0.2714, 0.2733, 0.2747, 0.2755, 0.276 , 0.276 , 0.2758, 0.2753, 0.2745, 0.2735, 0.2723, 0.2709, 0.2693, 0.2676, 0.2658, 0.2639, 0.2618, 0.2597, 0.2573, 0.2547, 0.252 , 0.2494, 0.2467, 0.244 , 0.2412, 0.2385, 0.2358, 0.233 , 0.2303, 0.2275, 0.2248, 0.2221, 0.2194, 0.2166, 0.2139, 0.2112, 0.2085, 0.2059, 0.2032, 0.2005, 0.1979, 0.1953, 0.1926, 0.19  ],\n",
       "        [   nan,    nan,    nan,    nan,    nan, 0.    , 0.2503, 0.2579, 0.2632, 0.2672, 0.2702, 0.2725, 0.2741, 0.2751, 0.2757, 0.2759, 0.2758, 0.2753, 0.2745, 0.2735, 0.2722, 0.2708, 0.2692, 0.2674, 0.2655, 0.2635, 0.2614, 0.2591, 0.2566, 0.2539, 0.2514, 0.2488, 0.2461, 0.2435, 0.2408, 0.2381, 0.2354, 0.2327, 0.23  , 0.2273, 0.2246, 0.222 , 0.2193, 0.2166, 0.2139, 0.2112, 0.2086, 0.2059, 0.2033, 0.2006, 0.198 , 0.1954, 0.1927],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2486, 0.2565, 0.2621, 0.2663, 0.2695, 0.2719, 0.2736, 0.2748, 0.2755, 0.2758, 0.2757, 0.2752, 0.2745, 0.2735, 0.2723, 0.2709, 0.2693, 0.2675, 0.2656, 0.2636, 0.2614, 0.259 , 0.2564, 0.2539, 0.2514, 0.2488, 0.2462, 0.2436, 0.2409, 0.2382, 0.2356, 0.2329, 0.2302, 0.2275, 0.2248, 0.2221, 0.2194, 0.2167, 0.214 , 0.2114, 0.2087, 0.206 , 0.2033, 0.2007, 0.198 , 0.1954],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2483, 0.2563, 0.2619, 0.2661, 0.2693, 0.2718, 0.2736, 0.2748, 0.2755, 0.2757, 0.2757, 0.2752, 0.2745, 0.2735, 0.2723, 0.2709, 0.2693, 0.2676, 0.2657, 0.2636, 0.2613, 0.2589, 0.2565, 0.2541, 0.2516, 0.249 , 0.2465, 0.2439, 0.2412, 0.2386, 0.236 , 0.2333, 0.2306, 0.228 , 0.2253, 0.2226, 0.2199, 0.2172, 0.2145, 0.2118, 0.2092, 0.2065, 0.2038, 0.2011, 0.1984],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2461, 0.2543, 0.2601, 0.2646, 0.268 , 0.2707, 0.2726, 0.274 , 0.2749, 0.2753, 0.2754, 0.2751, 0.2745, 0.2736, 0.2725, 0.2712, 0.2697, 0.2681, 0.2662, 0.2641, 0.2619, 0.2596, 0.2573, 0.2549, 0.2524, 0.2499, 0.2474, 0.2449, 0.2423, 0.2397, 0.237 , 0.2344, 0.2317, 0.2291, 0.2264, 0.2237, 0.221 , 0.2183, 0.2156, 0.2129, 0.2102, 0.2075, 0.2048, 0.2021],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2449, 0.2536, 0.2597, 0.2644, 0.268 , 0.2708, 0.2728, 0.2742, 0.275 , 0.2755, 0.2755, 0.2751, 0.2745, 0.2736, 0.2724, 0.2711, 0.2695, 0.2678, 0.2658, 0.2636, 0.2615, 0.2592, 0.2569, 0.2545, 0.2521, 0.2496, 0.247 , 0.2445, 0.2419, 0.2393, 0.2367, 0.234 , 0.2314, 0.2287, 0.226 , 0.2233, 0.2206, 0.2179, 0.2152, 0.2124, 0.2097, 0.207 , 0.2042],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2423, 0.2513, 0.2577, 0.2627, 0.2665, 0.2695, 0.2717, 0.2733, 0.2744, 0.275 , 0.2751, 0.275 , 0.2745, 0.2737, 0.2727, 0.2714, 0.27  , 0.2682, 0.2663, 0.2643, 0.2622, 0.2599, 0.2577, 0.2553, 0.2529, 0.2504, 0.2478, 0.2453, 0.2427, 0.2401, 0.2374, 0.2347, 0.232 , 0.2293, 0.2266, 0.2238, 0.221 , 0.2183, 0.2155, 0.2127, 0.2099, 0.2071],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.24  , 0.2496, 0.2565, 0.2618, 0.2659, 0.2691, 0.2714, 0.2732, 0.2743, 0.275 , 0.2752, 0.275 , 0.2745, 0.2737, 0.2726, 0.2713, 0.2698, 0.268 , 0.2661, 0.2641, 0.262 , 0.2598, 0.2575, 0.2551, 0.2527, 0.2502, 0.2477, 0.2451, 0.2425, 0.2399, 0.2373, 0.2346, 0.2319, 0.2291, 0.2264, 0.2236, 0.2208, 0.218 , 0.2152, 0.2124, 0.2096],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2375, 0.2477, 0.255 , 0.2606, 0.265 , 0.2684, 0.2709, 0.2728, 0.2741, 0.2748, 0.2751, 0.275 , 0.2745, 0.2737, 0.2727, 0.2714, 0.2698, 0.2681, 0.2662, 0.2642, 0.2622, 0.26  , 0.2577, 0.2554, 0.2529, 0.2505, 0.248 , 0.2454, 0.2428, 0.2401, 0.2375, 0.2348, 0.232 , 0.2293, 0.2265, 0.2237, 0.2209, 0.218 , 0.2152, 0.2123],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2358, 0.2463, 0.2539, 0.2597, 0.2643, 0.2679, 0.2705, 0.2725, 0.2739, 0.2747, 0.275 , 0.2749, 0.2745, 0.2738, 0.2727, 0.2714, 0.2699, 0.2682, 0.2664, 0.2645, 0.2624, 0.2603, 0.2581, 0.2557, 0.2534, 0.2509, 0.2484, 0.2459, 0.2433, 0.2406, 0.2379, 0.2352, 0.2325, 0.2297, 0.2269, 0.2241, 0.2212, 0.2184, 0.2155],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2352, 0.2459, 0.2537, 0.2597, 0.2644, 0.268 , 0.2707, 0.2727, 0.274 , 0.2748, 0.2751, 0.275 , 0.2745, 0.2737, 0.2726, 0.2714, 0.2699, 0.2683, 0.2665, 0.2646, 0.2626, 0.2606, 0.2584, 0.2561, 0.2538, 0.2514, 0.249 , 0.2465, 0.244 , 0.2414, 0.2387, 0.2361, 0.2334, 0.2306, 0.2278, 0.225 , 0.2222, 0.2194],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2342, 0.2455, 0.2537, 0.2599, 0.2648, 0.2685, 0.2713, 0.2732, 0.2745, 0.2752, 0.2754, 0.2751, 0.2746, 0.2736, 0.2725, 0.2711, 0.2696, 0.268 , 0.2662, 0.2642, 0.2622, 0.2601, 0.2579, 0.2556, 0.2533, 0.2508, 0.2484, 0.2458, 0.2432, 0.2406, 0.2379, 0.2352, 0.2324, 0.2296, 0.2268, 0.2239, 0.221 ],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2345, 0.246 , 0.2543, 0.2606, 0.2655, 0.2692, 0.2719, 0.2738, 0.275 , 0.2756, 0.2756, 0.2753, 0.2746, 0.2736, 0.2724, 0.271 , 0.2695, 0.2678, 0.266 , 0.264 , 0.262 , 0.2598, 0.2576, 0.2553, 0.2529, 0.2504, 0.2479, 0.2453, 0.2427, 0.24  , 0.2372, 0.2344, 0.2316, 0.2287, 0.2258, 0.2229],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2351, 0.2466, 0.2548, 0.261 , 0.2658, 0.2695, 0.2721, 0.274 , 0.2752, 0.2756, 0.2756, 0.2752, 0.2745, 0.2736, 0.2724, 0.2711, 0.2696, 0.268 , 0.2662, 0.2643, 0.2623, 0.2601, 0.2579, 0.2556, 0.2533, 0.2508, 0.2483, 0.2457, 0.2431, 0.2404, 0.2376, 0.2348, 0.232 , 0.2291, 0.2262],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2361, 0.2477, 0.256 , 0.2623, 0.2671, 0.2706, 0.2732, 0.2749, 0.2757, 0.276 , 0.2759, 0.2753, 0.2745, 0.2735, 0.2723, 0.2709, 0.2693, 0.2676, 0.2658, 0.2638, 0.2618, 0.2596, 0.2573, 0.255 , 0.2525, 0.25  , 0.2474, 0.2447, 0.242 , 0.2392, 0.2364, 0.2335, 0.2306, 0.2276],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2393, 0.2505, 0.2584, 0.2643, 0.2688, 0.2721, 0.2744, 0.2754, 0.2761, 0.2762, 0.2759, 0.2753, 0.2745, 0.2735, 0.2723, 0.2709, 0.2694, 0.2677, 0.2659, 0.264 , 0.262 , 0.2598, 0.2576, 0.2552, 0.2528, 0.2503, 0.2478, 0.2451, 0.2424, 0.2396, 0.2368, 0.2339, 0.231 ],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2438, 0.2544, 0.2618, 0.2673, 0.2713, 0.2742, 0.2755, 0.2763, 0.2767, 0.2765, 0.2761, 0.2754, 0.2745, 0.2735, 0.2722, 0.2709, 0.2693, 0.2677, 0.2659, 0.264 , 0.262 , 0.2599, 0.2577, 0.2554, 0.253 , 0.2506, 0.248 , 0.2454, 0.2427, 0.2399, 0.2371, 0.2343],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2452, 0.2556, 0.2629, 0.2683, 0.2723, 0.2741, 0.2754, 0.2762, 0.2764, 0.2763, 0.2759, 0.2753, 0.2745, 0.2735, 0.2724, 0.2711, 0.2696, 0.268 , 0.2663, 0.2645, 0.2625, 0.2605, 0.2583, 0.256 , 0.2537, 0.2512, 0.2487, 0.2461, 0.2434, 0.2406, 0.2378],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2511, 0.2607, 0.2673, 0.2721, 0.2741, 0.2756, 0.2765, 0.2767, 0.2767, 0.2765, 0.276 , 0.2753, 0.2745, 0.2735, 0.2724, 0.2711, 0.2697, 0.2682, 0.2666, 0.2648, 0.2629, 0.2609, 0.2589, 0.2567, 0.2544, 0.252 , 0.2496, 0.247 , 0.2444, 0.2417],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2554, 0.2644, 0.2705, 0.2729, 0.2748, 0.2759, 0.2764, 0.2766, 0.2765, 0.2763, 0.2758, 0.2752, 0.2745, 0.2736, 0.2725, 0.2713, 0.27  , 0.2686, 0.267 , 0.2654, 0.2636, 0.2617, 0.2596, 0.2575, 0.2552, 0.2529, 0.2504, 0.2479, 0.2453],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.261 , 0.2693, 0.2722, 0.2743, 0.2757, 0.2763, 0.2766, 0.2767, 0.2765, 0.2762, 0.2758, 0.2752, 0.2745, 0.2736, 0.2726, 0.2715, 0.2702, 0.2689, 0.2673, 0.2657, 0.264 , 0.2621, 0.2601, 0.258 , 0.2558, 0.2535, 0.2511, 0.2486],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.267 , 0.271 , 0.2736, 0.2752, 0.276 , 0.2764, 0.2766, 0.2766, 0.2765, 0.2762, 0.2757, 0.2751, 0.2744, 0.2736, 0.2727, 0.2716, 0.2704, 0.269 , 0.2675, 0.2659, 0.2642, 0.2623, 0.2603, 0.2582, 0.256 , 0.2536, 0.2512],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2685, 0.2723, 0.2744, 0.2754, 0.2761, 0.2764, 0.2766, 0.2766, 0.2764, 0.2761, 0.2757, 0.2751, 0.2744, 0.2736, 0.2727, 0.2716, 0.2703, 0.269 , 0.2675, 0.2658, 0.264 , 0.2621, 0.2601, 0.2579, 0.2556, 0.2533],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2648, 0.269 , 0.2713, 0.2729, 0.274 , 0.2748, 0.2753, 0.2756, 0.2757, 0.2756, 0.2754, 0.275 , 0.2744, 0.2737, 0.2728, 0.2717, 0.2705, 0.2691, 0.2676, 0.2659, 0.2641, 0.2621, 0.26  , 0.2578, 0.2555],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2569, 0.2624, 0.2661, 0.2688, 0.2708, 0.2724, 0.2736, 0.2744, 0.2749, 0.2751, 0.2751, 0.2749, 0.2744, 0.2737, 0.2728, 0.2717, 0.2704, 0.2689, 0.2673, 0.2655, 0.2635, 0.2614, 0.2591, 0.2567],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.251 , 0.2578, 0.2624, 0.2659, 0.2686, 0.2707, 0.2723, 0.2735, 0.2743, 0.2748, 0.2749, 0.2748, 0.2744, 0.2737, 0.2728, 0.2717, 0.2704, 0.2688, 0.2671, 0.2652, 0.2631, 0.2609, 0.2585],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2469, 0.2544, 0.2596, 0.2637, 0.2669, 0.2694, 0.2713, 0.2728, 0.2738, 0.2745, 0.2747, 0.2747, 0.2744, 0.2737, 0.2729, 0.2717, 0.2704, 0.2688, 0.2671, 0.2651, 0.2629, 0.2606],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2425, 0.251 , 0.2571, 0.2618, 0.2655, 0.2684, 0.2707, 0.2724, 0.2736, 0.2744, 0.2747, 0.2747, 0.2744, 0.2737, 0.2728, 0.2716, 0.2701, 0.2684, 0.2665, 0.2644, 0.2621],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2403, 0.2492, 0.2556, 0.2606, 0.2646, 0.2677, 0.2702, 0.272 , 0.2734, 0.2742, 0.2746, 0.2747, 0.2744, 0.2737, 0.2728, 0.2716, 0.2701, 0.2684, 0.2664, 0.2643],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2375, 0.2471, 0.254 , 0.2594, 0.2637, 0.2672, 0.2698, 0.2718, 0.2732, 0.2742, 0.2746, 0.2747, 0.2744, 0.2737, 0.2727, 0.2714, 0.2699, 0.2681, 0.266 ],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2328, 0.2435, 0.2512, 0.2573, 0.2622, 0.266 , 0.269 , 0.2713, 0.2729, 0.274 , 0.2746, 0.2747, 0.2744, 0.2737, 0.2727, 0.2714, 0.2697, 0.2679],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2317, 0.2427, 0.2508, 0.2571, 0.2621, 0.266 , 0.2691, 0.2714, 0.2731, 0.2742, 0.2747, 0.2748, 0.2744, 0.2736, 0.2725, 0.2711, 0.2694],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2319, 0.2428, 0.2507, 0.2569, 0.2619, 0.2658, 0.2689, 0.2712, 0.2729, 0.274 , 0.2746, 0.2747, 0.2744, 0.2737, 0.2726, 0.2713],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2313, 0.2426, 0.2508, 0.2572, 0.2623, 0.2663, 0.2694, 0.2717, 0.2734, 0.2744, 0.2749, 0.2749, 0.2744, 0.2736, 0.2724],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2296, 0.2412, 0.2497, 0.2564, 0.2617, 0.2659, 0.2691, 0.2715, 0.2732, 0.2743, 0.2748, 0.2749, 0.2744, 0.2736],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.229 , 0.241 , 0.2497, 0.2566, 0.262 , 0.2662, 0.2695, 0.2719, 0.2736, 0.2746, 0.2751, 0.275 , 0.2745],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2301, 0.242 , 0.2507, 0.2575, 0.2628, 0.267 , 0.2702, 0.2725, 0.2741, 0.275 , 0.2753, 0.2751],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2312, 0.2431, 0.2517, 0.2585, 0.2637, 0.2678, 0.2709, 0.2731, 0.2745, 0.2754, 0.2756],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.233 , 0.2447, 0.2532, 0.2597, 0.2648, 0.2687, 0.2717, 0.2737, 0.2751, 0.2757],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.233 , 0.2448, 0.2534, 0.26  , 0.2652, 0.2691, 0.272 , 0.2741, 0.2754],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2324, 0.2445, 0.2532, 0.2599, 0.2652, 0.2692, 0.2721, 0.2742],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2372, 0.2487, 0.257 , 0.2633, 0.2681, 0.2717, 0.2742],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2396, 0.2509, 0.259 , 0.2651, 0.2697, 0.2731],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2371, 0.249 , 0.2576, 0.264 , 0.2689],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2415, 0.253 , 0.2612, 0.2673],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2438, 0.2552, 0.2632],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2467, 0.2578],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2548],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    ]]),\n",
       " 0.5,\n",
       " array([[ 0.    , -0.2335, -0.2424, -0.2552, -0.2677, -0.2798, -0.2915, -0.3029, -0.3138, -0.3244, -0.3346, -0.3444, -0.3539, -0.363 , -0.3718, -0.3802, -0.3883, -0.3961, -0.4036, -0.4109, -0.4178, -0.4244, -0.4308, -0.4369, -0.4427, -0.4483, -0.4536, -0.461 , -0.476 , -0.4902, -0.4967, -0.5035, -0.5107, -0.5183, -0.5263, -0.5347, -0.5435, -0.5527, -0.5623, -0.5723, -0.5827, -0.5936, -0.6048, -0.6165, -0.6286, -0.641 , -0.6539, -0.6671, -0.6806, -0.6945, -0.7088, -0.7233, -0.7381],\n",
       "        [    nan,  0.    , -0.2429, -0.2559, -0.2685, -0.2808, -0.2927, -0.3043, -0.3154, -0.3262, -0.3367, -0.3468, -0.3566, -0.3661, -0.3752, -0.3841, -0.3927, -0.401 , -0.4091, -0.4169, -0.4245, -0.4319, -0.439 , -0.4458, -0.4525, -0.4589, -0.465 , -0.4725, -0.4873, -0.5009, -0.5063, -0.5122, -0.5186, -0.5254, -0.5326, -0.5402, -0.5484, -0.5569, -0.566 , -0.5754, -0.5854, -0.5958, -0.6066, -0.6179, -0.6297, -0.6419, -0.6545, -0.6675, -0.6809, -0.6946, -0.7088, -0.7233, -0.738 ],\n",
       "        [    nan,     nan,  0.    , -0.2569, -0.2697, -0.2822, -0.2944, -0.3061, -0.3174, -0.3284, -0.3389, -0.3492, -0.359 , -0.3686, -0.3779, -0.3868, -0.3955, -0.4039, -0.4121, -0.42  , -0.4276, -0.435 , -0.4422, -0.4492, -0.4559, -0.4624, -0.4687, -0.4761, -0.4908, -0.5042, -0.5094, -0.515 , -0.5211, -0.5277, -0.5347, -0.5421, -0.55  , -0.5584, -0.5673, -0.5766, -0.5864, -0.5967, -0.6074, -0.6186, -0.6303, -0.6424, -0.6549, -0.6678, -0.6811, -0.6949, -0.7089, -0.7233, -0.7381],\n",
       "        [    nan,     nan,     nan,  0.    , -0.2712, -0.2838, -0.296 , -0.3077, -0.3191, -0.33  , -0.3406, -0.3508, -0.3607, -0.3702, -0.3794, -0.3883, -0.3969, -0.4053, -0.4134, -0.4213, -0.4289, -0.4363, -0.4435, -0.4504, -0.4572, -0.4637, -0.47  , -0.4775, -0.4923, -0.5057, -0.5109, -0.5165, -0.5226, -0.5292, -0.5362, -0.5436, -0.5516, -0.56  , -0.5688, -0.5781, -0.5879, -0.5981, -0.6088, -0.62  , -0.6316, -0.6436, -0.656 , -0.6688, -0.682 , -0.6956, -0.7096, -0.7238, -0.7384],\n",
       "        [    nan,     nan,     nan,     nan,  0.    , -0.2859, -0.2981, -0.3099, -0.3213, -0.3323, -0.343 , -0.3533, -0.3633, -0.3729, -0.3823, -0.3913, -0.4001, -0.4086, -0.4169, -0.4249, -0.4327, -0.4402, -0.4475, -0.4546, -0.4615, -0.4682, -0.4746, -0.4822, -0.497 , -0.5103, -0.5151, -0.5205, -0.5263, -0.5326, -0.5394, -0.5467, -0.5544, -0.5626, -0.5713, -0.5804, -0.59  , -0.6001, -0.6107, -0.6217, -0.6331, -0.645 , -0.6573, -0.6699, -0.683 , -0.6964, -0.7102, -0.7243, -0.7387],\n",
       "        [    nan,     nan,     nan,     nan,     nan,  0.    , -0.301 , -0.3126, -0.3241, -0.3352, -0.3459, -0.3564, -0.3665, -0.3764, -0.386 , -0.3953, -0.4045, -0.4133, -0.422 , -0.4304, -0.4387, -0.4467, -0.4546, -0.4623, -0.4697, -0.477 , -0.4841, -0.4918, -0.5063, -0.5191, -0.5232, -0.5278, -0.5329, -0.5385, -0.5447, -0.5514, -0.5585, -0.5662, -0.5745, -0.5832, -0.5924, -0.6021, -0.6123, -0.6231, -0.6342, -0.6459, -0.6579, -0.6704, -0.6834, -0.6967, -0.7103, -0.7243, -0.7387],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.3166, -0.3277, -0.3388, -0.3496, -0.36  , -0.3702, -0.3801, -0.3898, -0.3992, -0.4083, -0.4173, -0.426 , -0.4346, -0.4429, -0.4511, -0.4591, -0.4669, -0.4745, -0.4819, -0.4892, -0.4969, -0.5113, -0.5239, -0.5275, -0.5317, -0.5365, -0.5418, -0.5476, -0.554 , -0.5609, -0.5684, -0.5764, -0.5849, -0.5939, -0.6035, -0.6135, -0.6241, -0.6351, -0.6466, -0.6585, -0.6709, -0.6838, -0.697 , -0.7106, -0.7245, -0.7388],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.3326, -0.3433, -0.3541, -0.3646, -0.3748, -0.3848, -0.3945, -0.4039, -0.4132, -0.4222, -0.4309, -0.4395, -0.4479, -0.456 , -0.464 , -0.4718, -0.4794, -0.4868, -0.494 , -0.5016, -0.5158, -0.528 , -0.5312, -0.5349, -0.5393, -0.5442, -0.5497, -0.5558, -0.5624, -0.5696, -0.5774, -0.5857, -0.5945, -0.6039, -0.6138, -0.6242, -0.6352, -0.6466, -0.6585, -0.6708, -0.6836, -0.6968, -0.7104, -0.7244, -0.7387],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.349 , -0.3589, -0.3691, -0.3791, -0.3888, -0.3982, -0.4075, -0.4164, -0.4252, -0.4337, -0.442 , -0.4501, -0.4579, -0.4656, -0.4731, -0.4804, -0.4875, -0.4944, -0.5017, -0.5157, -0.5276, -0.5306, -0.5342, -0.5384, -0.5432, -0.5486, -0.5546, -0.5611, -0.5683, -0.576 , -0.5843, -0.5931, -0.6025, -0.6124, -0.6229, -0.6339, -0.6453, -0.6573, -0.6698, -0.6827, -0.696 , -0.7098, -0.7239, -0.7384],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.3658, -0.3751, -0.3849, -0.3946, -0.4042, -0.4135, -0.4227, -0.4317, -0.4404, -0.449 , -0.4574, -0.4657, -0.4737, -0.4815, -0.4892, -0.4967, -0.504 , -0.5114, -0.5253, -0.5369, -0.5391, -0.5421, -0.5457, -0.5499, -0.5547, -0.5602, -0.5663, -0.573 , -0.5803, -0.5882, -0.5967, -0.6057, -0.6153, -0.6255, -0.6362, -0.6474, -0.6591, -0.6713, -0.684 , -0.697 , -0.7106, -0.7245, -0.7387],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.3828, -0.3912, -0.4002, -0.4093, -0.4182, -0.4269, -0.4355, -0.4439, -0.452 , -0.46  , -0.4678, -0.4755, -0.4829, -0.4901, -0.4971, -0.5039, -0.5112, -0.525 , -0.5364, -0.5387, -0.5416, -0.5452, -0.5494, -0.5543, -0.5598, -0.566 , -0.5727, -0.5801, -0.588 , -0.5966, -0.6057, -0.6153, -0.6255, -0.6362, -0.6475, -0.6592, -0.6714, -0.6841, -0.6972, -0.7107, -0.7245, -0.7388],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.4002, -0.4077, -0.4161, -0.4247, -0.4333, -0.4417, -0.4501, -0.4584, -0.4666, -0.4746, -0.4824, -0.4902, -0.4977, -0.5052, -0.5124, -0.5198, -0.5334, -0.5445, -0.5461, -0.5484, -0.5514, -0.5551, -0.5596, -0.5646, -0.5703, -0.5767, -0.5837, -0.5913, -0.5995, -0.6083, -0.6177, -0.6276, -0.6381, -0.6491, -0.6606, -0.6726, -0.685 , -0.6979, -0.7113, -0.725 , -0.739 ],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.4179, -0.4244, -0.4321, -0.4401, -0.4482, -0.4563, -0.4644, -0.4724, -0.4803, -0.4882, -0.4959, -0.5035, -0.511 , -0.5184, -0.5256, -0.539 , -0.5497, -0.5508, -0.5527, -0.5553, -0.5586, -0.5627, -0.5675, -0.5729, -0.579 , -0.5858, -0.5932, -0.6012, -0.6098, -0.619 , -0.6288, -0.6391, -0.65  , -0.6614, -0.6733, -0.6856, -0.6984, -0.7116, -0.7252, -0.7392],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.4361, -0.4416, -0.4485, -0.4559, -0.4635, -0.4712, -0.479 , -0.4867, -0.4944, -0.502 , -0.5095, -0.517 , -0.5243, -0.5314, -0.5445, -0.5548, -0.5552, -0.5566, -0.5587, -0.5616, -0.5653, -0.5697, -0.5748, -0.5807, -0.5872, -0.5943, -0.6021, -0.6106, -0.6196, -0.6293, -0.6395, -0.6502, -0.6616, -0.6734, -0.6857, -0.6984, -0.7116, -0.7252, -0.7391],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.4549, -0.4595, -0.4659, -0.4729, -0.4803, -0.4878, -0.4955, -0.5032, -0.5109, -0.5187, -0.5264, -0.534 , -0.5408, -0.5534, -0.5629, -0.5623, -0.5627, -0.564 , -0.5661, -0.5691, -0.5729, -0.5774, -0.5827, -0.5887, -0.5954, -0.6029, -0.611 , -0.6198, -0.6292, -0.6392, -0.6499, -0.6611, -0.6728, -0.6851, -0.6979, -0.7111, -0.7248, -0.7389],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.474 , -0.4777, -0.4837, -0.4905, -0.4979, -0.5057, -0.5136, -0.5218, -0.53  , -0.5383, -0.5467, -0.5537, -0.5662, -0.5752, -0.5738, -0.5734, -0.5739, -0.5753, -0.5776, -0.5808, -0.5847, -0.5895, -0.595 , -0.6013, -0.6083, -0.6159, -0.6243, -0.6333, -0.643 , -0.6532, -0.664 , -0.6754, -0.6873, -0.6997, -0.7125, -0.7258, -0.7395],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.4939, -0.4968, -0.5025, -0.5092, -0.5166, -0.5244, -0.5326, -0.541 , -0.5496, -0.5583, -0.5654, -0.5777, -0.5861, -0.5839, -0.5827, -0.5825, -0.5833, -0.585 , -0.5876, -0.5911, -0.5954, -0.6004, -0.6063, -0.6129, -0.6202, -0.6282, -0.6369, -0.6462, -0.6561, -0.6666, -0.6776, -0.6892, -0.7013, -0.7138, -0.7267, -0.74  ],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.5143, -0.5164, -0.5215, -0.5279, -0.5351, -0.5428, -0.5508, -0.5591, -0.5677, -0.5745, -0.5864, -0.5941, -0.591 , -0.589 , -0.5881, -0.5882, -0.5893, -0.5913, -0.5943, -0.5981, -0.6028, -0.6083, -0.6146, -0.6216, -0.6293, -0.6378, -0.6469, -0.6567, -0.667 , -0.6779, -0.6894, -0.7014, -0.7139, -0.7267, -0.74  ],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.5352, -0.5368, -0.5419, -0.5484, -0.5558, -0.564 , -0.5726, -0.5816, -0.5886, -0.6003, -0.6075, -0.6034, -0.6006, -0.5989, -0.5983, -0.5987, -0.6001, -0.6025, -0.6058, -0.6099, -0.6149, -0.6207, -0.6273, -0.6346, -0.6426, -0.6513, -0.6606, -0.6705, -0.681 , -0.6921, -0.7036, -0.7156, -0.728 , -0.7408],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.5573, -0.5589, -0.564 , -0.5706, -0.5783, -0.5867, -0.5955, -0.6023, -0.6133, -0.6196, -0.6142, -0.6101, -0.6073, -0.6057, -0.6052, -0.6058, -0.6074, -0.61  , -0.6136, -0.618 , -0.6233, -0.6294, -0.6363, -0.644 , -0.6524, -0.6614, -0.6711, -0.6814, -0.6923, -0.7037, -0.7156, -0.728 , -0.7408],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.5803, -0.5823, -0.5879, -0.5951, -0.6034, -0.6125, -0.6192, -0.6297, -0.6349, -0.628 , -0.6226, -0.6185, -0.6156, -0.614 , -0.6136, -0.6142, -0.6159, -0.6187, -0.6224, -0.627 , -0.6325, -0.6389, -0.6461, -0.654 , -0.6627, -0.6721, -0.6822, -0.6928, -0.704 , -0.7158, -0.7281, -0.7408],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.603 , -0.6045, -0.61  , -0.6172, -0.6257, -0.6318, -0.6415, -0.6456, -0.6374, -0.6307, -0.6255, -0.6218, -0.6193, -0.618 , -0.618 , -0.6191, -0.6213, -0.6245, -0.6287, -0.6338, -0.6399, -0.6468, -0.6545, -0.663 , -0.6723, -0.6822, -0.6928, -0.704 , -0.7157, -0.728 , -0.7407],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.6276, -0.6302, -0.6366, -0.6448, -0.6508, -0.6598, -0.6627, -0.6527, -0.6444, -0.6377, -0.6325, -0.6286, -0.6261, -0.6249, -0.6249, -0.6261, -0.6285, -0.6319, -0.6363, -0.6418, -0.6481, -0.6554, -0.6635, -0.6725, -0.6822, -0.6926, -0.7036, -0.7154, -0.7276, -0.7405],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.6523, -0.6555, -0.6625, -0.6679, -0.676 , -0.6775, -0.6658, -0.6559, -0.6477, -0.6411, -0.636 , -0.6323, -0.6301, -0.6292, -0.6296, -0.6313, -0.6341, -0.638 , -0.643 , -0.649 , -0.656 , -0.6638, -0.6726, -0.6821, -0.6924, -0.7035, -0.7152, -0.7275, -0.7404],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.6781, -0.6824, -0.6873, -0.6946, -0.6949, -0.6815, -0.67  , -0.6603, -0.6523, -0.6459, -0.6411, -0.6378, -0.6358, -0.6353, -0.6361, -0.6381, -0.6414, -0.6457, -0.6512, -0.6577, -0.6652, -0.6736, -0.6828, -0.6929, -0.7037, -0.7153, -0.7275, -0.7403],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7046, -0.7075, -0.7138, -0.7128, -0.6977, -0.6847, -0.6735, -0.6642, -0.6565, -0.6505, -0.6461, -0.6432, -0.6418, -0.6418, -0.6431, -0.6456, -0.6494, -0.6544, -0.6604, -0.6674, -0.6754, -0.6843, -0.6941, -0.7047, -0.716 , -0.728 , -0.7406],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7301, -0.7338, -0.7313, -0.7146, -0.7001, -0.6876, -0.677 , -0.6682, -0.6611, -0.6557, -0.6519, -0.6496, -0.6487, -0.6493, -0.6512, -0.6544, -0.6587, -0.6642, -0.6707, -0.6783, -0.6868, -0.6961, -0.7063, -0.7172, -0.7288, -0.7411],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7545, -0.7475, -0.728 , -0.7115, -0.6975, -0.6857, -0.6759, -0.668 , -0.6619, -0.6574, -0.6546, -0.6533, -0.6534, -0.6549, -0.6577, -0.6617, -0.6669, -0.6731, -0.6804, -0.6886, -0.6977, -0.7075, -0.7182, -0.7296, -0.7416],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7678, -0.7418, -0.7221, -0.7061, -0.693 , -0.6823, -0.6738, -0.6673, -0.6626, -0.6596, -0.6581, -0.6581, -0.6595, -0.6622, -0.666 , -0.671 , -0.677 , -0.684 , -0.6918, -0.7005, -0.71  , -0.7202, -0.7311, -0.7425],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7646, -0.7382, -0.7187, -0.7033, -0.691 , -0.6813, -0.6739, -0.6685, -0.6649, -0.663 , -0.6627, -0.6637, -0.6661, -0.6696, -0.6743, -0.68  , -0.6867, -0.6943, -0.7027, -0.7118, -0.7216, -0.7321, -0.7432],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7622, -0.7359, -0.7168, -0.702 , -0.6905, -0.6817, -0.6753, -0.6708, -0.6682, -0.6672, -0.6677, -0.6696, -0.6728, -0.6771, -0.6825, -0.6889, -0.6961, -0.7042, -0.7131, -0.7227, -0.7329, -0.7436],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.76  , -0.7338, -0.7155, -0.7016, -0.6912, -0.6835, -0.6781, -0.6748, -0.6733, -0.6733, -0.6748, -0.6775, -0.6815, -0.6865, -0.6925, -0.6993, -0.707 , -0.7155, -0.7246, -0.7343, -0.7445],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7587, -0.7333, -0.7157, -0.7026, -0.693 , -0.6861, -0.6816, -0.679 , -0.6782, -0.679 , -0.6812, -0.6846, -0.6891, -0.6947, -0.7012, -0.7085, -0.7167, -0.7255, -0.7349, -0.7449],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7574, -0.7327, -0.716 , -0.7039, -0.6952, -0.6893, -0.6857, -0.6841, -0.6841, -0.6856, -0.6885, -0.6926, -0.6977, -0.7038, -0.7107, -0.7184, -0.7269, -0.7359, -0.7456],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7554, -0.7312, -0.7152, -0.7041, -0.6965, -0.6916, -0.689 , -0.6883, -0.6893, -0.6917, -0.6953, -0.7001, -0.7058, -0.7124, -0.7199, -0.728 , -0.7368, -0.7461],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.755 , -0.732 , -0.7172, -0.7071, -0.7005, -0.6965, -0.6947, -0.6948, -0.6964, -0.6994, -0.7036, -0.7088, -0.7149, -0.7219, -0.7296, -0.7379, -0.7468],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7551, -0.7335, -0.7197, -0.7104, -0.7044, -0.701 , -0.6998, -0.7003, -0.7024, -0.7059, -0.7105, -0.7161, -0.7227, -0.7301, -0.7382, -0.7469],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7551, -0.735 , -0.7225, -0.7143, -0.7094, -0.7069, -0.7065, -0.7077, -0.7105, -0.7144, -0.7194, -0.7254, -0.7322, -0.7397, -0.7479],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7548, -0.7358, -0.7243, -0.717 , -0.7129, -0.7112, -0.7115, -0.7135, -0.7168, -0.7213, -0.7268, -0.7332, -0.7404, -0.7483],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7549, -0.7374, -0.7271, -0.7209, -0.7177, -0.7168, -0.7178, -0.7204, -0.7242, -0.7291, -0.735 , -0.7417, -0.7491],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7556, -0.7398, -0.7307, -0.7255, -0.7231, -0.7229, -0.7245, -0.7275, -0.7317, -0.7369, -0.7431, -0.7499],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7562, -0.7421, -0.7342, -0.73  , -0.7284, -0.7288, -0.7309, -0.7344, -0.7389, -0.7445, -0.7508],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7571, -0.7446, -0.7379, -0.7345, -0.7336, -0.7347, -0.7372, -0.7411, -0.7459, -0.7517],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7576, -0.7465, -0.7408, -0.7383, -0.7382, -0.7399, -0.743 , -0.7472, -0.7525],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.758 , -0.7482, -0.7436, -0.742 , -0.7426, -0.7448, -0.7485, -0.7532],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7593, -0.7514, -0.7479, -0.7471, -0.7483, -0.7509, -0.7547],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7602, -0.7537, -0.7513, -0.7512, -0.7529, -0.756 ],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7605, -0.7551, -0.7536, -0.7543, -0.7567],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7616, -0.7577, -0.757 , -0.7584],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7624, -0.7597, -0.7599],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7632, -0.7617],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7642],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    ]]))"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LMMSABR:\n",
    "    def __init__(\n",
    "        self,\n",
    "        rho_mat,\n",
    "        theta_mat,\n",
    "        phi_mat,\n",
    "        g_params,\n",
    "        h_params,\n",
    "        epsilon_exp,\n",
    "        k0_exp,\n",
    "        df_cap,\n",
    "        df_raw_spot,\n",
    "        tau=0.5,\n",
    "        tenor=1,\n",
    "        resolution=2,\n",
    "        max_expiry=9.5,\n",
    "        beta=0.5,\n",
    "        B=0.5\n",
    "        \n",
    "        \n",
    "    ):\n",
    "        self.rho_mat = rho_mat\n",
    "        self.theta_mat = theta_mat\n",
    "        self.phi_mat = phi_mat\n",
    "        self.g = partial(get_instant_vol_func, params=g_params)\n",
    "        self.h = partial(get_instant_vol_func, params=h_params)\n",
    "        self.epsilon_exp = epsilon_exp\n",
    "        self.k0_exp = k0_exp\n",
    "\n",
    "        self.tau = tau\n",
    "        self.tenor = tenor\n",
    "        self.resolution = resolution\n",
    "        self.max_expiry = max_expiry\n",
    "        self.dt = tau / resolution\n",
    "        self.beta = beta\n",
    "        self.B = B\n",
    "\n",
    "        # Store raw curve inputs\n",
    "        self.df_cap = df_cap\n",
    "        self.df_raw_spot = df_raw_spot\n",
    "\n",
    "        # Placeholders\n",
    "        self.df_init = None\n",
    "        self.f_sim = None\n",
    "        self.k_mat = None\n",
    "        self.swap_sim = None\n",
    "        self.prepare_curves()\n",
    "        self.G_tensor = self.precompute_G_tensor()\n",
    "        self.ggh_tensor = self.build_V_tensor_from_scalar( tenor=self.tenor, resolution=self.resolution, tau=self.tau, max_expiry=self.max_expiry)\n",
    "    def h_ij_vectorized_from_grid(self,t, u_arr, T_i, T_j):\n",
    "        \"\"\"\n",
    "        Compute h_ij for each u in u_arr using the self.t_arr as the integration grid.\n",
    "        Uses searchsorted for efficient pre-filtering of integration intervals.\n",
    "        \"\"\"\n",
    "        t_arr = self.t_arr\n",
    "        t_idx = np.searchsorted(t_arr, t, side='left')  # index just after t\n",
    "        hij_vals = []\n",
    "\n",
    "        for u in u_arr:\n",
    "            if u <= t:\n",
    "                hij_vals.append(0.0)\n",
    "                continue\n",
    "\n",
    "            u_idx = np.searchsorted(t_arr, u, side='right')\n",
    "            s_grid = t_arr[t_idx:u_idx]\n",
    "            if len(s_grid) == 0:\n",
    "                hij_vals.append(0.0)\n",
    "                continue\n",
    "\n",
    "            h_prod = self.h(T_i - s_grid) * self.h(T_j - s_grid)\n",
    "            integral = np.trapz(h_prod, s_grid)\n",
    "            hij_vals.append(np.sqrt(integral / (u - t)))\n",
    "\n",
    "        return np.array(hij_vals)\n",
    "\n",
    "\n",
    "    def integral_term_V(self, t_idx, T_idx, i, j):\n",
    "        \"\"\"\n",
    "        Compute the integral:\n",
    "        ∫ₜᵀ g_i(u)*g_j(u)*[h_ij(t,u)]²*(u-t) du\n",
    "        using lmm.t_arr as the integration grid.\n",
    "        \"\"\"\n",
    "        t = self.t_arr[t_idx]\n",
    "        T = self.t_arr[T_idx]\n",
    "        if T <= t:\n",
    "            return 0.0\n",
    "\n",
    "        u_arr = self.t_arr[t_idx:T_idx+1]\n",
    "        dt_arr = u_arr - t\n",
    "        T_i = self.t_arr[i]\n",
    "        T_j = self.t_arr[j]\n",
    "\n",
    "        h_vals = self.h_ij_vectorized_from_grid(t, u_arr, T_i, T_j)\n",
    "        h_sq = h_vals**2\n",
    "\n",
    "        g_i_arr = self.g(T_i - u_arr)\n",
    "        g_j_arr = self.g(T_j - u_arr)\n",
    "\n",
    "        integrand = g_i_arr * g_j_arr * h_sq * dt_arr\n",
    "        return np.trapz(integrand, u_arr)\n",
    "        \n",
    "    def build_V_tensor_from_scalar(self, tenor, resolution, tau, max_expiry, n_grid=100):\n",
    "        \"\"\"\n",
    "        Build the V_tensor using scalar integral_term_V, memoizing based on\n",
    "        time-translation invariance.\n",
    "\n",
    "        Returns:\n",
    "        - V_tensor: np.ndarray, shape (n_t, n_t, n, n)\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        tenor = self.tenor\n",
    "        resolution = self.resolution\n",
    "        tau = self.tau\n",
    "        max_expiry = self.max_expiry\n",
    "        n = int(tenor / tau)\n",
    "        max_expiry_steps = int(max_expiry * resolution / tau)\n",
    "        num_t = len(self.t_arr) - n * resolution\n",
    "        V_tensor = np.full((num_t, num_t, n, n), np.nan)\n",
    "\n",
    "        cache = {}  # key: (delta_T, delta_i, delta_j) -> float\n",
    "\n",
    "        for t_idx in range(num_t):\n",
    "            expiry_limit = min(t_idx + max_expiry_steps+1, num_t)\n",
    "\n",
    "            for T_idx in range(t_idx, expiry_limit):\n",
    "                start_idx = T_idx\n",
    "                end_idx = T_idx + n * resolution\n",
    "                indices = list(range(start_idx, end_idx, resolution))\n",
    "\n",
    "                if len(indices) != n:\n",
    "                    print(f\"Skipping incomplete swap at indices: {indices}\")\n",
    "                    continue  # Skip incomplete swaps at boundary\n",
    "\n",
    "                delta_T = T_idx - t_idx\n",
    "\n",
    "                for i_local, i_global in enumerate(indices):\n",
    "                    for j_local, j_global in enumerate(indices):\n",
    "                        delta_i = i_global - T_idx\n",
    "                        delta_j = j_global - T_idx\n",
    "\n",
    "                        key = (delta_T, delta_i, delta_j)\n",
    "\n",
    "                        if key not in cache:\n",
    "                            # Compute and store\n",
    "                            cache[key] = self.integral_term_V(\n",
    "                                t_idx, T_idx, i_global, j_global\n",
    "                            )\n",
    "                        V_tensor[t_idx, T_idx, i_local, j_local] = cache[key]\n",
    "\n",
    "        return V_tensor\n",
    "\n",
    "    def precompute_G_tensor(self):\n",
    "        \"\"\"\n",
    "        Precompute a G_tensor using memoization and the self.t_arr as integration grid.\n",
    "        Uses:\n",
    "            G[t_idx, T_idx, i_local, j_local] = ∫ₜᵀ g(T_i - u) * g(T_j - u) du\n",
    "        with T_i, T_j based on T_idx and forward rate spacing.\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "\n",
    "        t_arr = self.t_arr\n",
    "        resolution = self.resolution\n",
    "        tau = self.tau\n",
    "        tenor = self.tenor\n",
    "        max_expiry = self.max_expiry\n",
    "\n",
    "        num_t = len(t_arr) - int(tenor * resolution / tau)\n",
    "        n = int(tenor / tau)\n",
    "        G_tensor = np.zeros((num_t, num_t, n, n))*np.nan\n",
    "        # set the diagonal of the n,m,k,l tensor to 0\n",
    "        G_tensor[np.diag_indices(num_t)] = 0\n",
    "\n",
    "        cache = {}  # (delta_T_idx, delta_i, delta_j) → float\n",
    "        max_expiry_idx = int(max_expiry * resolution / tau)  # max expiry in steps\n",
    "        for t_idx in range(num_t):\n",
    "            for T_idx in range(t_idx, num_t):\n",
    "                delta_T_idx = T_idx - t_idx\n",
    "                # check for max expiry\n",
    "                if delta_T_idx > max_expiry_idx:\n",
    "                    continue\n",
    "                # Use the actual model time grid for integration\n",
    "                s_idx_start = t_idx   # strictly > t\n",
    "                s_idx_end = T_idx + 1    # include T\n",
    "                u_arr = t_arr[s_idx_start:s_idx_end]\n",
    "                if len(u_arr) < 1:\n",
    "                    G_tensor[t_idx, T_idx] = np.zeros((n, n))  # no integration needed\n",
    "                    #print(\"Skipping integration for empty u_arr at indices:\", s_idx_start, s_idx_end)\n",
    "                    continue  # skip if no points to integrate over\n",
    "\n",
    "                start_idx = T_idx\n",
    "                end_idx = T_idx + n * resolution\n",
    "                indices = list(range(start_idx, end_idx, resolution))\n",
    "                if len(indices) != n:\n",
    "                    continue  # incomplete swap\n",
    "\n",
    "                for i_local, i_global in enumerate(indices):\n",
    "                    for j_local, j_global in enumerate(indices):\n",
    "                        delta_i = i_global - T_idx\n",
    "                        delta_j = j_global - T_idx\n",
    "                        key = (delta_T_idx, delta_i, delta_j)\n",
    "\n",
    "                        if key not in cache:\n",
    "                            T_i = t_arr[i_global]\n",
    "                            T_j = t_arr[j_global]\n",
    "                            g_i = self.g(T_i - u_arr)\n",
    "                            g_j = self.g(T_j - u_arr)\n",
    "                            cache[key] = np.trapz(g_i * g_j, u_arr)\n",
    "\n",
    "                        G_tensor[t_idx, T_idx, i_local, j_local] = cache[key]\n",
    "\n",
    "        return G_tensor\n",
    "\n",
    "\n",
    "\n",
    "    def prepare_curves(self):\n",
    "        self.df_init = create_df_init(\n",
    "            self.df_cap, self.df_raw_spot, resolution=self.resolution, tau=self.tau\n",
    "        ).query(f\"Tenor <= {self.max_expiry + 1e-6}\")\n",
    "        self.tenors = self.df_init[\"Tenor\"].values\n",
    "        self.t_arr = self.tenors\n",
    "        self.ids_fwd_canon = self.df_init[\"Forward\"].dropna().index.values\n",
    "        self.num_forwards = len(self.ids_fwd_canon)\n",
    "        self.n_steps = len(self.df_init)\n",
    "\n",
    "    def precompute_vol_surfaces(self):\n",
    "        ttm_mat = self.tenors[None, :] - self.tenors[:,None]\n",
    "        self.ttm_mat = ttm_mat\n",
    "\n",
    "        self.h_mat = self.h(ttm_mat[1:, self.ids_fwd_canon])\n",
    "        self.g_mat = self.g(ttm_mat[:, self.ids_fwd_canon])\n",
    "\n",
    "    def precompute_interpolation(self):\n",
    "        self.interp_func, self.interp_vol_func = interp_func_fac(\n",
    "            self.df_init,\n",
    "            resolution=self.resolution,\n",
    "            tau=self.tau,\n",
    "            rho_mat=self.rho_mat,\n",
    "            g_func=self.g,\n",
    "            interp_vol=True,\n",
    "        )\n",
    "        self.rho_mat_0m_interpolated = interpolate_correlation_matrix(self.rho_mat, self.resolution)\n",
    "        self.theta_mat_0m_interpolated = interpolate_correlation_matrix(self.theta_mat, self.resolution)\n",
    "        self.phi_mat_0m_interpolated = interpolate_correlation_matrix(self.phi_mat, self.resolution)\n",
    "\n",
    "    def simulate_forwards(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "        dt = self.dt\n",
    "        dt_sqrt = np.sqrt(dt)\n",
    "\n",
    "        dZ_f = np.random.multivariate_normal(\n",
    "            np.zeros(self.num_forwards),\n",
    "            self.rho_mat[:self.num_forwards, :self.num_forwards],\n",
    "            self.n_steps-1,\n",
    "        ) * dt_sqrt\n",
    "        dW_s = np.random.multivariate_normal(\n",
    "            np.zeros(self.num_forwards),\n",
    "            self.theta_mat[:self.num_forwards, :self.num_forwards],\n",
    "            self.n_steps-1,\n",
    "        ) * dt_sqrt\n",
    "\n",
    "        f_0 = self.df_init[\"Forward\"].values\n",
    "        f_sim = np.full((self.n_steps, len(f_0)), np.nan)\n",
    "        f_sim[0] = f_0*4   # temporary adjustment\n",
    "        self.f_sim = f_sim\n",
    "        self.dZ_f = dZ_f\n",
    "        self.dW_s = dW_s\n",
    "\n",
    "        self._simulate_vol_surface()\n",
    "        self._simulate_forward_dynamics()\n",
    "\n",
    "    def _simulate_vol_surface(self):\n",
    "        g_mat = self.g_mat\n",
    "        h_mat = self.h_mat\n",
    "\n",
    "        k_mat = np.concatenate([self.k0_exp[:self.num_forwards].reshape(1, -1), (self.k0_exp[:self.num_forwards] * np.cumprod(1 + self.epsilon_exp[:self.num_forwards].reshape(1, -1) * self.dW_s * h_mat, axis=0))])\n",
    "\n",
    "        self.k_mat = k_mat\n",
    "        self.s_mat = g_mat * k_mat\n",
    "        self.k_mat_full_res = np.zeros((self.n_steps, self.n_steps))*np.nan\n",
    "        s_mat_full_res = np.zeros((self.n_steps, self.n_steps))*np.nan\n",
    "        \n",
    "        s_mat_full_res[:, self.ids_fwd_canon] = k_mat * self.g_mat\n",
    "\n",
    "        for i in range(self.n_steps):\n",
    "            self.k_mat_full_res[i,:-self.resolution] = self.interp_vol_func(s_mat_full_res[i], t_idx=i)\n",
    "            if i >= self.resolution:\n",
    "                self.k_mat_full_res[i, :(i-self.resolution)] = np.nan\n",
    "        self.s_mat_full_res = self.k_mat_full_res * self.g(self.ttm_mat)\n",
    "    def _simulate_forward_dynamics(self):\n",
    "        interp_func = self.interp_func\n",
    "        k_mat = self.k_mat\n",
    "\n",
    "        f_sim = self.f_sim\n",
    "        dZ_f = self.dZ_f\n",
    "        drift_correction = np.zeros(len(f_sim[0]))\n",
    "        drift_shared = np.zeros(len(f_sim[0]))\n",
    "\n",
    "        \n",
    "        ids_rev = self.ids_fwd_canon[::-1]\n",
    "        ids_short_rev = ids_rev // self.resolution\n",
    "        non_canon_idx = np.setdiff1d(np.arange(len(f_sim[0]))[:-self.resolution], self.ids_fwd_canon)\n",
    "        f_sim[0, non_canon_idx] = self.interp_func(f_sim[0])[non_canon_idx]\n",
    "        for t in range(1, self.n_steps-1):\n",
    "            drift_correction.fill(0)\n",
    "            drift_shared.fill(0)\n",
    "            # next loop runs from longest to shortest tenor\n",
    "            for canon_short_idx, canon_idx in zip(ids_short_rev, ids_rev):\n",
    "                if self.ttm_mat[t, canon_idx] +self.tau+1e-8>= 0:     # TODO <------------ THIS IS IMPORTANT\n",
    "                    s_t, dZ_f_t,  f_t = self.s_mat[t-1, canon_short_idx], dZ_f[t-1,canon_short_idx], f_sim[t-1,canon_idx]\n",
    "                    f_beta_t = f_t**self.beta\n",
    "                    \n",
    "                    drift_f = (-self.g_mat[t, canon_short_idx] * k_mat[t, canon_short_idx] * f_beta_t * drift_shared[canon_short_idx])\n",
    "                    df_t =  drift_f + f_beta_t*s_t*dZ_f_t\n",
    "                    f_t += df_t\n",
    "                    \n",
    "                    f_t_new =  f_t + df_t if f_t + df_t > 0 else 0  # zero absorbing boundary, interest rates cannot be negative due to arbitrage\n",
    "                    f_sim[t,canon_idx] = f_t_new\n",
    "\n",
    "                    if canon_short_idx > 0:\n",
    "                        drift_correction[canon_short_idx-1] = self.rho_mat[canon_short_idx-1, canon_short_idx] * self.tau * self.g_mat[t,canon_short_idx] * k_mat[t, canon_short_idx] * f_beta_t / (1 + self.tau * f_t)\n",
    "                        drift_shared[canon_short_idx-1] = np.sum(drift_correction[canon_short_idx-1:])\n",
    "            #print(f_sim[t])\n",
    "            f_sim[t, non_canon_idx] = interp_func(f_sim[t])[non_canon_idx]\n",
    "\n",
    "    def simulate(self, seed=None):\n",
    "        self.prepare_curves()\n",
    "        self.precompute_vol_surfaces()\n",
    "        self.precompute_interpolation()\n",
    "        self.simulate_forwards(seed=seed)\n",
    "        return self.f_sim\n",
    "    \n",
    "    \n",
    "    def get_swap_matrix(self):\n",
    "        T_idxs = np.arange(len(self.f_sim)-int(self.tenor/self.tau*self.resolution))\n",
    "        swap_sim, W = get_swap_matrix(\n",
    "            self.f_sim, T_idxs=T_idxs, resolution=self.resolution, tau=self.tau, tenor=self.tenor, df=self.df_init\n",
    "        )\n",
    "        swap_sim[np.tril_indices_from(swap_sim, k=-1)] = np.nan\n",
    "        self.swap_sim = swap_sim\n",
    "        self.W = W\n",
    "        assert self.k_mat_full_res.shape == self.f_sim.shape, f\"Shape mismatch: {self.k_mat_full_res.shape} != {self.f_sim.shape}\"\n",
    "        return swap_sim, W\n",
    "    \n",
    "\n",
    "    def get_sabr_params(self):\n",
    "        # ==========================================================\n",
    "        #          Create tensors for the SABR parameters\n",
    "        # ==========================================================\n",
    "        \n",
    "        swap_idxs = np.arange(self.swap_sim.shape[0])\n",
    "        swap_indexer = make_swap_indexer(n_steps = self.f_sim.shape[0], T_idxs=swap_idxs,resolution=self.resolution, tau=0.5, tenor=1,)\n",
    "        k_tensor = swap_indexer(self.k_mat_full_res)\n",
    "\n",
    "        rho_mat_0m_interpolated = self.rho_mat_0m_interpolated\n",
    "        rho_tensor = build_swap_correlation_tensor(rho_mat_0m_interpolated, T_idxs=swap_idxs, resolution=self.resolution, tau=0.5, tenor=1)\n",
    "        k_tensor_prod = pairwise_outer(k_tensor)\n",
    "        W_tensor_prod = pairwise_outer(self.W)\n",
    "        # ==========================================================\n",
    "        # for the m,n,k,l tensor, sum such that we have a m,n tensor\n",
    "        \n",
    "        # ===========================================================\n",
    "        #               compute sigma tensor\n",
    "        # ==========================================================\n",
    "        prod = rho_tensor*W_tensor_prod*k_tensor_prod*self.G_tensor\n",
    "\n",
    "        numerator = np.sum(prod, axis=(2, 3))\n",
    "        denominator = self.ttm_mat[np.ix_(swap_idxs, swap_idxs)]\n",
    "        sigma_sq = np.divide(\n",
    "            numerator,\n",
    "            denominator,\n",
    "            out=np.zeros_like(numerator),  # fill result with 0 where denominator == 0\n",
    "            where=denominator != 0\n",
    "        )\n",
    "        sigma = np.sqrt(sigma_sq)\n",
    "\n",
    "\n",
    "        # ==========================================================\n",
    "        #              compute V and Phi tensor       \n",
    "        # ==========================================================\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        V_terms = rho_tensor*W_tensor_prod*k_tensor_prod*self.ggh_tensor\n",
    "        V_sum = np.sum(V_terms, axis=(2, 3))\n",
    "        V_numerator = np.sqrt(2*V_sum)\n",
    "        V_denominator = sigma*self.ttm_mat[np.ix_(swap_idxs, swap_idxs)]\n",
    "        V = np.divide(\n",
    "            V_numerator, \n",
    "            V_denominator,\n",
    "            out=np.zeros_like(numerator),  # fill result with 0 where denominator == 0\n",
    "            where=denominator != 0\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        omega_tensor = np.divide(V_terms, V_sum[..., None, None], out=np.zeros_like(V_terms), where=V_sum[..., None, None] != 0)\n",
    "        phi_tensor = build_swap_correlation_tensor(self.phi_mat_0m_interpolated, T_idxs=swap_idxs, resolution=self.resolution, tau=0.5, tenor=1)\n",
    "        phi = np.sum(phi_tensor * omega_tensor, axis=(2, 3))\n",
    "\n",
    "        return sigma, V, self.B, phi\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def plot(self, mat):\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        # Create mesh grid the shape of self.swap_sim\n",
    "        X = np.arange(mat.shape[0])*self.dt\n",
    "        Y = np.arange(mat.shape[1])\n",
    "        X, Y = np.meshgrid(X, Y)\n",
    "        Z = mat.T\n",
    "        # Create surface plot\n",
    "        surf = ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')\n",
    "        # Add labels and colorbar\n",
    "        ax.set_xlabel('Time Steps')\n",
    "        ax.set_ylabel('Expiry')\n",
    "        ax.set_zlabel('Rate')\n",
    "        fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "        # angle so we look down from above\n",
    "        ax.view_init(elev=45, azim=210)\n",
    "\n",
    "\n",
    "lmm = LMMSABR(\n",
    "    rho_mat=rho_mat_0m,\n",
    "    theta_mat=theta_mat_0m,\n",
    "    phi_mat=phi_mat_0m,\n",
    "    g_params=params_g,\n",
    "    h_params=params_h,\n",
    "    epsilon_exp=epsilon_exp,\n",
    "    k0_exp=s0_exp,\n",
    "    df_cap=df_cap,\n",
    "    df_raw_spot=df_raw_spot,\n",
    "    resolution=26, max_expiry=2\n",
    ")\n",
    "# print all rows and columns of the numpy array with setting printoptions to remove limits, also reduce precition to 4 \n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)\n",
    "\n",
    "lmm.simulate(seed=None)\n",
    "lmm.get_swap_matrix()\n",
    "lmm.get_sabr_params()\n",
    "#lmm.plot(lmm.f_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_run(seed):\n",
    "    lmm.simulate(seed=seed)  # use a seed if needed for reproducibility\n",
    "    lmm.get_swap_matrix()\n",
    "    return lmm.get_sabr_params()  # optionally return sigma, V, phi, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "n_jobs = -1  # uses all available CPU cores\n",
    "n_sims = 1000\n",
    "\n",
    "results = Parallel(n_jobs=n_jobs)(\n",
    "    delayed(single_run)(seed) for seed in range(n_sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.    , 0.0106, 0.0105, 0.0105, 0.0105, 0.0106, 0.0106, 0.0107, 0.0108, 0.0109, 0.011 , 0.0111, 0.0113, 0.0114, 0.0116, 0.0118, 0.012 , 0.0122, 0.0124, 0.0126, 0.0128, 0.0131, 0.0133, 0.0136, 0.0138, 0.0141, 0.0144, 0.0144, 0.0144, 0.0144, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0142, 0.0143, 0.0143, 0.0143, 0.0143, 0.0144, 0.0144, 0.0145, 0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.015 , 0.0151, 0.0152, 0.0154, 0.0156],\n",
       "        [   nan, 0.    , 0.0105, 0.0104, 0.0104, 0.0105, 0.0105, 0.0106, 0.0107, 0.0108, 0.0109, 0.011 , 0.0112, 0.0113, 0.0115, 0.0117, 0.0119, 0.0121, 0.0123, 0.0125, 0.0127, 0.013 , 0.0132, 0.0135, 0.0137, 0.014 , 0.0143, 0.0143, 0.0143, 0.0143, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0143, 0.0143, 0.0144, 0.0145, 0.0146, 0.0146, 0.0147, 0.0149, 0.015 , 0.0151, 0.0153, 0.0154, 0.0156],\n",
       "        [   nan,    nan, 0.    , 0.0111, 0.0111, 0.0111, 0.0111, 0.0112, 0.0112, 0.0113, 0.0114, 0.0116, 0.0117, 0.0118, 0.012 , 0.0122, 0.0124, 0.0126, 0.0128, 0.013 , 0.0132, 0.0134, 0.0137, 0.0139, 0.0142, 0.0145, 0.0148, 0.0148, 0.0148, 0.0147, 0.0147, 0.0146, 0.0146, 0.0146, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0146, 0.0146, 0.0146, 0.0147, 0.0147, 0.0148, 0.0149, 0.015 , 0.0151, 0.0152, 0.0153, 0.0154, 0.0156, 0.0157],\n",
       "        [   nan,    nan,    nan, 0.    , 0.0122, 0.0122, 0.0122, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0129, 0.013 , 0.0132, 0.0134, 0.0136, 0.0138, 0.0141, 0.0143, 0.0145, 0.0148, 0.0151, 0.0154, 0.0156, 0.0159, 0.0159, 0.0159, 0.0158, 0.0158, 0.0157, 0.0156, 0.0156, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0156, 0.0156, 0.0157, 0.0157, 0.0158, 0.0159, 0.016 , 0.0161, 0.0163, 0.0164],\n",
       "        [   nan,    nan,    nan,    nan, 0.    , 0.0124, 0.0124, 0.0124, 0.0125, 0.0126, 0.0127, 0.0128, 0.0129, 0.0131, 0.0132, 0.0134, 0.0136, 0.0138, 0.014 , 0.0142, 0.0145, 0.0147, 0.015 , 0.0153, 0.0155, 0.0158, 0.0161, 0.0161, 0.0161, 0.016 , 0.0159, 0.0159, 0.0158, 0.0158, 0.0157, 0.0157, 0.0157, 0.0156, 0.0156, 0.0156, 0.0156, 0.0157, 0.0157, 0.0157, 0.0158, 0.0158, 0.0159, 0.016 , 0.0161, 0.0162, 0.0163, 0.0164, 0.0166],\n",
       "        [   nan,    nan,    nan,    nan,    nan, 0.    , 0.0123, 0.0123, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0128, 0.013 , 0.0132, 0.0134, 0.0135, 0.0138, 0.014 , 0.0142, 0.0145, 0.0147, 0.015 , 0.0153, 0.0156, 0.0159, 0.0158, 0.0158, 0.0158, 0.0157, 0.0156, 0.0156, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0156, 0.0156, 0.0157, 0.0158, 0.0158, 0.0159, 0.016 , 0.0162, 0.0163, 0.0164, 0.0166],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0123, 0.0123, 0.0124, 0.0125, 0.0125, 0.0127, 0.0128, 0.0129, 0.0131, 0.0133, 0.0135, 0.0137, 0.0139, 0.0141, 0.0144, 0.0146, 0.0149, 0.0152, 0.0155, 0.0158, 0.0158, 0.0157, 0.0157, 0.0156, 0.0156, 0.0155, 0.0155, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0155, 0.0155, 0.0156, 0.0156, 0.0157, 0.0158, 0.0159, 0.016 , 0.0161, 0.0162, 0.0163, 0.0165, 0.0167],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0124, 0.0124, 0.0125, 0.0125, 0.0126, 0.0127, 0.0129, 0.013 , 0.0132, 0.0134, 0.0136, 0.0138, 0.014 , 0.0143, 0.0145, 0.0148, 0.015 , 0.0153, 0.0156, 0.0156, 0.0156, 0.0155, 0.0155, 0.0154, 0.0154, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153, 0.0154, 0.0154, 0.0155, 0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.016 , 0.0161, 0.0163, 0.0164, 0.0166],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0118, 0.0118, 0.0118, 0.0119, 0.012 , 0.0121, 0.0122, 0.0124, 0.0126, 0.0127, 0.0129, 0.0131, 0.0134, 0.0136, 0.0138, 0.0141, 0.0143, 0.0146, 0.0146, 0.0146, 0.0146, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0146, 0.0146, 0.0146, 0.0147, 0.0148, 0.0149, 0.015 , 0.0151, 0.0152, 0.0153, 0.0154, 0.0156, 0.0157, 0.0159, 0.0161],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0109, 0.011 , 0.011 , 0.0111, 0.0112, 0.0113, 0.0114, 0.0115, 0.0117, 0.0119, 0.012 , 0.0122, 0.0124, 0.0126, 0.0129, 0.0131, 0.0133, 0.0134, 0.0134, 0.0134, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0134, 0.0134, 0.0134, 0.0135, 0.0135, 0.0136, 0.0137, 0.0138, 0.0139, 0.014 , 0.0141, 0.0142, 0.0143, 0.0145, 0.0147, 0.0148, 0.015 , 0.0152],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.011 , 0.011 , 0.011 , 0.0111, 0.0112, 0.0113, 0.0114, 0.0116, 0.0117, 0.0119, 0.0121, 0.0123, 0.0125, 0.0127, 0.0129, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0133, 0.0133, 0.0134, 0.0135, 0.0135, 0.0136, 0.0137, 0.0138, 0.014 , 0.0141, 0.0142, 0.0144, 0.0145, 0.0147, 0.0149, 0.0151],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0108, 0.0108, 0.0108, 0.0109, 0.011 , 0.0111, 0.0113, 0.0114, 0.0116, 0.0118, 0.012 , 0.0122, 0.0124, 0.0126, 0.0128, 0.0128, 0.0129, 0.0129, 0.0129, 0.0129, 0.0129, 0.0129, 0.0129, 0.013 , 0.013 , 0.013 , 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136, 0.0137, 0.0138, 0.014 , 0.0141, 0.0143, 0.0144, 0.0146, 0.0148, 0.015 ],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0109, 0.0109, 0.011 , 0.011 , 0.0112, 0.0113, 0.0114, 0.0116, 0.0118, 0.012 , 0.0121, 0.0124, 0.0126, 0.0128, 0.0128, 0.0129, 0.0129, 0.0129, 0.0129, 0.0129, 0.0129, 0.0129, 0.013 , 0.013 , 0.0131, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136, 0.0137, 0.0139, 0.014 , 0.0142, 0.0144, 0.0145, 0.0147, 0.0149, 0.0151],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0116, 0.0117, 0.0118, 0.0119, 0.012 , 0.0122, 0.0123, 0.0125, 0.0127, 0.0129, 0.0131, 0.0134, 0.0136, 0.0136, 0.0137, 0.0137, 0.0136, 0.0136, 0.0137, 0.0137, 0.0137, 0.0137, 0.0138, 0.0138, 0.0139, 0.014 , 0.0141, 0.0142, 0.0143, 0.0144, 0.0145, 0.0147, 0.0148, 0.015 , 0.0152, 0.0154, 0.0155, 0.0158, 0.016 ],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0119, 0.012 , 0.0121, 0.0122, 0.0123, 0.0125, 0.0126, 0.0128, 0.013 , 0.0133, 0.0135, 0.0137, 0.0137, 0.0138, 0.0138, 0.0137, 0.0137, 0.0138, 0.0138, 0.0138, 0.0138, 0.0139, 0.014 , 0.014 , 0.0141, 0.0142, 0.0143, 0.0144, 0.0145, 0.0147, 0.0148, 0.015 , 0.0151, 0.0153, 0.0155, 0.0157, 0.0159, 0.0162],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0117, 0.0117, 0.0118, 0.0119, 0.0121, 0.0122, 0.0124, 0.0126, 0.0127, 0.013 , 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0133, 0.0133, 0.0134, 0.0134, 0.0135, 0.0136, 0.0136, 0.0137, 0.0139, 0.014 , 0.0141, 0.0142, 0.0144, 0.0145, 0.0147, 0.0149, 0.0151, 0.0153, 0.0155, 0.0158],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0115, 0.0116, 0.0116, 0.0117, 0.0119, 0.012 , 0.0122, 0.0124, 0.0126, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0129, 0.0129, 0.013 , 0.013 , 0.0131, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0137, 0.0138, 0.0139, 0.0141, 0.0143, 0.0144, 0.0146, 0.0148, 0.015 , 0.0153, 0.0155],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0122, 0.0122, 0.0123, 0.0124, 0.0126, 0.0128, 0.0129, 0.0131, 0.0133, 0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0135, 0.0135, 0.0136, 0.0136, 0.0137, 0.0138, 0.0139, 0.014 , 0.0141, 0.0142, 0.0144, 0.0145, 0.0147, 0.0149, 0.0151, 0.0153, 0.0155, 0.0157, 0.0159, 0.0162],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0123, 0.0124, 0.0125, 0.0126, 0.0128, 0.0129, 0.0131, 0.0133, 0.0133, 0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0135, 0.0135, 0.0136, 0.0137, 0.0138, 0.0138, 0.014 , 0.0141, 0.0142, 0.0143, 0.0145, 0.0147, 0.0148, 0.015 , 0.0152, 0.0154, 0.0156, 0.0159, 0.0161, 0.0164],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0133, 0.0134, 0.0135, 0.0137, 0.0138, 0.014 , 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0144, 0.0144, 0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.015 , 0.0151, 0.0153, 0.0154, 0.0156, 0.0158, 0.016 , 0.0162, 0.0164, 0.0166, 0.0169, 0.0172, 0.0174],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0131, 0.0132, 0.0133, 0.0135, 0.0137, 0.0138, 0.0139, 0.0139, 0.0139, 0.0139, 0.0139, 0.014 , 0.014 , 0.0141, 0.0141, 0.0142, 0.0143, 0.0144, 0.0145, 0.0147, 0.0148, 0.015 , 0.0151, 0.0153, 0.0155, 0.0157, 0.0159, 0.0161, 0.0164, 0.0166, 0.0169, 0.0172],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0127, 0.0127, 0.0129, 0.013 , 0.0132, 0.0132, 0.0132, 0.0133, 0.0133, 0.0133, 0.0134, 0.0134, 0.0135, 0.0136, 0.0137, 0.0138, 0.0139, 0.014 , 0.0141, 0.0143, 0.0144, 0.0146, 0.0148, 0.015 , 0.0152, 0.0154, 0.0157, 0.0159, 0.0162, 0.0164, 0.0167],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0134, 0.0135, 0.0136, 0.0138, 0.0138, 0.0138, 0.0139, 0.0139, 0.0139, 0.0139, 0.014 , 0.0141, 0.0141, 0.0142, 0.0143, 0.0144, 0.0146, 0.0147, 0.0149, 0.015 , 0.0152, 0.0154, 0.0156, 0.0158, 0.0161, 0.0163, 0.0166, 0.0168, 0.0171, 0.0174],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0137, 0.0138, 0.0139, 0.0139, 0.014 , 0.014 , 0.014 , 0.014 , 0.0141, 0.0141, 0.0142, 0.0143, 0.0144, 0.0145, 0.0146, 0.0148, 0.0149, 0.0151, 0.0153, 0.0155, 0.0157, 0.0159, 0.0161, 0.0164, 0.0166, 0.0169, 0.0172, 0.0175, 0.0178],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0135, 0.0136, 0.0136, 0.0136, 0.0137, 0.0137, 0.0137, 0.0138, 0.0138, 0.0139, 0.014 , 0.0141, 0.0142, 0.0144, 0.0145, 0.0147, 0.0148, 0.015 , 0.0152, 0.0154, 0.0156, 0.0159, 0.0161, 0.0164, 0.0166, 0.0169, 0.0172, 0.0175],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0133, 0.0133, 0.0134, 0.0134, 0.0134, 0.0134, 0.0135, 0.0135, 0.0136, 0.0137, 0.0138, 0.014 , 0.0141, 0.0142, 0.0144, 0.0146, 0.0148, 0.015 , 0.0152, 0.0154, 0.0156, 0.0159, 0.0161, 0.0164, 0.0167, 0.017 , 0.0173],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0134, 0.0134, 0.0135, 0.0136, 0.0137, 0.0138, 0.014 , 0.0141, 0.0143, 0.0145, 0.0147, 0.0149, 0.0151, 0.0153, 0.0155, 0.0158, 0.0161, 0.0163, 0.0166, 0.0169, 0.0173],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0134, 0.0134, 0.0134, 0.0134, 0.0134, 0.0135, 0.0136, 0.0137, 0.0138, 0.0139, 0.014 , 0.0141, 0.0143, 0.0145, 0.0147, 0.0149, 0.0151, 0.0153, 0.0155, 0.0158, 0.0161, 0.0163, 0.0166, 0.0169, 0.0173],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.013 , 0.013 , 0.013 , 0.013 , 0.0131, 0.0131, 0.0132, 0.0133, 0.0134, 0.0136, 0.0137, 0.0138, 0.014 , 0.0142, 0.0144, 0.0146, 0.0148, 0.0151, 0.0153, 0.0156, 0.0158, 0.0161, 0.0164, 0.0168],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0138, 0.0137, 0.0137, 0.0138, 0.0138, 0.0139, 0.014 , 0.0141, 0.0142, 0.0143, 0.0145, 0.0147, 0.0148, 0.015 , 0.0153, 0.0155, 0.0157, 0.016 , 0.0162, 0.0165, 0.0168, 0.0171, 0.0175],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0141, 0.014 , 0.014 , 0.0141, 0.0141, 0.0142, 0.0143, 0.0144, 0.0146, 0.0147, 0.0149, 0.0151, 0.0153, 0.0155, 0.0157, 0.0159, 0.0162, 0.0165, 0.0168, 0.0171, 0.0174, 0.0177],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0144, 0.0143, 0.0144, 0.0144, 0.0145, 0.0146, 0.0147, 0.0148, 0.015 , 0.0152, 0.0154, 0.0156, 0.0158, 0.016 , 0.0163, 0.0165, 0.0168, 0.0171, 0.0174, 0.0177, 0.0181],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0144, 0.0144, 0.0144, 0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0151, 0.0153, 0.0155, 0.0157, 0.0159, 0.0161, 0.0164, 0.0167, 0.0169, 0.0173, 0.0176, 0.0179],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0149, 0.0149, 0.0149, 0.015 , 0.0151, 0.0152, 0.0153, 0.0154, 0.0156, 0.0158, 0.016 , 0.0162, 0.0165, 0.0167, 0.017 , 0.0173, 0.0176, 0.0179, 0.0182],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0168, 0.0167, 0.0168, 0.0168, 0.0169, 0.0171, 0.0172, 0.0174, 0.0176, 0.0178, 0.018 , 0.0183, 0.0185, 0.0188, 0.0191, 0.0194, 0.0198, 0.0201],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0161, 0.0161, 0.0161, 0.0162, 0.0163, 0.0164, 0.0166, 0.0168, 0.017 , 0.0172, 0.0174, 0.0177, 0.018 , 0.0183, 0.0186, 0.0189, 0.0193],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0153, 0.0153, 0.0153, 0.0154, 0.0155, 0.0156, 0.0158, 0.016 , 0.0162, 0.0164, 0.0166, 0.0169, 0.0172, 0.0174, 0.0178, 0.0181],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0166, 0.0166, 0.0167, 0.0167, 0.0169, 0.017 , 0.0172, 0.0174, 0.0177, 0.0179, 0.0182, 0.0185, 0.0188, 0.0191, 0.0195],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.015 , 0.015 , 0.015 , 0.0151, 0.0152, 0.0153, 0.0155, 0.0157, 0.0159, 0.0161, 0.0163, 0.0166, 0.0169, 0.0172],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0154, 0.0154, 0.0154, 0.0155, 0.0156, 0.0158, 0.016 , 0.0161, 0.0164, 0.0166, 0.0169, 0.0171, 0.0174],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0156, 0.0156, 0.0157, 0.0158, 0.0159, 0.0161, 0.0162, 0.0164, 0.0167, 0.0169, 0.0172, 0.0175],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.016 , 0.016 , 0.016 , 0.0162, 0.0163, 0.0165, 0.0167, 0.0169, 0.0171, 0.0174, 0.0177],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0169, 0.0169, 0.017 , 0.0171, 0.0173, 0.0175, 0.0177, 0.0179, 0.0182, 0.0185],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0172, 0.0172, 0.0173, 0.0175, 0.0176, 0.0178, 0.0181, 0.0183, 0.0186],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.019 , 0.019 , 0.0192, 0.0193, 0.0195, 0.0198, 0.0201, 0.0204],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0203, 0.0204, 0.0205, 0.0208, 0.021 , 0.0213, 0.0217],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0195, 0.0196, 0.0198, 0.02  , 0.0203, 0.0206],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0192, 0.0193, 0.0195, 0.0197, 0.02  ],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0184, 0.0185, 0.0186, 0.0189],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0185, 0.0187, 0.0189],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0195, 0.0197],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.0199],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    ]]),\n",
       " array([[0.    , 0.2692, 0.2736, 0.2759, 0.2773, 0.2782, 0.2786, 0.2787, 0.2786, 0.2782, 0.2775, 0.2767, 0.2758, 0.2746, 0.2733, 0.2719, 0.2704, 0.2688, 0.267 , 0.2652, 0.2633, 0.2613, 0.2592, 0.2571, 0.255 , 0.2528, 0.2505, 0.2479, 0.245 , 0.2421, 0.2392, 0.2364, 0.2336, 0.2307, 0.2279, 0.2252, 0.2224, 0.2196, 0.2169, 0.2142, 0.2115, 0.2089, 0.2062, 0.2036, 0.201 , 0.1984, 0.1959, 0.1933, 0.1908, 0.1883, 0.1858, 0.1833, 0.1809],\n",
       "        [   nan, 0.    , 0.2645, 0.2694, 0.2724, 0.2745, 0.276 , 0.2769, 0.2774, 0.2776, 0.2774, 0.2771, 0.2764, 0.2756, 0.2746, 0.2734, 0.272 , 0.2705, 0.2689, 0.2672, 0.2653, 0.2633, 0.2613, 0.2592, 0.257 , 0.2548, 0.2525, 0.25  , 0.2471, 0.2443, 0.2415, 0.2387, 0.236 , 0.2332, 0.2305, 0.2277, 0.225 , 0.2223, 0.2196, 0.2169, 0.2142, 0.2116, 0.209 , 0.2063, 0.2038, 0.2012, 0.1986, 0.1961, 0.1936, 0.191 , 0.1886, 0.1861, 0.1836],\n",
       "        [   nan,    nan, 0.    , 0.265 , 0.2695, 0.2722, 0.2742, 0.2755, 0.2764, 0.2769, 0.2771, 0.277 , 0.2767, 0.2762, 0.2754, 0.2745, 0.2735, 0.2723, 0.2709, 0.2694, 0.2678, 0.2661, 0.2643, 0.2624, 0.2604, 0.2584, 0.2563, 0.2539, 0.2511, 0.2483, 0.2456, 0.2428, 0.24  , 0.2373, 0.2345, 0.2317, 0.2289, 0.2262, 0.2234, 0.2207, 0.218 , 0.2153, 0.2126, 0.2099, 0.2073, 0.2046, 0.202 , 0.1994, 0.1968, 0.1942, 0.1916, 0.1891, 0.1865],\n",
       "        [   nan,    nan,    nan, 0.    , 0.2657, 0.2697, 0.2721, 0.2738, 0.275 , 0.2758, 0.2763, 0.2765, 0.2765, 0.2763, 0.2758, 0.2753, 0.2745, 0.2736, 0.2725, 0.2713, 0.27  , 0.2686, 0.267 , 0.2654, 0.2637, 0.2619, 0.26  , 0.2576, 0.255 , 0.2523, 0.2496, 0.247 , 0.2442, 0.2415, 0.2387, 0.236 , 0.2332, 0.2304, 0.2277, 0.2249, 0.2222, 0.2194, 0.2167, 0.214 , 0.2113, 0.2086, 0.206 , 0.2033, 0.2007, 0.198 , 0.1954, 0.1928, 0.1902],\n",
       "        [   nan,    nan,    nan,    nan, 0.    , 0.2621, 0.2666, 0.2695, 0.2716, 0.2731, 0.2743, 0.2751, 0.2755, 0.2758, 0.2758, 0.2755, 0.2751, 0.2745, 0.2737, 0.2727, 0.2716, 0.2704, 0.269 , 0.2675, 0.2659, 0.2643, 0.2625, 0.2603, 0.2578, 0.2552, 0.2526, 0.25  , 0.2473, 0.2446, 0.2418, 0.2391, 0.2363, 0.2336, 0.2308, 0.2281, 0.2253, 0.2226, 0.2198, 0.2171, 0.2144, 0.2116, 0.2089, 0.2062, 0.2036, 0.2009, 0.1982, 0.1956, 0.1929],\n",
       "        [   nan,    nan,    nan,    nan,    nan, 0.    , 0.2602, 0.2652, 0.2684, 0.2708, 0.2726, 0.2739, 0.2748, 0.2754, 0.2757, 0.2757, 0.2755, 0.2751, 0.2745, 0.2737, 0.2727, 0.2715, 0.2703, 0.2689, 0.2674, 0.2658, 0.2641, 0.262 , 0.2596, 0.2571, 0.2547, 0.2521, 0.2495, 0.2469, 0.2443, 0.2416, 0.239 , 0.2363, 0.2336, 0.2309, 0.2282, 0.2255, 0.2228, 0.2201, 0.2174, 0.2147, 0.2121, 0.2094, 0.2067, 0.2041, 0.2014, 0.1988, 0.1961],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2576, 0.2632, 0.2669, 0.2697, 0.2718, 0.2733, 0.2745, 0.2752, 0.2756, 0.2757, 0.2755, 0.2751, 0.2745, 0.2736, 0.2726, 0.2715, 0.2702, 0.2687, 0.2672, 0.2655, 0.2635, 0.2613, 0.2589, 0.2565, 0.2541, 0.2516, 0.2491, 0.2465, 0.244 , 0.2414, 0.2388, 0.2361, 0.2335, 0.2309, 0.2282, 0.2256, 0.2229, 0.2203, 0.2176, 0.215 , 0.2124, 0.2097, 0.2071, 0.2044, 0.2018, 0.1992],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2539, 0.2601, 0.2644, 0.2676, 0.27  , 0.2719, 0.2733, 0.2743, 0.2749, 0.2752, 0.2752, 0.2749, 0.2744, 0.2737, 0.2729, 0.2718, 0.2706, 0.2692, 0.2678, 0.2659, 0.2638, 0.2616, 0.2593, 0.257 , 0.2545, 0.2521, 0.2495, 0.247 , 0.2444, 0.2418, 0.2391, 0.2365, 0.2338, 0.2311, 0.2284, 0.2258, 0.2231, 0.2204, 0.2177, 0.215 , 0.2123, 0.2095, 0.2068, 0.2041, 0.2014],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2497, 0.2569, 0.2618, 0.2656, 0.2686, 0.2708, 0.2725, 0.2738, 0.2745, 0.275 , 0.2751, 0.2749, 0.2744, 0.2738, 0.2729, 0.2718, 0.2706, 0.2692, 0.2676, 0.2656, 0.2634, 0.2613, 0.259 , 0.2566, 0.2542, 0.2517, 0.2492, 0.2467, 0.2441, 0.2414, 0.2388, 0.2361, 0.2334, 0.2307, 0.228 , 0.2253, 0.2226, 0.2199, 0.2171, 0.2144, 0.2116, 0.2089, 0.2061, 0.2034],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2457, 0.2538, 0.2596, 0.264 , 0.2675, 0.2701, 0.2721, 0.2735, 0.2744, 0.2749, 0.2751, 0.2749, 0.2745, 0.2737, 0.2728, 0.2717, 0.2704, 0.2688, 0.2669, 0.2649, 0.2627, 0.2605, 0.2582, 0.2558, 0.2534, 0.2509, 0.2483, 0.2457, 0.2431, 0.2404, 0.2378, 0.2351, 0.2323, 0.2296, 0.2268, 0.2241, 0.2213, 0.2185, 0.2157, 0.2129, 0.2101, 0.2073, 0.2045],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.244 , 0.2525, 0.2586, 0.2632, 0.2668, 0.2696, 0.2717, 0.2732, 0.2742, 0.2748, 0.275 , 0.2749, 0.2745, 0.2738, 0.2729, 0.2718, 0.2704, 0.2687, 0.2668, 0.2649, 0.2628, 0.2606, 0.2583, 0.2559, 0.2535, 0.251 , 0.2485, 0.2459, 0.2432, 0.2406, 0.2379, 0.2352, 0.2324, 0.2297, 0.2269, 0.2241, 0.2213, 0.2185, 0.2157, 0.2129, 0.21  , 0.2072],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2427, 0.2517, 0.2581, 0.263 , 0.2668, 0.2697, 0.2719, 0.2734, 0.2744, 0.275 , 0.2751, 0.275 , 0.2745, 0.2737, 0.2727, 0.2715, 0.27  , 0.2682, 0.2664, 0.2644, 0.2623, 0.2601, 0.2579, 0.2555, 0.2531, 0.2506, 0.2481, 0.2456, 0.243 , 0.2403, 0.2376, 0.2349, 0.2322, 0.2295, 0.2267, 0.2239, 0.2211, 0.2183, 0.2155, 0.2126, 0.2098],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2417, 0.2511, 0.2577, 0.2627, 0.2666, 0.2696, 0.2719, 0.2734, 0.2745, 0.275 , 0.2752, 0.275 , 0.2745, 0.2737, 0.2727, 0.2714, 0.2698, 0.2681, 0.2663, 0.2643, 0.2623, 0.2601, 0.2578, 0.2555, 0.2531, 0.2507, 0.2482, 0.2457, 0.2431, 0.2404, 0.2378, 0.2351, 0.2324, 0.2296, 0.2268, 0.224 , 0.2212, 0.2184, 0.2155, 0.2127],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2423, 0.2515, 0.2581, 0.2631, 0.2669, 0.2699, 0.272 , 0.2736, 0.2746, 0.2751, 0.2753, 0.275 , 0.2745, 0.2737, 0.2726, 0.2713, 0.2698, 0.2681, 0.2663, 0.2644, 0.2624, 0.2603, 0.2581, 0.2558, 0.2535, 0.2511, 0.2486, 0.2461, 0.2436, 0.241 , 0.2383, 0.2357, 0.233 , 0.2302, 0.2275, 0.2247, 0.2219, 0.2191, 0.2162],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2415, 0.2509, 0.2576, 0.2627, 0.2666, 0.2696, 0.2718, 0.2734, 0.2745, 0.2751, 0.2752, 0.275 , 0.2745, 0.2737, 0.2726, 0.2713, 0.2698, 0.2682, 0.2664, 0.2645, 0.2625, 0.2604, 0.2582, 0.2559, 0.2536, 0.2512, 0.2487, 0.2462, 0.2436, 0.241 , 0.2384, 0.2356, 0.2329, 0.2301, 0.2273, 0.2245, 0.2216, 0.2188],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2396, 0.2493, 0.2563, 0.2616, 0.2658, 0.2689, 0.2713, 0.273 , 0.2742, 0.2748, 0.2751, 0.275 , 0.2745, 0.2737, 0.2727, 0.2714, 0.27  , 0.2684, 0.2666, 0.2647, 0.2627, 0.2606, 0.2584, 0.2562, 0.2538, 0.2514, 0.2489, 0.2463, 0.2437, 0.241 , 0.2383, 0.2355, 0.2327, 0.2299, 0.227 , 0.2241, 0.2212],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2392, 0.2492, 0.2563, 0.2618, 0.2659, 0.2691, 0.2715, 0.2732, 0.2744, 0.275 , 0.2752, 0.2751, 0.2745, 0.2737, 0.2726, 0.2713, 0.2699, 0.2683, 0.2665, 0.2646, 0.2626, 0.2605, 0.2583, 0.2559, 0.2536, 0.2511, 0.2486, 0.246 , 0.2433, 0.2406, 0.2379, 0.2351, 0.2322, 0.2293, 0.2264, 0.2235],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2402, 0.25  , 0.257 , 0.2624, 0.2665, 0.2696, 0.2719, 0.2735, 0.2746, 0.2752, 0.2753, 0.2751, 0.2745, 0.2737, 0.2726, 0.2714, 0.27  , 0.2684, 0.2667, 0.2648, 0.2629, 0.2608, 0.2586, 0.2564, 0.254 , 0.2516, 0.2491, 0.2466, 0.244 , 0.2413, 0.2385, 0.2358, 0.2329, 0.2301, 0.2271],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.24  , 0.25  , 0.2571, 0.2625, 0.2666, 0.2698, 0.2721, 0.2737, 0.2747, 0.2752, 0.2754, 0.2751, 0.2745, 0.2737, 0.2727, 0.2714, 0.2701, 0.2685, 0.2668, 0.265 , 0.2631, 0.2611, 0.2589, 0.2567, 0.2543, 0.2519, 0.2494, 0.2469, 0.2442, 0.2415, 0.2388, 0.236 , 0.2331, 0.2302],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2438, 0.2534, 0.2601, 0.2651, 0.2689, 0.2717, 0.2737, 0.2748, 0.2755, 0.2758, 0.2756, 0.2752, 0.2745, 0.2736, 0.2726, 0.2714, 0.2701, 0.2686, 0.267 , 0.2653, 0.2634, 0.2615, 0.2595, 0.2573, 0.2551, 0.2528, 0.2504, 0.248 , 0.2454, 0.2428, 0.2401, 0.2374, 0.2346],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.245 , 0.2545, 0.2611, 0.2661, 0.2698, 0.2725, 0.274 , 0.2751, 0.2757, 0.2758, 0.2756, 0.2751, 0.2745, 0.2736, 0.2726, 0.2715, 0.2702, 0.2687, 0.2672, 0.2655, 0.2637, 0.2617, 0.2597, 0.2576, 0.2554, 0.2531, 0.2507, 0.2482, 0.2456, 0.243 , 0.2403, 0.2376],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2453, 0.2549, 0.2616, 0.2666, 0.2703, 0.2724, 0.274 , 0.2751, 0.2755, 0.2756, 0.2755, 0.2751, 0.2745, 0.2736, 0.2727, 0.2715, 0.2702, 0.2687, 0.2671, 0.2654, 0.2635, 0.2615, 0.2594, 0.2572, 0.2549, 0.2525, 0.25  , 0.2475, 0.2448, 0.2421, 0.2393],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2496, 0.2587, 0.265 , 0.2695, 0.2719, 0.2737, 0.2749, 0.2755, 0.2757, 0.2757, 0.2755, 0.2751, 0.2745, 0.2737, 0.2727, 0.2716, 0.2704, 0.269 , 0.2675, 0.2659, 0.2641, 0.2623, 0.2603, 0.2582, 0.256 , 0.2537, 0.2512, 0.2488, 0.2462, 0.2435],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2529, 0.2616, 0.2676, 0.2704, 0.2726, 0.2741, 0.2749, 0.2754, 0.2756, 0.2756, 0.2754, 0.275 , 0.2744, 0.2737, 0.2728, 0.2718, 0.2706, 0.2693, 0.2679, 0.2663, 0.2646, 0.2627, 0.2608, 0.2587, 0.2565, 0.2542, 0.2518, 0.2493, 0.2468],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2567, 0.265 , 0.2685, 0.2711, 0.273 , 0.274 , 0.2748, 0.2752, 0.2754, 0.2754, 0.2753, 0.2749, 0.2744, 0.2737, 0.2729, 0.272 , 0.2709, 0.2696, 0.2682, 0.2667, 0.265 , 0.2632, 0.2613, 0.2593, 0.2571, 0.2548, 0.2524, 0.2499],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2611, 0.2661, 0.2695, 0.2718, 0.2731, 0.2741, 0.2747, 0.2751, 0.2754, 0.2754, 0.2752, 0.2749, 0.2744, 0.2738, 0.273 , 0.272 , 0.2709, 0.2697, 0.2683, 0.2668, 0.2651, 0.2633, 0.2614, 0.2593, 0.2571, 0.2548, 0.2524],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2644, 0.2687, 0.2712, 0.2727, 0.2737, 0.2744, 0.2749, 0.2752, 0.2753, 0.2753, 0.2751, 0.2748, 0.2744, 0.2738, 0.2731, 0.2722, 0.2712, 0.27  , 0.2687, 0.2672, 0.2656, 0.2639, 0.262 , 0.26  , 0.2579, 0.2556],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2613, 0.2659, 0.2686, 0.2704, 0.2719, 0.2729, 0.2737, 0.2743, 0.2747, 0.2748, 0.2749, 0.2747, 0.2744, 0.2739, 0.2732, 0.2724, 0.2714, 0.2702, 0.2689, 0.2674, 0.2658, 0.264 , 0.262 , 0.26  , 0.2578],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2548, 0.2603, 0.264 , 0.2668, 0.269 , 0.2707, 0.2721, 0.2731, 0.2738, 0.2743, 0.2746, 0.2746, 0.2743, 0.2739, 0.2733, 0.2724, 0.2714, 0.2702, 0.2687, 0.2672, 0.2654, 0.2635, 0.2614, 0.2592],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2546, 0.2599, 0.2635, 0.2663, 0.2685, 0.2702, 0.2716, 0.2727, 0.2735, 0.2741, 0.2744, 0.2745, 0.2743, 0.274 , 0.2734, 0.2727, 0.2717, 0.2706, 0.2693, 0.2677, 0.2661, 0.2642, 0.2622],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2501, 0.2562, 0.2604, 0.2637, 0.2663, 0.2685, 0.2702, 0.2716, 0.2727, 0.2735, 0.274 , 0.2743, 0.2743, 0.2741, 0.2736, 0.2729, 0.272 , 0.2709, 0.2696, 0.2681, 0.2664, 0.2645],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2462, 0.253 , 0.2578, 0.2616, 0.2647, 0.2672, 0.2693, 0.271 , 0.2723, 0.2732, 0.2739, 0.2742, 0.2743, 0.2741, 0.2736, 0.2729, 0.272 , 0.2708, 0.2695, 0.2679, 0.2661],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2422, 0.2497, 0.2551, 0.2595, 0.263 , 0.2659, 0.2683, 0.2702, 0.2717, 0.2729, 0.2737, 0.2741, 0.2743, 0.2742, 0.2737, 0.273 , 0.2721, 0.2709, 0.2695, 0.2679],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2389, 0.247 , 0.2528, 0.2575, 0.2614, 0.2646, 0.2672, 0.2694, 0.2711, 0.2725, 0.2734, 0.274 , 0.2743, 0.2742, 0.2739, 0.2733, 0.2724, 0.2713, 0.2699],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2373, 0.2454, 0.2512, 0.256 , 0.26  , 0.2633, 0.2662, 0.2685, 0.2704, 0.2719, 0.273 , 0.2738, 0.2743, 0.2744, 0.2742, 0.2738, 0.273 , 0.272 ],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.232 , 0.2414, 0.2482, 0.2538, 0.2584, 0.2623, 0.2655, 0.2681, 0.2702, 0.2719, 0.2731, 0.2739, 0.2743, 0.2743, 0.2741, 0.2735, 0.2726],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2277, 0.2381, 0.2458, 0.2521, 0.2573, 0.2615, 0.265 , 0.2679, 0.2702, 0.2719, 0.2731, 0.2739, 0.2743, 0.2743, 0.2739, 0.2732],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2268, 0.2374, 0.2453, 0.2517, 0.257 , 0.2613, 0.2649, 0.2678, 0.2702, 0.2719, 0.2732, 0.2739, 0.2743, 0.2743, 0.2738],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2246, 0.2359, 0.2444, 0.2512, 0.2568, 0.2614, 0.2651, 0.2681, 0.2704, 0.2722, 0.2734, 0.2741, 0.2743, 0.2742],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2235, 0.2352, 0.2438, 0.2508, 0.2565, 0.2612, 0.265 , 0.2681, 0.2705, 0.2722, 0.2734, 0.2741, 0.2743],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2223, 0.2343, 0.2432, 0.2504, 0.2562, 0.261 , 0.2649, 0.268 , 0.2704, 0.2722, 0.2734, 0.2741],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2225, 0.2346, 0.2437, 0.2509, 0.2568, 0.2616, 0.2655, 0.2685, 0.2708, 0.2725, 0.2737],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2231, 0.2353, 0.2443, 0.2515, 0.2574, 0.2621, 0.2659, 0.2689, 0.2712, 0.2728],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2222, 0.2347, 0.244 , 0.2514, 0.2574, 0.2622, 0.2661, 0.2691, 0.2714],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2251, 0.2373, 0.2464, 0.2535, 0.2593, 0.2639, 0.2675, 0.2703],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.229 , 0.2409, 0.2496, 0.2565, 0.2619, 0.2662, 0.2694],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2299, 0.2419, 0.2507, 0.2576, 0.263 , 0.2672],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2337, 0.2454, 0.2539, 0.2605, 0.2656],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2353, 0.247 , 0.2555, 0.262 ],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2404, 0.2516, 0.2596],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2497, 0.2598],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    , 0.2569],\n",
       "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.    ]]),\n",
       " 0.5,\n",
       " array([[ 0.    , -0.2335, -0.2424, -0.2552, -0.2677, -0.2798, -0.2915, -0.3029, -0.3138, -0.3244, -0.3346, -0.3444, -0.3539, -0.363 , -0.3718, -0.3802, -0.3883, -0.3961, -0.4036, -0.4109, -0.4178, -0.4244, -0.4308, -0.4369, -0.4427, -0.4483, -0.4536, -0.461 , -0.476 , -0.4902, -0.4967, -0.5035, -0.5107, -0.5183, -0.5263, -0.5347, -0.5435, -0.5527, -0.5623, -0.5723, -0.5827, -0.5936, -0.6048, -0.6165, -0.6286, -0.641 , -0.6539, -0.6671, -0.6806, -0.6945, -0.7088, -0.7233, -0.7381],\n",
       "        [    nan,  0.    , -0.2429, -0.2559, -0.2685, -0.2808, -0.2926, -0.3041, -0.3152, -0.3259, -0.3362, -0.3462, -0.3558, -0.3651, -0.3741, -0.3828, -0.3912, -0.3993, -0.4072, -0.4148, -0.4221, -0.4292, -0.436 , -0.4426, -0.4489, -0.455 , -0.4608, -0.4682, -0.4831, -0.4968, -0.5026, -0.5088, -0.5155, -0.5225, -0.53  , -0.5379, -0.5463, -0.5551, -0.5643, -0.574 , -0.5841, -0.5947, -0.6057, -0.6172, -0.6291, -0.6414, -0.6541, -0.6672, -0.6806, -0.6945, -0.7087, -0.7232, -0.738 ],\n",
       "        [    nan,     nan,  0.    , -0.2569, -0.2698, -0.2824, -0.2945, -0.3062, -0.3174, -0.3282, -0.3386, -0.3485, -0.3581, -0.3673, -0.3761, -0.3845, -0.3926, -0.4003, -0.4078, -0.4149, -0.4217, -0.4282, -0.4345, -0.4405, -0.4461, -0.4515, -0.4567, -0.4639, -0.4786, -0.4924, -0.4984, -0.5049, -0.5117, -0.519 , -0.5268, -0.5349, -0.5435, -0.5525, -0.562 , -0.5719, -0.5822, -0.593 , -0.6042, -0.6158, -0.6279, -0.6403, -0.6532, -0.6664, -0.68  , -0.694 , -0.7083, -0.723 , -0.7379],\n",
       "        [    nan,     nan,     nan,  0.    , -0.2714, -0.2842, -0.2966, -0.3085, -0.32  , -0.3309, -0.3413, -0.3513, -0.3607, -0.3698, -0.3783, -0.3865, -0.3942, -0.4016, -0.4086, -0.4152, -0.4215, -0.4274, -0.4331, -0.4384, -0.4434, -0.448 , -0.4524, -0.4593, -0.4738, -0.4875, -0.4936, -0.5002, -0.5073, -0.5147, -0.5226, -0.531 , -0.5397, -0.5489, -0.5586, -0.5687, -0.5792, -0.5902, -0.6015, -0.6134, -0.6256, -0.6383, -0.6514, -0.6648, -0.6787, -0.6929, -0.7074, -0.7223, -0.7375],\n",
       "        [    nan,     nan,     nan,     nan,  0.    , -0.2862, -0.2987, -0.3107, -0.3223, -0.3333, -0.3438, -0.3538, -0.3633, -0.3724, -0.3809, -0.3891, -0.3968, -0.4041, -0.4111, -0.4177, -0.4239, -0.4298, -0.4354, -0.4406, -0.4456, -0.4502, -0.4545, -0.4613, -0.4757, -0.4892, -0.4952, -0.5016, -0.5085, -0.5158, -0.5236, -0.5318, -0.5404, -0.5496, -0.5591, -0.5691, -0.5796, -0.5905, -0.6018, -0.6136, -0.6258, -0.6384, -0.6515, -0.6649, -0.6787, -0.6929, -0.7075, -0.7223, -0.7375],\n",
       "        [    nan,     nan,     nan,     nan,     nan,  0.    , -0.3015, -0.3136, -0.3254, -0.3367, -0.3475, -0.3578, -0.3677, -0.377 , -0.3859, -0.3944, -0.4024, -0.4101, -0.4173, -0.4242, -0.4307, -0.4369, -0.4427, -0.4482, -0.4534, -0.4583, -0.4628, -0.4695, -0.4836, -0.4967, -0.5019, -0.5075, -0.5137, -0.5204, -0.5276, -0.5352, -0.5434, -0.552 , -0.5612, -0.5708, -0.5809, -0.5914, -0.6025, -0.614 , -0.626 , -0.6385, -0.6514, -0.6647, -0.6785, -0.6926, -0.7072, -0.7221, -0.7373],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.3171, -0.3288, -0.3402, -0.3512, -0.3617, -0.3718, -0.3814, -0.3906, -0.3994, -0.4078, -0.4158, -0.4234, -0.4307, -0.4376, -0.4442, -0.4504, -0.4564, -0.462 , -0.4673, -0.4723, -0.479 , -0.4928, -0.5053, -0.5097, -0.5146, -0.52  , -0.526 , -0.5325, -0.5396, -0.5472, -0.5553, -0.564 , -0.5731, -0.5828, -0.5931, -0.6038, -0.615 , -0.6268, -0.639 , -0.6517, -0.6649, -0.6786, -0.6926, -0.7071, -0.722 , -0.7373],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.3331, -0.3441, -0.3549, -0.3654, -0.3754, -0.3849, -0.3941, -0.4027, -0.411 , -0.4189, -0.4264, -0.4335, -0.4403, -0.4468, -0.4529, -0.4587, -0.4641, -0.4692, -0.474 , -0.4807, -0.4946, -0.507 , -0.5113, -0.5161, -0.5215, -0.5274, -0.5339, -0.5409, -0.5485, -0.5566, -0.5652, -0.5744, -0.584 , -0.5942, -0.6049, -0.6161, -0.6278, -0.64  , -0.6527, -0.6658, -0.6793, -0.6933, -0.7076, -0.7224, -0.7375],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.3493, -0.3595, -0.3698, -0.3797, -0.3893, -0.3985, -0.4073, -0.4157, -0.4238, -0.4315, -0.4389, -0.446 , -0.4527, -0.4591, -0.4652, -0.4709, -0.4764, -0.4815, -0.4883, -0.5021, -0.5143, -0.5181, -0.5224, -0.5274, -0.5329, -0.539 , -0.5457, -0.5529, -0.5607, -0.569 , -0.5779, -0.5873, -0.5972, -0.6077, -0.6186, -0.6301, -0.6421, -0.6545, -0.6674, -0.6807, -0.6944, -0.7085, -0.723 , -0.7379],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.3658, -0.3752, -0.3849, -0.3944, -0.4036, -0.4125, -0.4212, -0.4296, -0.4377, -0.4455, -0.453 , -0.4602, -0.4672, -0.4739, -0.4804, -0.4865, -0.4924, -0.4995, -0.5134, -0.5254, -0.5285, -0.5323, -0.5367, -0.5417, -0.5473, -0.5535, -0.5603, -0.5676, -0.5755, -0.584 , -0.593 , -0.6025, -0.6126, -0.6232, -0.6343, -0.6458, -0.6578, -0.6703, -0.6832, -0.6965, -0.7102, -0.7242, -0.7386],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.383 , -0.3916, -0.4007, -0.4097, -0.4185, -0.4271, -0.4354, -0.4434, -0.4512, -0.4587, -0.466 , -0.4729, -0.4796, -0.4861, -0.4922, -0.4981, -0.5052, -0.5189, -0.5306, -0.5332, -0.5366, -0.5406, -0.5452, -0.5505, -0.5564, -0.5629, -0.57  , -0.5777, -0.5859, -0.5947, -0.6041, -0.614 , -0.6244, -0.6353, -0.6467, -0.6586, -0.671 , -0.6838, -0.697 , -0.7105, -0.7245, -0.7387],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.4006, -0.4085, -0.4171, -0.4258, -0.4344, -0.4429, -0.4512, -0.4593, -0.4671, -0.4748, -0.4822, -0.4894, -0.4964, -0.5031, -0.5095, -0.5167, -0.5302, -0.5413, -0.543 , -0.5455, -0.5487, -0.5526, -0.5571, -0.5624, -0.5683, -0.5748, -0.5819, -0.5897, -0.5981, -0.607 , -0.6165, -0.6266, -0.6372, -0.6483, -0.66  , -0.6721, -0.6846, -0.6976, -0.711 , -0.7248, -0.7389],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.4186, -0.4257, -0.4338, -0.4421, -0.4505, -0.4587, -0.4668, -0.4748, -0.4826, -0.4902, -0.4976, -0.5048, -0.5118, -0.5185, -0.5256, -0.5389, -0.5494, -0.5504, -0.5521, -0.5546, -0.5579, -0.5619, -0.5666, -0.572 , -0.5781, -0.5848, -0.5922, -0.6002, -0.6089, -0.6181, -0.628 , -0.6383, -0.6493, -0.6607, -0.6727, -0.6851, -0.698 , -0.7113, -0.7249, -0.739 ],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.4373, -0.4438, -0.4514, -0.4594, -0.4675, -0.4755, -0.4834, -0.4912, -0.4988, -0.5062, -0.5135, -0.5205, -0.5273, -0.5343, -0.5471, -0.557 , -0.5571, -0.558 , -0.5598, -0.5624, -0.5657, -0.5699, -0.5747, -0.5804, -0.5867, -0.5937, -0.6014, -0.6097, -0.6187, -0.6283, -0.6385, -0.6493, -0.6606, -0.6725, -0.6849, -0.6977, -0.711 , -0.7247, -0.7389],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.4562, -0.4618, -0.4688, -0.4763, -0.4839, -0.4915, -0.499 , -0.5065, -0.5138, -0.5209, -0.5279, -0.5346, -0.5415, -0.5541, -0.5636, -0.5631, -0.5635, -0.5648, -0.5669, -0.5699, -0.5737, -0.5783, -0.5836, -0.5896, -0.5964, -0.6038, -0.6119, -0.6207, -0.6301, -0.6401, -0.6507, -0.6619, -0.6736, -0.6858, -0.6985, -0.7116, -0.7252, -0.7391],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.4752, -0.4797, -0.4859, -0.4927, -0.4997, -0.5068, -0.5139, -0.5209, -0.5278, -0.5345, -0.5411, -0.5478, -0.5602, -0.5693, -0.5682, -0.5682, -0.5691, -0.5709, -0.5735, -0.577 , -0.5813, -0.5864, -0.5922, -0.5988, -0.6061, -0.614 , -0.6226, -0.6319, -0.6418, -0.6522, -0.6632, -0.6747, -0.6868, -0.6993, -0.7123, -0.7256, -0.7394],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.495 , -0.4986, -0.5043, -0.5107, -0.5175, -0.5244, -0.5314, -0.5383, -0.5452, -0.552 , -0.5587, -0.5709, -0.5794, -0.5775, -0.5767, -0.5769, -0.5781, -0.5801, -0.5831, -0.5869, -0.5915, -0.5969, -0.6031, -0.61  , -0.6176, -0.6259, -0.6348, -0.6444, -0.6545, -0.6652, -0.6765, -0.6883, -0.7005, -0.7132, -0.7263, -0.7398],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.5156, -0.5185, -0.5237, -0.5299, -0.5364, -0.5431, -0.5499, -0.5568, -0.5635, -0.57  , -0.5815, -0.5892, -0.5861, -0.5843, -0.5835, -0.5838, -0.585 , -0.5873, -0.5904, -0.5944, -0.5993, -0.605 , -0.6115, -0.6187, -0.6267, -0.6353, -0.6447, -0.6546, -0.6652, -0.6763, -0.688 , -0.7003, -0.713 , -0.7261, -0.7396],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.5363, -0.5383, -0.543 , -0.5487, -0.5549, -0.5614, -0.5681, -0.5748, -0.5809, -0.5919, -0.5987, -0.5946, -0.5917, -0.5901, -0.5896, -0.5902, -0.5918, -0.5943, -0.5979, -0.6023, -0.6076, -0.6137, -0.6206, -0.6283, -0.6367, -0.6458, -0.6556, -0.666 , -0.677 , -0.6885, -0.7006, -0.7132, -0.7263, -0.7397],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.5586, -0.5608, -0.5658, -0.5719, -0.5786, -0.5857, -0.593 , -0.5989, -0.6091, -0.6146, -0.6087, -0.6041, -0.6009, -0.5989, -0.5981, -0.5984, -0.5999, -0.6023, -0.6058, -0.6102, -0.6156, -0.6219, -0.629 , -0.6369, -0.6457, -0.6551, -0.6653, -0.6762, -0.6877, -0.6998, -0.7124, -0.7256, -0.7393],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.5807, -0.5824, -0.5872, -0.5932, -0.6   , -0.6071, -0.6127, -0.6223, -0.6269, -0.6197, -0.614 , -0.6097, -0.6067, -0.6051, -0.6046, -0.6053, -0.6071, -0.61  , -0.6139, -0.6188, -0.6246, -0.6314, -0.6389, -0.6474, -0.6565, -0.6665, -0.6771, -0.6884, -0.7003, -0.7128, -0.7259, -0.7395],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.603 , -0.604 , -0.6084, -0.6142, -0.6208, -0.6261, -0.6351, -0.6389, -0.6305, -0.6239, -0.6188, -0.6151, -0.6128, -0.6117, -0.6119, -0.6132, -0.6157, -0.6192, -0.6237, -0.6292, -0.6355, -0.6428, -0.6509, -0.6597, -0.6693, -0.6796, -0.6906, -0.7021, -0.7143, -0.7269, -0.7401],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.6271, -0.6286, -0.6336, -0.6402, -0.6451, -0.6533, -0.6556, -0.6454, -0.6369, -0.6301, -0.6249, -0.6211, -0.6188, -0.6178, -0.618 , -0.6195, -0.6221, -0.6259, -0.6307, -0.6365, -0.6433, -0.651 , -0.6595, -0.6689, -0.679 , -0.6899, -0.7014, -0.7136, -0.7264, -0.7397],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.6515, -0.6533, -0.6587, -0.6631, -0.6703, -0.6713, -0.6593, -0.6493, -0.641 , -0.6344, -0.6295, -0.626 , -0.624 , -0.6233, -0.624 , -0.6259, -0.629 , -0.6333, -0.6386, -0.645 , -0.6523, -0.6605, -0.6696, -0.6795, -0.6902, -0.7016, -0.7137, -0.7264, -0.7397],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.6766, -0.6789, -0.6823, -0.6884, -0.6879, -0.6741, -0.6624, -0.6527, -0.6447, -0.6384, -0.6338, -0.6307, -0.6291, -0.6289, -0.6301, -0.6325, -0.6362, -0.641 , -0.6469, -0.6538, -0.6617, -0.6705, -0.6802, -0.6907, -0.7019, -0.7139, -0.7265, -0.7398],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7025, -0.7034, -0.7082, -0.7062, -0.6906, -0.6773, -0.6661, -0.6568, -0.6493, -0.6435, -0.6394, -0.6369, -0.6358, -0.6362, -0.6379, -0.6409, -0.6451, -0.6504, -0.6569, -0.6643, -0.6727, -0.682 , -0.6921, -0.7031, -0.7147, -0.7271, -0.7401],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7285, -0.7307, -0.7271, -0.7095, -0.6943, -0.6814, -0.6704, -0.6613, -0.6541, -0.6486, -0.6448, -0.6426, -0.6419, -0.6427, -0.6448, -0.6482, -0.6529, -0.6588, -0.6657, -0.6737, -0.6827, -0.6925, -0.7033, -0.7148, -0.7271, -0.74  ],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7532, -0.7448, -0.7243, -0.7071, -0.6926, -0.6803, -0.6701, -0.662 , -0.6556, -0.6511, -0.6482, -0.647 , -0.6472, -0.6489, -0.6519, -0.6562, -0.6616, -0.6682, -0.6759, -0.6845, -0.6941, -0.7045, -0.7158, -0.7278, -0.7404],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7669, -0.74  , -0.7195, -0.7027, -0.6888, -0.6775, -0.6685, -0.6616, -0.6565, -0.6533, -0.6517, -0.6517, -0.6531, -0.6559, -0.6599, -0.6652, -0.6715, -0.6789, -0.6872, -0.6965, -0.7065, -0.7174, -0.729 , -0.7412],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.766 , -0.7401, -0.7202, -0.704 , -0.6907, -0.6799, -0.6714, -0.665 , -0.6605, -0.6578, -0.6568, -0.6573, -0.6593, -0.6626, -0.6672, -0.673 , -0.68  , -0.6879, -0.6969, -0.7067, -0.7174, -0.7289, -0.7411],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7636, -0.7374, -0.7178, -0.702 , -0.6894, -0.6794, -0.6717, -0.6662, -0.6626, -0.6609, -0.6608, -0.6623, -0.6652, -0.6694, -0.6749, -0.6815, -0.6892, -0.6979, -0.7076, -0.7181, -0.7294, -0.7414],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7614, -0.7355, -0.7163, -0.7013, -0.6895, -0.6805, -0.6738, -0.6693, -0.6668, -0.666 , -0.6669, -0.6692, -0.673 , -0.678 , -0.6843, -0.6916, -0.6999, -0.7092, -0.7193, -0.7303, -0.7419],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7594, -0.7337, -0.7151, -0.7008, -0.6899, -0.6818, -0.6761, -0.6726, -0.671 , -0.6712, -0.6729, -0.6762, -0.6808, -0.6866, -0.6935, -0.7015, -0.7105, -0.7203, -0.731 , -0.7424],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7579, -0.7326, -0.7146, -0.7011, -0.6909, -0.6836, -0.6787, -0.676 , -0.6753, -0.6763, -0.6789, -0.6829, -0.6882, -0.6948, -0.7025, -0.7111, -0.7208, -0.7312, -0.7425],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7571, -0.7326, -0.7152, -0.7022, -0.6926, -0.6859, -0.6816, -0.6796, -0.6795, -0.6812, -0.6845, -0.6892, -0.6953, -0.7027, -0.7111, -0.7206, -0.731 , -0.7423],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7551, -0.731 , -0.7146, -0.7028, -0.6945, -0.6891, -0.6861, -0.6853, -0.6864, -0.6891, -0.6934, -0.699 , -0.7058, -0.7138, -0.7227, -0.7326, -0.7433],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7537, -0.7303, -0.715 , -0.7044, -0.6973, -0.6931, -0.6913, -0.6916, -0.6937, -0.6974, -0.7025, -0.7088, -0.7162, -0.7247, -0.734 , -0.7442],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7536, -0.7316, -0.7174, -0.7078, -0.7017, -0.6984, -0.6975, -0.6985, -0.7013, -0.7057, -0.7113, -0.7182, -0.7262, -0.7351, -0.7448],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7531, -0.7325, -0.7197, -0.7114, -0.7065, -0.7043, -0.7043, -0.7063, -0.7098, -0.7148, -0.721 , -0.7283, -0.7366, -0.7458],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7532, -0.734 , -0.7224, -0.7152, -0.7112, -0.7099, -0.7107, -0.7134, -0.7176, -0.7231, -0.7299, -0.7377, -0.7464],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7534, -0.7356, -0.7251, -0.719 , -0.716 , -0.7155, -0.7171, -0.7205, -0.7253, -0.7315, -0.7387, -0.747 ],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.754 , -0.738 , -0.7288, -0.7238, -0.7218, -0.7222, -0.7245, -0.7284, -0.7338, -0.7404, -0.748 ],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7548, -0.7405, -0.7326, -0.7286, -0.7275, -0.7286, -0.7316, -0.7361, -0.742 , -0.7489],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7553, -0.7425, -0.7358, -0.7329, -0.7327, -0.7347, -0.7383, -0.7435, -0.7498],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7566, -0.7457, -0.7405, -0.7385, -0.7391, -0.7416, -0.7458, -0.7513],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.758 , -0.7491, -0.7451, -0.7441, -0.7454, -0.7485, -0.7529],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7589, -0.7515, -0.7487, -0.7486, -0.7506, -0.7542],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7601, -0.7544, -0.7527, -0.7534, -0.756 ],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.761 , -0.7567, -0.756 , -0.7575],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7622, -0.7593, -0.7595],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7634, -0.7619],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    , -0.7643],\n",
       "        [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.    ]]))"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SABR():\n",
    "    def __init__(self, sigma_0, beta, rho, volvol, K, r, q):\n",
    "        self.sigma_0 = sigma_0\n",
    "        self.beta = beta\n",
    "        self.rho = rho\n",
    "        self.volvol = volvol\n",
    "        self.K = K\n",
    "        self.r = r\n",
    "        self.q = q\n",
    "\n",
    "    def get_implied_vol(self, tt, price):\n",
    "        \"\"\"Convert SABR instantaneous vol to option implied vol\n",
    "\n",
    "        Args:\n",
    "            tt (np.ndarray): time to maturity in shape (num_period,)\n",
    "            price (np.ndarray): underlying stock price in shape (num_path, num_period)\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: implied vol in shape (num_path, num_period)\n",
    "        \"\"\"\n",
    "        return self._sabr_implied_vol(self.sigma_0 * np.ones_like(price), tt, price)\n",
    "    def _sabr_implied_vol(self, vol, tt, price):\n",
    "            \"\"\"Convert SABR instantaneous vol to option implied vol\n",
    "\n",
    "            Args:\n",
    "                vol (np.ndarray): SABR instantaneous vol in shape (num_path, num_period)\n",
    "                tt (np.ndarray): time to maturity in shape (num_period,)\n",
    "                price (np.ndarray): underlying stock price in shape (num_path, num_period)\n",
    "\n",
    "            Returns:\n",
    "                np.ndarray: implied vol in shape (num_path, num_period)\n",
    "            \"\"\"\n",
    "            F = price * np.exp((self.r - self.q) * tt)\n",
    "            x = (F * self.K) ** ((1 - self.beta) / 2)\n",
    "            y = (1 - self.beta) * np.log(F / self.K)\n",
    "            A = vol / (x * (1 + y * y / 24 + y * y * y * y / 1920))\n",
    "            B = 1 + tt * (\n",
    "                    ((1 - self.beta) ** 2) * (vol * vol) / (24 * x * x)\n",
    "                    + self.rho * self.beta * self.volvol * vol / (4 * x)\n",
    "                    + self.volvol * self.volvol * (2 - 3 * self.rho * self.rho) / 24\n",
    "            )\n",
    "            Phi = (self.volvol * x / vol) * np.log(F / self.K)\n",
    "            # print(\"CHI INTERNAL\")\n",
    "            # print((np.sqrt(1 - 2 * self.rho * Phi + Phi * Phi) + Phi - self.rho) / (1 - self.rho))\n",
    "            Chi = np.log((np.sqrt(1 - 2 * self.rho * Phi + Phi * Phi) + Phi - self.rho) / (1 - self.rho))\n",
    "            # print(\"JUST CHI\")\n",
    "            # print(Chi)\n",
    "            Chi = Chi\n",
    "\n",
    "            epsilon = 1e-12\n",
    "\n",
    "            # if abs(F - self.K) < epsilon:\n",
    "            #     # ATM case — guard against F being 0\n",
    "            #     safe_F = max(F, epsilon)\n",
    "            #     SABRIV = vol * B / (safe_F ** (1 - self.beta))\n",
    "            # else:\n",
    "            #     # non-ATM case — guard against Chi being too small\n",
    "            #     safe_Chi = Chi if abs(Chi) > epsilon else epsilon\n",
    "            #     SABRIV = A * B * Phi / safe_Chi\n",
    "            SABRIV = np.where(np.isclose(F, self.K), vol * B / (F ** (1 - self.beta)), A * B * Phi / Chi)\n",
    "\n",
    "            return SABRIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "self.resolution = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lmm = LMMSABR(rho_mat_0m, theta_mat_0m, phi_mat, params_g, params_h, epsilon_exp, k0_exp=s0_exp, yell=False)\n",
    "lmm.simulate(self.resolution=self.resolution, max_expiry=10, tenor=1)\n",
    "\n",
    "\n",
    "#V_sum*100\n",
    "\n",
    "np.sum(omega_tensor, axis=(2, 3))\n",
    "phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
