{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "from scipy import interpolate\n",
    "from functools import partial\n",
    "from scipy.stats import norm\n",
    "import ipympl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def doust_corr(beta, n):\n",
    "    '''\n",
    "    create nxn doust correlation with beta decay exponential\n",
    "    n = # of semi-annual expiries\n",
    "    '''\n",
    "    tau = np.arange(0, n+1)/2 # start from spot\n",
    "    a = np.exp(- beta / np.arange(1, len(tau[:-1])+1) )\n",
    "    doust = np.zeros((n, n))\n",
    "    dim = doust.shape\n",
    "    for i in range(doust.shape[0]):\n",
    "        for j in range(doust.shape[1]):\n",
    "            if i == j:\n",
    "                doust[i, j] = 1\n",
    "            elif i > j:\n",
    "                doust[i, j] = np.prod(a[j:i])\n",
    "    #reflect\n",
    "    doust[np.triu_indices(dim[0], 1)] = doust.T[np.triu_indices(dim[0], 1)]\n",
    "    return(doust)\n",
    "\n",
    "\n",
    "def interpolate_correlation_matrix(matrix: np.ndarray, resolution: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Interpolates a correlation matrix using bilinear interpolation.\n",
    "\n",
    "    Args:\n",
    "        matrix (np.ndarray): The input correlation matrix (must be square).\n",
    "        resolution (int): The resolution factor. For a 4x4 and resolution=2, output will be 7x7.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Interpolated correlation matrix.\n",
    "    \"\"\"\n",
    "    if matrix.shape[0] != matrix.shape[1]:\n",
    "        raise ValueError(\"Input must be a square matrix.\")\n",
    "\n",
    "    # Compute zoom factor: new_size = original_size + (original_size - 1) * (resolution - 1)\n",
    "    zoom_factor = resolution\n",
    "\n",
    "    # Use order=1 for bilinear interpolation\n",
    "    interpolated = zoom(matrix, zoom=zoom_factor, order=1)\n",
    "\n",
    "    # Adjust shape to match expected output: new size = original + (n-1)*(res-1)\n",
    "    target_size = matrix.shape[0] + (matrix.shape[0] - 1) * (resolution - 1)\n",
    "    interpolated = interpolated[:target_size, :target_size]\n",
    "\n",
    "    return interpolated\n",
    "\n",
    "\n",
    "\n",
    "def get_instant_vol_func(tau , params):\n",
    "    '''\n",
    "    Return the instantaneous volatility ,\n",
    "    computed in terms of the parametric\n",
    "    form proposed by Rebonato , at a given time t.\n",
    "    @var t: time at which we want to compute the\n",
    "    instantaneous volatility (in years)\n",
    "    @var expiry: caplet expiry (in years)\n",
    "    @var a: parameter a of Rebonato ’s instant. vol. function\n",
    "    @var b: parameter b of Rebonato ’s instant. vol. function\n",
    "    @var c: parameter c of Rebonato ’s instant. vol. function\n",
    "    @var d: parameter d of Rebonato ’s instant. vol. function\n",
    "    \n",
    "    #g(T - t) & h(T - t)\n",
    "    '''\n",
    "    tau = np.maximum(tau, 0)\n",
    "    a,b,c,d = params\n",
    "    instantaneous_vol = (a + b * tau) * np.exp(-c * tau) + d\n",
    "    return instantaneous_vol\n",
    "\n",
    "\n",
    "def build_phi_matrix(T, phi_diag, lambda3, lambda4, add_spot=True):\n",
    "    n = len(T)\n",
    "    phi = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            Ti, Tj = T[i], T[j]\n",
    "            phi_ii, phi_jj = phi_diag[i], phi_diag[j]\n",
    "            A = np.sign(phi_ii) * np.sqrt(abs(phi_ii * phi_jj))\n",
    "            decay = np.exp(-lambda3 * max(Ti - Tj, 0) - lambda4 * max(Tj - Ti, 0))\n",
    "            phi[i, j] = A * decay\n",
    "\n",
    "    return phi\n",
    "\n",
    "\n",
    "def black_price(F, K, sigma, T, r=0.0, option_type=\"call\"):\n",
    "    \"\"\"\n",
    "    Black's formula for European options on forwards.\n",
    "\n",
    "    Args:\n",
    "        F (float): Forward rate\n",
    "        K (float): Strike\n",
    "        sigma (float): Implied volatility\n",
    "        T (float): Time to maturity\n",
    "        r (float): Discount rate (e.g. risk-free rate)\n",
    "        option_type (str): 'call' or 'put'\n",
    "\n",
    "    Returns:\n",
    "        float: Present value of the option\n",
    "    \"\"\"\n",
    "    print(\"T AND SIGMA\",T, sigma)\n",
    "    if np.isclose(T,0) or np.isclose(sigma,0):\n",
    "        print(\"WE AT MATURITY\")\n",
    "        intrinsic = max(F - K, 0) if option_type == \"call\" else max(K - F, 0)\n",
    "        return np.exp(-r * T) * intrinsic\n",
    "\n",
    "    d1 = (np.log(F / K) + 0.5 * sigma ** 2 * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "\n",
    "    if option_type == \"call\":\n",
    "        price = np.exp(-r * T) * (F * norm.cdf(d1) - K * norm.cdf(d2))\n",
    "    elif option_type == \"put\":\n",
    "        price = np.exp(-r * T) * (K * norm.cdf(-d2) - F * norm.cdf(-d1))\n",
    "    else:\n",
    "        raise ValueError(\"option_type must be 'call' or 'put'\")\n",
    "\n",
    "    return price\n",
    "\n",
    "def pairwise_outer(arr):\n",
    "    \"\"\"\n",
    "    Given an array of shape (..., d), return an array of shape (..., d, d),\n",
    "    where each (...)-indexed vector is expanded to an outer product with itself.\n",
    "\n",
    "    Parameters:\n",
    "    - arr: np.ndarray, shape (..., d)\n",
    "\n",
    "    Returns:\n",
    "    - out: np.ndarray, shape (..., d, d)\n",
    "    \"\"\"\n",
    "    return arr[..., :, None] * arr[..., None, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOADING AND CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_params = os.path.join(os.getcwd(), \"parameters\")\n",
    "forwards = load_object(path_params+\"/spot_forwards.pkl\")\n",
    "s0_exp = load_object(path_params+\"/vol_initial_correction.pkl\")\n",
    "epsilon_exp = load_object(path_params+\"/volvol_initial_correction.pkl\")\n",
    "doust_fwd_fwd = load_object(path_params+\"/fwdfwd_corr.pkl\")\n",
    "doust_vol_vol = load_object(path_params+\"/volvol_corr.pkl\")\n",
    "corr_fwd_vol = load_object(path_params+\"/fwdvol_corr.pkl\")\n",
    "params_g = load_object(path_params+\"/vol_params_g.pkl\")\n",
    "params_h= load_object(path_params+\"/volvol_params_h.pkl\")\n",
    "spots= load_object(path_params+\"/spot_rates.pkl\")\n",
    "params_g = np.array([-0.00557585, -0.00864318,  0.89466108,  0.00755986])\n",
    "params_h = np.array([1.42258187e-08, 3.01935702e01, 4.57201647e00, 4.05843346e-12,])\n",
    "epsilon_exp = np.concatenate([epsilon_exp[[0]],epsilon_exp])\n",
    "s0_exp = np.concatenate([s0_exp[[0]], s0_exp])\n",
    "\n",
    "\n",
    "\n",
    "rho_mat_6m = doust_fwd_fwd[:19, :19]\n",
    "theta_mat_6m = doust_vol_vol[:19, :19] #TODO: check if this is correct, or it should remove the first row and column instead\n",
    "phi_mat_diag = corr_fwd_vol\n",
    "fwd_tenors = np.arange(1,10.5,0.5)\n",
    "# self defined\n",
    "params_g = np.array([0.005, 0.04, 1, 0.001])\n",
    "params_h = np.array([0.001, 3, 3, 0.01])\n",
    "s0_exp = np.ones_like(s0_exp)\n",
    "\n",
    "\n",
    "# Create new matrices \n",
    "\n",
    "\n",
    "beta_6m = 0.20696204\n",
    "beta_0m = 0.25697769\n",
    "beta_theta_0m = 0.1556888\n",
    "beta_theta_6m = 0.12135651\n",
    "n=20\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create phi \n",
    "\n",
    "\n",
    "\n",
    "# interpolate phi diag\n",
    "phi_diag = np.diag(corr_fwd_vol)\n",
    "\n",
    "phi_diag = np.concatenate([phi_diag[[0]],phi_diag])\n",
    "T_phi = np.arange(0,10, 0.5)\n",
    "\n",
    "# SHOULD BE IN LMM CLASS\n",
    "\n",
    "rho_mat_0m = doust_corr(beta_0m, n)\n",
    "theta_mat_0m = doust_corr(beta_theta_0m, n)\n",
    "phi_mat_0m = build_phi_matrix(T_phi, phi_diag, 0.0087931, 0.051319)\n",
    "\n",
    "\n",
    "# create rates \n",
    "path = os.path.join(os.getcwd(), \"raw_dataset\")\n",
    "df_cap = pd.read_excel(path+\"/caplet_raw.xlsx\", sheet_name = 2, header = 0)\n",
    "df_raw_spot = pd.read_csv(path+\"/spot.csv\")\n",
    "df_raw_spot[\"Tenor\"] = np.array([1/12, 2/12, 3/12, 0.5, 0.75, 1, 2, 3, 4, 5, 7, 9, 10, 12, 15, 20, 30, 50])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMMSABR RELATED FUNCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_df_init(df_fwd, df_raw_spot, resolution, tau=0.5):\n",
    "\n",
    "    # Get the LIBOR 6-month spot rate\n",
    "    libor_6m_spot = df_raw_spot.loc[df_raw_spot[\"Tenor\"] == 0.5, \"Spot\"].iloc[0] # floating point comparison only safe due to 0.5 bein representable with 2**-1\n",
    "\n",
    "    # Create a dataframe with the initial values for the spot rate\n",
    "    spot_row = pd.DataFrame({\n",
    "        'Fixing': [0.0],\n",
    "        'Reset Rate': [libor_6m_spot],  # Convert back to percentage\n",
    "        'Maturity': [0.5]\n",
    "    })\n",
    "\n",
    "    # Initialize df_init with df_cap data\n",
    "    df_full = df_fwd[['Fixing', 'Reset Rate', 'Maturity']].copy()\n",
    "\n",
    "    # Concatenate with the spot rate row and reset index\n",
    "    df_full = pd.concat([spot_row, df_full], ignore_index=True)\n",
    "    df_full['Reset Rate'] = df_full['Reset Rate'] / 100  # Convert to percentage\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    #                               TIME INDEXING\n",
    "    # =============================================================================\n",
    "    ts_fwd_expiry = df_full['Fixing'].values\n",
    "\n",
    "    dt = tau / resolution\n",
    "    ids_fwd_interp = (ts_fwd_expiry / dt).astype(int) # divide by dt to get indices in the new time unit\n",
    "    \n",
    "    \n",
    "    n_fwd = len(ts_fwd_expiry)-1 # exclude period covering the last forward rate tenor\n",
    "    ts_fwd_interp= np.linspace(0, n_fwd*tau, int(n_fwd * resolution +1))\n",
    "    #print(f\"{ts_fwd_interp=}\")\n",
    "    assert np.all(np.isin(ts_fwd_expiry, ts_fwd_interp)), \"Not all forward expirys are in the time grid\"\n",
    "    # =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    #           Create the zcb interpolated curve\n",
    "    # =============================================================================\n",
    "\n",
    "\n",
    "    fwd_canon = df_full['Reset Rate'].values\n",
    "    discount_factors = 1 / (1 + fwd_canon[:-1] * 0.5) # leave out last as we dont use zcb prices after the last forward rate\n",
    "    zcb_from_fwd = np.concatenate(([1], np.cumprod(discount_factors)))\n",
    "\n",
    "    zcb_cs = interpolate.CubicSpline(ts_fwd_expiry, zcb_from_fwd)\n",
    "    zcb_interp = zcb_cs(ts_fwd_interp)\n",
    "\n",
    "    # =============================================================================\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    #          Construct dataframe with tenors, zcb and forward rates\n",
    "    # =============================================================================\n",
    "    df = pd.DataFrame({'Tenor': ts_fwd_interp, 'zcb': zcb_interp, 'Forward': np.nan, 's0': np.nan})\n",
    "    df.loc[ids_fwd_interp, 'Forward'] = fwd_canon\n",
    "    df.loc[ids_fwd_interp, 'k0'] = s0_exp\n",
    "    # add column with backfilled forward indices, such that the value in this column is 0 from 0 to 5, 1 from 6 to 11, 2 from 12 to 17, etc.    \n",
    "    df['i_s'] = (np.arange(len(df)) // resolution)*resolution\n",
    "    df['i_sp1'] = (np.arange(len(df)) // resolution+1)*resolution\n",
    "    df['mod_accrual'] = tau - (df['Tenor'] % tau)\n",
    "    df_temp = df.merge(\n",
    "    df[['zcb', 'Forward']],\n",
    "    left_on='i_s',     # Column with pointers to index\n",
    "    right_index=True,      # Use index from right DataFrame\n",
    "    how='left',            # Keep all rows from original\n",
    "    suffixes=('', '_i_s')  # Add suffix to avoid column name conflicts\n",
    "    )\n",
    "    df_temp = df_temp.merge(\n",
    "    df[['zcb']],\n",
    "    left_on='i_sp1',     # Column with pointers to index\n",
    "    right_index=True,      # Use index from right DataFrame\n",
    "    how='left',            # Keep all rows from original\n",
    "    suffixes=('', '_i_sp1')  # Add suffix to avoid column name conflicts\n",
    "    )\n",
    "    df['gamma']= (df_temp['zcb'] / df_temp['zcb_i_sp1'] -1) / df_temp['mod_accrual'] /df_temp['Forward_i_s']\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    return df\n",
    "\n",
    "\n",
    "def interp_func_fac(df_init, resolution=2, tau=0.5, beta=0.5, rho_mat=None, g_func=None, interp_vol = False, zcb_interp = False):\n",
    "    df = df_init\n",
    "    #fwd = df['Forward'].values # only used for test, not to be uncommented\n",
    "\n",
    "    i_s = df['i_s'].values[:-resolution]\n",
    "    i_sp1 = df['i_sp1'].values[:-resolution]\n",
    "    i_e = i_sp1\n",
    "\n",
    "    s = np.arange(len(df)-resolution)\n",
    "    #print(\"len s\",len(s))\n",
    "    e = s + resolution\n",
    "\n",
    "    theta = (df['mod_accrual'].values)\n",
    "    gamma = (df['gamma'].values)\n",
    "    gamma_theta = gamma * theta\n",
    "\n",
    "    #print(f\"{len(i_s)}, {len(i_e)}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def get_interp_rates(fwd):\n",
    "        p_s_e = (1 + fwd[i_e] * gamma_theta[e]) / (1 + fwd[i_s] * gamma_theta[s]) * 1/(1+fwd[i_e]*tau)\n",
    "        f_s_e = (1 / p_s_e - 1) / tau \n",
    "        return f_s_e\n",
    "    \n",
    "    \n",
    "    if zcb_interp:\n",
    "\n",
    "\n",
    "        def build_forward_zcb_matrix_from_f_sim(f_sim_input, max_tenor=10):\n",
    "            \"\"\"\n",
    "            Compute a forward zero-coupon bond (ZCB) matrix P[t,e](t) in a fully vectorized manner,\n",
    "            but only for maturities up to max_tenor (in years) beyond the current time.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            f_sim : np.ndarray, shape (n, n)\n",
    "                Simulated forward rate matrix. f_sim[t, i] is the forward rate observed at time t\n",
    "                for the interval starting at the tenor corresponding to index i.\n",
    "            gamma_theta : np.ndarray, shape (n,)\n",
    "                Effective stub accrual factors (γ[i] * θ[i]) for each tenor.\n",
    "            tau : float\n",
    "                Canonical accrual period (e.g., 0.5 years).\n",
    "            resolution : int\n",
    "                Number of time steps per tau (e.g., 2 means grid spacing of tau/resolution, i.e., 0.25 years if tau=0.5).\n",
    "            max_tenor : float\n",
    "                Maximum tenor (in years) ahead of the current time for which to calculate ZCB prices.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            P : np.ndarray, shape (n, n)\n",
    "                The forward zero-coupon bond (ZCB) matrix such that P[t, e] = P(t, e)\n",
    "                for e >= t and for e <= t + max_steps. Entries for e < t or e > t + max_steps are set to NaN.\n",
    "            \"\"\"\n",
    "            f_sim = f_sim_input.copy()\n",
    "            f_sim[np.isnan(f_sim)] = 0\n",
    "            n = f_sim.shape[0]\n",
    "            gamma_theta_trunc = gamma_theta[:n]  # Ensure gamma_theta is the same length as f_sim\n",
    "            # --- Stub Start: for each row t, discount from t to the next stub endpoint.\n",
    "            stub_start = 1.0 / (1.0 + np.diag(f_sim) * gamma_theta_trunc)  # shape (n,)\n",
    "            \n",
    "            # --- Stub End: for each (t,e), discount adjustment for the end stub.\n",
    "            stub_end = 1.0 + f_sim * gamma_theta_trunc[np.newaxis, :]  # shape (n, n)\n",
    "            \n",
    "            # --- Canonical Product\n",
    "            # Define canonical grid indices: these are every 'resolution' step.\n",
    "            can = np.arange(0, n, resolution)  # e.g., [0, resolution, 2*resolution, ...]\n",
    "            n_can = len(can)\n",
    "            \n",
    "            # Build a matrix M: for each row t, M[t, j] = 1/(1 + f_sim[t, can[j]] * tau)\n",
    "            M = 1.0 / (1.0 + f_sim[:, can] * tau)  # shape (n, n_can)\n",
    "            # Cumulative product along the canonical axis\n",
    "            CP = np.cumprod(M, axis=1)  # shape (n, n_can)\n",
    "            \n",
    "            # For each row t, define J_s = floor(t/resolution) + 1 (starting canonical index)\n",
    "            t_idx = np.arange(n)\n",
    "            J_s = (t_idx // resolution) + 1  # shape (n,)\n",
    "            # For each column e, define J_e = floor(e/resolution)\n",
    "            J_e = np.arange(n) // resolution  # shape (n,)\n",
    "            \n",
    "            # Broadcast J_s and J_e to form matrices\n",
    "            J_s_mat = J_s[:, None]  # shape (n, 1)\n",
    "            J_e_mat = J_e[None, :]  # shape (1, n)\n",
    "            \n",
    "            # Get canonical cumulative product:\n",
    "            A = CP[np.arange(n)[:, None], J_e_mat]  # shape (n, n)\n",
    "            B = np.where(J_s_mat > 0,\n",
    "                        CP[np.arange(n)[:, None], (J_s_mat - 1)],\n",
    "                        1.0)\n",
    "            # Canonical product for each (t,e)\n",
    "            canon_prod = np.where(J_s_mat <= J_e_mat, A / B, 1.0)\n",
    "            \n",
    "            # --- Combine All Terms: full ZCB from t to e\n",
    "            stub_start_matrix = stub_start[:, None]  # shape (n, 1)\n",
    "            P = stub_start_matrix * canon_prod * stub_end  # shape (n, n)\n",
    "            \n",
    "            # Ensure lower-triangular entries (e < t) are NaN and diagonal is 1.\n",
    "            P = np.triu(P, k=0)\n",
    "            np.fill_diagonal(P, 1.0)\n",
    "            \n",
    "            # --- Apply max_tenor: Only keep P[t,e] for e <= t + max_steps.\n",
    "            max_steps = int(max_tenor * resolution / tau)\n",
    "            row_indices = np.arange(n)[:, None]\n",
    "            col_indices = np.arange(n)[None, :]\n",
    "            mask = col_indices > (row_indices + max_steps)\n",
    "            P[mask] = np.nan\n",
    "            # set lower triangle to nan\n",
    "            P[np.tril_indices(n, k=-1)] = np.nan\n",
    "            P[:,-resolution:] = np.nan\n",
    "            return P\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if interp_vol:\n",
    "        rho_mat_interpolated = interpolate_correlation_matrix(rho_mat, resolution)\n",
    "        fwd = df['Forward'].values\n",
    "        f1 = fwd[i_s]**beta      \n",
    "        f2 = fwd[i_e]**beta\n",
    "        w1 = gamma_theta[s] / tau\n",
    "        w2 = (tau - gamma_theta[e]) / tau\n",
    "        f_interp = get_interp_rates(fwd)**beta\n",
    "\n",
    "        term1 = (w1**2) * (f1**2) / f_interp[s]**2\n",
    "        term2 = (w2**2) * (f2**2) / f_interp[s]**2\n",
    "        rho = rho_mat_interpolated[i_s, i_e]  \n",
    "        cross = 2 * w1 * w2 * f1 * f2 * rho / f_interp[s]**2\n",
    "\n",
    "        tenors = df['Tenor'].values\n",
    "        ttm_mat = tenors[None, :] - tenors[:, None]\n",
    "        g_mat = g_func(ttm_mat)\n",
    "\n",
    "        def get_interp_vol_matrix(s_mat):\n",
    "            \"\"\"\n",
    "            Vectorized volatility interpolation for an entire volatility matrix.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            s_mat : np.ndarray of shape (n_steps, n_forwards)\n",
    "                Matrix of instantaneous volatilities.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            s_interp : np.ndarray of shape (n_steps, len(s))\n",
    "                Interpolated volatilities at subgrid points.\n",
    "            \"\"\"\n",
    "            s1 = s_mat[:, i_s]  # shape (n_steps, len(s))\n",
    "            s2 = s_mat[:, i_e]  # shape (n_steps, len(s))\n",
    "\n",
    "            # Compute numerator vectorized over time\n",
    "            sigma_sum = (\n",
    "                term1[None, :] * s1**2 +\n",
    "                term2[None, :] * s2**2 +\n",
    "                cross[None, :] * s1 * s2\n",
    "            )  # shape (n_steps, len(s))\n",
    "\n",
    "            s_interp = np.sqrt(sigma_sum)\n",
    "\n",
    "            return s_interp / g_mat[:, s]  # shape (n_steps, len(s))\n",
    "\n",
    "        if zcb_interp:\n",
    "            return get_interp_rates, get_interp_vol_matrix, build_forward_zcb_matrix_from_f_sim\n",
    "        else:\n",
    "            return get_interp_rates, get_interp_vol_matrix\n",
    "    \n",
    "    return get_interp_rates\n",
    "\n",
    "\n",
    "\n",
    "def get_swap_matrix(f_sim, shape, resolution, tau, tenor, df, expiry_max=1, expiry_min=1, beta=0.5, B=0.5):\n",
    "    \"\"\"\n",
    "    Compute time-evolving swap rates from a simulated forward path for a set of swap expiries.\n",
    "\n",
    "    Parameters:\n",
    "    - f_sim: np.ndarray, shape (n_steps, n_forwards)\n",
    "        Simulated forward curve matrix (lower-triangular in time).\n",
    "    - T_idxs: list or np.ndarray of int\n",
    "        Forward indices (expiry) at which each swap starts.\n",
    "    - resolution: int\n",
    "        Number of simulation steps per accrual period (tau).\n",
    "    - tau: float\n",
    "        Accrual period of the swap (e.g., 0.5 for semiannual).\n",
    "    - tenor: float\n",
    "        Total length of the swap in years (e.g., 1.0 for a 1y swap).\n",
    "    - df: pd.DataFrame\n",
    "        DataFrame containing initial zero-coupon bond prices in column 'zcb'.\n",
    "    - expiry: float, optional\n",
    "        If provided, the maximum length of a simulated swap path will be limited to this value (in year units).\n",
    "    Returns:\n",
    "    - swap_paths: np.ndarray, shape (n_valid_steps, n_swaps)\n",
    "        Matrix of swap rates over time for each swap expiry.\n",
    "    - valid_steps: np.ndarray\n",
    "        Array of time steps for which all required forward rates exist.\n",
    "    - used_T_idxs: np.ndarray\n",
    "        Final T_idxs that were valid and included in the result.\n",
    "    \"\"\"\n",
    "    f_sim = np.asarray(f_sim)\n",
    "    expiry_min_idx = int(expiry_min * resolution / tau)\n",
    "    n_swaps = np.arange(shape[1]+expiry_min_idx)\n",
    "    zcbs = df['zcb'].values\n",
    "\n",
    "    n_steps, n_forwards = f_sim.shape\n",
    "    n_payments = int(tenor / tau)\n",
    "    swap_len = n_payments  # number of forward rates needed\n",
    "\n",
    "    # Compute max usable T_idx based on number of forward rates\n",
    "    #max_T_idx = n_forwards -   swap_len * resolution\n",
    "    #n_swaps = n_swaps # we remove the first columns afterwards so that the first column has expiry_min time to expiry\n",
    "    if len(n_swaps) == 0:\n",
    "        raise ValueError(\"No valid T_idxs remain after filtering based on swap length and resolution.\")\n",
    "\n",
    "    # Compute all valid time steps\n",
    "    t_end = shape[0]\n",
    "    valid_steps = np.arange(0, t_end)\n",
    "\n",
    "    # Compute forward rate indices for each swap\n",
    "    forward_offsets = np.arange(n_payments) * resolution\n",
    "    col_indices = n_swaps[:, None] + forward_offsets[None, :]\n",
    "    # Check bounds\n",
    "    if np.any(col_indices >= n_forwards):\n",
    "        raise IndexError(\"Computed forward indices exceed available f_sim columns.\")\n",
    "\n",
    "    # Compute ZCB indices needed for annuity weights\n",
    "    zcb_offsets = np.arange(1,n_payments+1) * resolution\n",
    "    zcb_indices = n_swaps[:, None] + zcb_offsets[None, :]\n",
    "    if np.any(zcb_indices >= len(zcbs)):\n",
    "        raise IndexError(\"Computed ZCB indices exceed available zcb entries.\")\n",
    "    \n",
    "\n",
    "    # Compute frozen swap weights\n",
    "    zcb_sets = np.stack([zcbs[idxs] for idxs in zcb_indices])\n",
    "    swap_weights = zcb_sets * tau\n",
    "    annuity = np.sum(swap_weights, axis=1, keepdims=True)\n",
    "    swap_weights = swap_weights / annuity  # Normalize to 1\n",
    "    # Gather simulated forward rate and volatility slices\n",
    "    f_curves = f_sim[valid_steps]\n",
    "    fwd_subsets = np.stack([f_curves[:, idxs] for idxs in col_indices], axis=1)\n",
    "\n",
    "    #print(fwd_subsets.shape)\n",
    "    # Compute weighted sum (dot product): (n_valid_steps, n_swaps)\n",
    "    swap_paths = np.einsum('tsp,sp->ts', fwd_subsets, swap_weights)\n",
    "    swap_paths[np.triu_indices_from(swap_paths, k=int(expiry_max*resolution/tau+1))] = np.nan  # Set upper triangle to NaN\n",
    "\n",
    "    # Compute W: (n_valid_steps, n_swaps, n_payments)\n",
    "    swaps_expanded = swap_paths[:, :, None]  # (n_valid_steps, n_swaps, 1)\n",
    "    W = swap_weights[None, :, :] * (fwd_subsets**beta) / (swaps_expanded**B)  # default: beta=0.5, B=0.5\n",
    "    #print(swap_weights)\n",
    "    #W = np.nan_to_num(W)\n",
    "    \n",
    "\n",
    "    return swap_paths[:,expiry_min_idx:], W[:,expiry_min_idx:] # we start cut the first columns so the first column has has expiry_min time to expiry\n",
    "\n",
    "\n",
    "def make_swap_indexer(n_steps, swap_idxs, resolution, tau, tenor, expiry, return_indices=False):\n",
    "    \"\"\"\n",
    "    this is a function factory to create an indexer that given a matrix of similar structure to f_sim\n",
    "    will return an indexer function that takes a matrix of simulated values connected to a set of values relevant to the swap\n",
    "\n",
    "    Parameters:\n",
    "    - n_steps: int\n",
    "        Number of simulation steps.\n",
    "    - shape: tuple\n",
    "        Shape of the swap matrix that the final output will have for the first two dimensions.\n",
    "    - resolution: int\n",
    "        Number of simulation steps per accrual period (tau).\n",
    "    - tau: float\n",
    "        Accrual period of the swap (e.g., 0.5 for semiannual).\n",
    "    - tenor: float\n",
    "        Total length of the swap in years (e.g., 1.0 for a 1y swap).\n",
    "    - return_indices: bool, optional\n",
    "        If True, return the valid steps and column indices used for indexing.\n",
    "    Returns:\n",
    "    - indexer: callable\n",
    "        Function to index into the matrix of simulated forward rates.\n",
    "    - (valid_steps, col_indices): tuple of np.ndarray\n",
    "        If return_indices is True, returns the valid steps and column indices used for indexing.\n",
    "    \"\"\"\n",
    "    n_payments = int(tenor / tau)\n",
    "    swap_len = n_payments\n",
    "    t_end = int(n_steps - (expiry + swap_len) * resolution -1) # subtract 1 as first step doesnt count\n",
    "    valid_steps = swap_idxs[0]\n",
    "    col_indices = swap_idxs[1]\n",
    "    offsets = np.arange(n_payments) * resolution\n",
    "    col_indices = col_indices[:, None] + offsets[None, :]\n",
    "    col_indices = np.broadcast_to(col_indices, (len(valid_steps), *col_indices.shape))\n",
    "    def indexer(mat):\n",
    "        mat_short = mat[valid_steps]\n",
    "        mat_short = mat_short[:, None, :]\n",
    "        return np.take_along_axis(mat_short, col_indices, axis=2) # shape (n_valid_steps, n_swaps, n_payments)\n",
    "    if return_indices:\n",
    "        return indexer, (valid_steps, col_indices)\n",
    "    return indexer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batched stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_swap_correlation_tensor_batched(rho, T_idxs, resolution, tau, tenor, n_valid_steps=None):\n",
    "    \"\"\"\n",
    "    Batched version of build_swap_correlation_tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - rho: np.ndarray, shape (S, F, F)\n",
    "        Correlation matrices for each simulation.\n",
    "    - T_idxs: np.ndarray, shape (n_swaps,)\n",
    "        Forward start indices of each swap.\n",
    "    - resolution: int\n",
    "        Steps per tau.\n",
    "    - tau: float\n",
    "        Accrual period.\n",
    "    - tenor: float\n",
    "        Swap length in years.\n",
    "    - n_valid_steps: int, optional\n",
    "        Number of valid simulation steps.\n",
    "\n",
    "    Returns:\n",
    "    - rho_tensor: np.ndarray, shape (S, n_valid_steps, n_swaps, n_payments, n_payments)\n",
    "    \"\"\"\n",
    "    S, F, _ = rho.shape\n",
    "    n_swaps = len(T_idxs)\n",
    "    n_payments = int(tenor / tau)\n",
    "\n",
    "    if n_valid_steps is None:\n",
    "        n_valid_steps = len(T_idxs)\n",
    "\n",
    "    rho_subs = np.empty((S, n_swaps, n_payments, n_payments))\n",
    "\n",
    "    for j, T_idx in enumerate(T_idxs):\n",
    "        indices = T_idx + np.arange(n_payments) * resolution  # (n_payments,)\n",
    "        # Batched slice: (S, n_payments, n_payments)\n",
    "        rho_subs[:, j] = np.take(rho, indices[:, None], axis=1)[..., indices]\n",
    "\n",
    "    # Now tile over valid steps: (S, n_valid_steps, n_swaps, n_payments, n_payments)\n",
    "    rho_tensor = np.tile(rho_subs[:, None, :, :, :], (1, n_valid_steps, 1, 1, 1))\n",
    "\n",
    "    return rho_tensor\n",
    "\n",
    "\n",
    "def make_swap_indexer_batched(n_steps, T_idxs, resolution, tau, tenor, return_indices=False):\n",
    "    \"\"\"\n",
    "    Batched version of swap indexer — works on input shaped (n_sims, n_steps, n_forwards).\n",
    "\n",
    "    Returns an indexer that extracts the floating leg reset values for all simulations.\n",
    "    \"\"\"\n",
    "    n_payments = int(tenor / tau)\n",
    "    swap_len = n_payments\n",
    "    t_end = n_steps - swap_len * resolution\n",
    "    valid_steps = np.arange(0, t_end)\n",
    "\n",
    "    # (n_swaps, n_payments)\n",
    "    offsets = np.arange(n_payments) * resolution\n",
    "    col_indices = T_idxs[:, None] + offsets[None, :]\n",
    "    \n",
    "    # (n_valid_steps, n_swaps, n_payments)\n",
    "    col_indices = np.broadcast_to(col_indices, (len(valid_steps), *col_indices.shape))\n",
    "\n",
    "    def indexer(mat):\n",
    "        \"\"\"\n",
    "        mat: np.ndarray of shape (n_sims, n_steps, n_forwards)\n",
    "        returns: (n_sims, n_valid_steps, n_swaps, n_payments)\n",
    "        \"\"\"\n",
    "        n_sims = mat.shape[0]\n",
    "\n",
    "        # (n_valid_steps, n_steps) → pick rows from each sim\n",
    "        mat_short = mat[:, valid_steps, :]  # shape: (n_sims, n_valid_steps, n_forwards)\n",
    "\n",
    "        # Expand for swap index structure\n",
    "        # We want: (n_sims, n_valid_steps, n_swaps, n_payments)\n",
    "        # We can do this using np.take_along_axis\n",
    "\n",
    "        # col_indices: (n_valid_steps, n_swaps, n_payments)\n",
    "        # Broadcast to (1, n_valid_steps, n_swaps, n_payments)\n",
    "        col_indices_batched = np.broadcast_to(col_indices, (n_sims, *col_indices.shape))\n",
    "\n",
    "        # mat_short: (n_sims, n_valid_steps, n_forwards) → expand axis\n",
    "        mat_short_expanded = mat_short[:, :, None, :]  # (n_sims, n_valid_steps, 1, n_forwards)\n",
    "\n",
    "        return np.take_along_axis(mat_short_expanded, col_indices_batched, axis=3)\n",
    "\n",
    "    if return_indices:\n",
    "        return indexer, (valid_steps, col_indices)\n",
    "    return indexer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_swap_matrix_batched(f_sim, T_idxs, resolution, tau, tenor, df, expiry=1, beta=0.5, B=0.5):\n",
    "    \"\"\"\n",
    "    Batched version of get_swap_matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - f_sim: np.ndarray, shape (n_sims, n_steps, n_forwards)\n",
    "    - T_idxs: np.ndarray of shape (n_swaps,)\n",
    "    - df: pd.DataFrame with column 'zcb'\n",
    "    Returns:\n",
    "    - swap_paths: (n_sims, n_valid_steps, n_swaps)\n",
    "    - W: (n_sims, n_valid_steps, n_swaps, n_payments)\n",
    "    \"\"\"\n",
    "    f_sim = np.asarray(f_sim)\n",
    "    T_idxs = np.asarray(T_idxs)\n",
    "    zcbs = df['zcb'].values\n",
    "\n",
    "    S, n_steps, n_forwards = f_sim.shape\n",
    "    n_payments = int(tenor / tau)\n",
    "    swap_len = n_payments\n",
    "    max_T_idx = n_forwards - swap_len * resolution\n",
    "\n",
    "    if len(T_idxs) == 0:\n",
    "        raise ValueError(\"No valid T_idxs remain after filtering.\")\n",
    "\n",
    "    t_end = n_steps - swap_len * resolution\n",
    "    valid_steps = np.arange(0, t_end)\n",
    "\n",
    "    # (n_swaps, n_payments)\n",
    "    forward_offsets = np.arange(n_payments) * resolution\n",
    "    col_indices = T_idxs[:, None] + forward_offsets[None, :]\n",
    "\n",
    "    if np.any(col_indices >= n_forwards):\n",
    "        raise IndexError(\"Forward indices exceed f_sim dimension.\")\n",
    "\n",
    "    # ZCB weights\n",
    "    zcb_offsets = np.arange(1, n_payments + 1) * resolution\n",
    "    zcb_indices = T_idxs[:, None] + zcb_offsets[None, :]\n",
    "\n",
    "    if np.any(zcb_indices >= len(zcbs)):\n",
    "        raise IndexError(\"ZCB indices exceed available entries.\")\n",
    "\n",
    "    # Frozen swap weights: (n_swaps, n_payments)\n",
    "    zcb_sets = np.stack([zcbs[idxs] for idxs in zcb_indices])\n",
    "    swap_weights = zcb_sets * tau\n",
    "    swap_weights = swap_weights / swap_weights.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # Grab forward subsets\n",
    "    f_curves = f_sim[:, valid_steps, :]  # (S, T', F)\n",
    "    fwd_subsets = np.stack(\n",
    "        [np.take(f_curves, idxs, axis=2) for idxs in col_indices], axis=2\n",
    "    )  # shape: (S, T', n_swaps, n_payments)\n",
    "\n",
    "    # Compute weighted swap rates: einsum over payments axis\n",
    "    swap_paths = np.einsum(\"stnp,sp->stn\", fwd_subsets, swap_weights)\n",
    "\n",
    "    # Mask future expiry values\n",
    "    expiry_cutoff = int(expiry * resolution / tau) + 1\n",
    "    for i in range(swap_paths.shape[1]):\n",
    "        if i > expiry_cutoff:\n",
    "            swap_paths[:, i, :] = np.nan\n",
    "\n",
    "    # Compute W\n",
    "    swaps_expanded = swap_paths[..., None]  # (S, T', n_swaps, 1)\n",
    "    weights_expanded = swap_weights[None, None, :, :]  # (1, 1, n_swaps, n_payments)\n",
    "    W = weights_expanded * (fwd_subsets ** beta) / (swaps_expanded ** B)  # shape: (S, T', n_swaps, n_payments)\n",
    "\n",
    "    return swap_paths, W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_max set to 4\n",
      "Curve preparation: 0.02s\n",
      "G tensor preparation: 0.00s\n",
      "V tensor preparation: 0.01s\n",
      "[[ 464483.9393          nan          nan          nan          nan]\n",
      " [ 723543.9345  487822.0984          nan          nan          nan]\n",
      " [ 155864.8433  790582.0642  475444.2626          nan          nan]\n",
      " [   5850.2993 1882137.7563  703585.4278  485114.8793          nan]\n",
      " [         nan 4844300.0656 1300293.1437  732945.5038  410001.6947]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LMMSABR:\n",
    "    def __init__(\n",
    "        self,\n",
    "        rho_mat,\n",
    "        theta_mat,\n",
    "        phi_mat,\n",
    "        g_params,\n",
    "        h_params,\n",
    "        epsilon_exp,\n",
    "        k0_exp,\n",
    "        df_cap,\n",
    "        df_raw_spot,\n",
    "        tau=0.5,\n",
    "        resolution=2,\n",
    "        tenor=1,\n",
    "        sim_time = 1,\n",
    "        t_max=None,\n",
    "        beta=0.5,\n",
    "        B=0.5, swap_hedge_expiry=1, swap_client_expiry=2\n",
    "        \n",
    "        \n",
    "    ):\n",
    "        self.rho_mat = rho_mat\n",
    "        self.theta_mat = theta_mat\n",
    "        self.phi_mat = phi_mat\n",
    "        self.g = partial(get_instant_vol_func, params=g_params)\n",
    "        self.h = partial(get_instant_vol_func, params=h_params)\n",
    "        self.epsilon_exp = epsilon_exp\n",
    "        self.k0_exp = k0_exp\n",
    "\n",
    "        self.tau = tau\n",
    "        self.resolution = resolution\n",
    "        self.dt = tau / resolution\n",
    "        self.beta = beta\n",
    "        self.B = B\n",
    "        \n",
    "        # swap params\n",
    "        self.tenor = tenor\n",
    "        assert swap_hedge_expiry != swap_client_expiry, \"swap_hedge_expiry and swap_client_expiry should be different\"\n",
    "        self.swap_hedge_expiry = swap_hedge_expiry\n",
    "        self.swap_liab_expiry = swap_client_expiry\n",
    "        self.max_swap_expiry = np.maximum(self.swap_hedge_expiry, self.swap_liab_expiry)\n",
    "        self.min_swap_expiry = np.minimum(self.swap_hedge_expiry, self.swap_liab_expiry)\n",
    "        self.swap_hedge_expiry_relative = self.swap_hedge_expiry - self.min_swap_expiry\n",
    "        self.swap_liab_expiry_relative = self.swap_liab_expiry - self.min_swap_expiry\n",
    "        self.sim_time = sim_time\n",
    "        if t_max is None:\n",
    "            print(f\"t_max set to {self.max_swap_expiry + tenor + sim_time}\")\n",
    "            self.t_max = self.max_swap_expiry + tenor + sim_time\n",
    "        else:\n",
    "            self.t_max = t_max\n",
    "        self.swap_sim_shape = self.t_to_idx(self.sim_time)+1, self.t_to_idx(self.max_swap_expiry - self.min_swap_expiry+ self.sim_time)+1\n",
    "        self.swap_idxs = np.arange(self.swap_sim_shape[0]),np.arange(self.t_to_idx(self.min_swap_expiry), self.swap_sim_shape[1]+self.t_to_idx(self.min_swap_expiry))\n",
    "        \n",
    "        # Store raw curve inputs\n",
    "        self.df_cap = df_cap\n",
    "        self.df_raw_spot = df_raw_spot\n",
    "\n",
    "        # Placeholders\n",
    "        self.df_init = None\n",
    "        self.f_sim = None\n",
    "        self.k_mat = None\n",
    "        self.swap_sim = None\n",
    "        start_curve_prep = time.time()\n",
    "        self.prepare_curves()\n",
    "        self.precompute_interpolation()\n",
    "        # set up tensors for the swaps\n",
    "        self.rho_tensor = self.build_swap_correlation_tensor(self.rho_mat_0m_interpolated)\n",
    "        self.theta_tensor = self.build_swap_correlation_tensor(self.theta_mat_0m_interpolated)\n",
    "        self.phi_tensor = self.build_swap_correlation_tensor(self.phi_mat_0m_interpolated)\n",
    "\n",
    "        start_G_tensor = time.time()\n",
    "        self.G_tensor = self.precompute_G_tensor()\n",
    "        start_ggh_tensor = time.time()\n",
    "        self.ggh_tensor = self.build_V_tensor_from_scalar( tenor=self.tenor, resolution=self.resolution, tau=self.tau)[:self.swap_sim_shape[0],self.t_to_idx(self.min_swap_expiry):,:,:]\n",
    "        print(f\"Curve preparation: {start_G_tensor - start_curve_prep:.2f}s\")\n",
    "        print(f\"G tensor preparation: {start_ggh_tensor - start_G_tensor:.2f}s\")\n",
    "        print(f\"V tensor preparation: {time.time() - start_ggh_tensor:.2f}s\")\n",
    "    \n",
    "    def t_to_idx(self, t):\n",
    "        \"\"\"\n",
    "        Convert time time units to index units.\n",
    "        \"\"\"\n",
    "        return int(t / self.dt)\n",
    "\n",
    "\n",
    "    def h_ij_vectorized_from_grid(self,t, u_arr, T_i, T_j):\n",
    "        \"\"\"\n",
    "        Compute h_ij for each u in u_arr using the self.t_arr as the integration grid.\n",
    "        Uses searchsorted for efficient pre-filtering of integration intervals.\n",
    "        \"\"\"\n",
    "        t_arr = self.t_arr\n",
    "        t_idx = np.searchsorted(t_arr, t, side='left')  # index just after t\n",
    "        hij_vals = []\n",
    "\n",
    "        for u in u_arr:\n",
    "            if u <= t:\n",
    "                hij_vals.append(0.0)\n",
    "                continue\n",
    "\n",
    "            u_idx = np.searchsorted(t_arr, u, side='right')\n",
    "            s_grid = t_arr[t_idx:u_idx]\n",
    "            if len(s_grid) == 0:\n",
    "                hij_vals.append(0.0)\n",
    "                continue\n",
    "\n",
    "            h_prod = self.h(T_i - s_grid) * self.h(T_j - s_grid)\n",
    "            integral = np.trapz(h_prod, s_grid)\n",
    "            hij_vals.append(np.sqrt(integral / (u - t)))\n",
    "\n",
    "        return np.array(hij_vals)\n",
    "\n",
    "\n",
    "\n",
    "    def integral_term_V(self, t_idx, T_idx, i, j):\n",
    "        \"\"\"\n",
    "        Compute the integral:\n",
    "        ∫ₜᵀ g_i(u)*g_j(u)*[h_ij(t,u)]²*(u-t) du\n",
    "        using lmm.t_arr as the integration grid.\n",
    "        \"\"\"\n",
    "        t = self.t_arr[t_idx]\n",
    "        T = self.t_arr[T_idx]\n",
    "        if T <= t:\n",
    "            return 0.0\n",
    "\n",
    "        u_arr = self.t_arr[t_idx:T_idx+1]\n",
    "        dt_arr = u_arr - t\n",
    "        T_i = self.t_arr[i]\n",
    "        T_j = self.t_arr[j]\n",
    "\n",
    "        h_vals = self.h_ij_vectorized_from_grid(t, u_arr, T_i, T_j)\n",
    "        h_sq = h_vals**2\n",
    "\n",
    "        g_i_arr = self.g(T_i - u_arr)\n",
    "        g_j_arr = self.g(T_j - u_arr)\n",
    "\n",
    "        integrand = g_i_arr * g_j_arr * h_sq * dt_arr\n",
    "        return np.trapz(integrand, u_arr)\n",
    "    \n",
    "    \n",
    "    def build_swap_correlation_tensor(self, corr_mat):\n",
    "        \"\"\"\n",
    "        Build a (n_valid_steps, n_swaps, n_payments, n_payments) tensor of forward correlations for each swap.\n",
    "\n",
    "        Parameters:\n",
    "        - corr_mat: np.ndarray, shape (n_forwards, n_forwards)\n",
    "            Correlation matrix between forward rates.\n",
    "\n",
    "        Returns:\n",
    "        - corr_tensor: np.ndarray, shape (n_valid_steps, n_swaps, n_payments, n_payments)\n",
    "        \"\"\"\n",
    "        n_valid_steps = len(self.swap_idxs[0])\n",
    "        \n",
    "        n_swaps = len(self.swap_idxs[1])\n",
    "        n_payments = int(self.tenor / self.tau)\n",
    "        corr_subs = np.empty((n_swaps, n_payments, n_payments))\n",
    "        expiry_idxs = self.swap_idxs[1]\n",
    "        for i, T_idx in enumerate(expiry_idxs):\n",
    "            indices = T_idx + np.arange(n_payments) * self.resolution\n",
    "            corr_subs[i] = corr_mat[np.ix_(indices, indices)]\n",
    "\n",
    "        # Tile over time steps\n",
    "        corr_tensor = np.tile(corr_subs[None, :, :, :], (n_valid_steps, 1, 1, 1))\n",
    "        return corr_tensor        \n",
    "    \n",
    "\n",
    "    def build_V_tensor_from_scalar(self, tenor, resolution, tau):\n",
    "        \"\"\"\n",
    "        Build the V_tensor using scalar integral_term_V, memoizing based on\n",
    "        time-translation invariance.\n",
    "\n",
    "        Returns:\n",
    "        - V_tensor: np.ndarray, shape (n_t, n_t, n, n)\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        tenor = self.tenor\n",
    "        resolution = self.resolution\n",
    "        tau = self.tau\n",
    "        max_expiry = self.max_swap_expiry\n",
    "        n = int(tenor / tau)\n",
    "        max_expiry_steps = int(max_expiry * resolution / tau)\n",
    "        num_t = len(self.t_arr) - n * resolution\n",
    "        V_tensor = np.full((num_t, num_t, n, n), np.nan)\n",
    "\n",
    "        cache = {}  # key: (delta_T, delta_i, delta_j) -> float\n",
    "\n",
    "        for t_idx in range(num_t):\n",
    "            expiry_limit = min(t_idx + max_expiry_steps+1, num_t)\n",
    "\n",
    "            for T_idx in range(t_idx, expiry_limit):\n",
    "                start_idx = T_idx\n",
    "                end_idx = T_idx + n * resolution\n",
    "                indices = list(range(start_idx, end_idx, resolution))\n",
    "\n",
    "                if len(indices) != n:\n",
    "                    print(f\"Skipping incomplete swap at indices: {indices}\")\n",
    "                    continue  # Skip incomplete swaps at boundary\n",
    "\n",
    "                delta_T = T_idx - t_idx\n",
    "\n",
    "                for i_local, i_global in enumerate(indices):\n",
    "                    for j_local, j_global in enumerate(indices):\n",
    "                        delta_i = i_global - T_idx\n",
    "                        delta_j = j_global - T_idx\n",
    "\n",
    "                        key = (delta_T, delta_i, delta_j)\n",
    "\n",
    "                        if key not in cache:\n",
    "                            # Compute and store\n",
    "                            cache[key] = self.integral_term_V(\n",
    "                                t_idx, T_idx, i_global, j_global\n",
    "                            )\n",
    "                        V_tensor[t_idx, T_idx, i_local, j_local] = cache[key]\n",
    "\n",
    "        return V_tensor\n",
    "\n",
    "    def precompute_G_tensor(self):\n",
    "        \"\"\"\n",
    "        Precompute a G_tensor using memoization and the self.t_arr as integration grid.\n",
    "        Uses:\n",
    "            G[t_idx, T_idx, i_local, j_local] = ∫ₜᵀ g(T_i - u) * g(T_j - u) du\n",
    "        with T_i, T_j based on T_idx and forward rate spacing.\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "\n",
    "        t_arr = self.t_arr\n",
    "        resolution = self.resolution\n",
    "        tau = self.tau\n",
    "        tenor = self.tenor\n",
    "        max_expiry = self.max_swap_expiry\n",
    "\n",
    "        num_t = len(t_arr) - int(tenor * resolution / tau)#-self.t_to_idx(self.min_swap_expiry)\n",
    "        n = int(tenor / tau)\n",
    "        G_tensor = np.zeros((num_t, num_t, n, n))*np.nan\n",
    "        # set the diagonal of the n,m,k,l tensor to 0\n",
    "        G_tensor[np.diag_indices(num_t)] = 0\n",
    "\n",
    "        cache = {}  # (delta_T_idx, delta_i, delta_j) → float\n",
    "        max_expiry_idx = int(max_expiry * resolution / tau)  # max expiry in steps\n",
    "        for t_idx in range(num_t):\n",
    "            for T_idx in range(np.maximum(t_idx -self.t_to_idx(self.min_swap_expiry), 0), num_t):\n",
    "                delta_T_idx = T_idx - t_idx\n",
    "                # check for max expiry\n",
    "                if delta_T_idx > max_expiry_idx:\n",
    "                    continue\n",
    "                # Use the actual model time grid for integration\n",
    "                s_idx_start = t_idx   # strictly > t\n",
    "                s_idx_end = T_idx + 1    # include T\n",
    "                u_arr = t_arr[s_idx_start:s_idx_end]\n",
    "                if len(u_arr) < 1:\n",
    "                    G_tensor[t_idx, T_idx] = np.nan#np.zeros((n, n))  # no integration needed\n",
    "                    #print(\"Skipping integration for empty u_arr at indices:\", s_idx_start, s_idx_end)\n",
    "                    continue  # skip if no points to integrate over\n",
    "\n",
    "                start_idx = T_idx\n",
    "                end_idx = T_idx + n * resolution\n",
    "                indices = list(range(start_idx, end_idx, resolution))\n",
    "                if len(indices) != n:\n",
    "                    continue  # incomplete swap\n",
    "\n",
    "                for i_local, i_global in enumerate(indices):\n",
    "                    for j_local, j_global in enumerate(indices):\n",
    "                        delta_i = i_global - T_idx\n",
    "                        delta_j = j_global - T_idx\n",
    "                        key = (delta_T_idx, delta_i, delta_j)\n",
    "\n",
    "                        if key not in cache:\n",
    "                            T_i = t_arr[i_global]\n",
    "                            T_j = t_arr[j_global]\n",
    "                            g_i = self.g(T_i - u_arr)\n",
    "                            g_j = self.g(T_j - u_arr)\n",
    "                            cache[key] = np.trapz(g_i * g_j, u_arr)\n",
    "\n",
    "                        G_tensor[t_idx, T_idx, i_local, j_local] = cache[key]\n",
    "\n",
    "        return G_tensor[:self.swap_sim_shape[0],self.t_to_idx(self.min_swap_expiry):]\n",
    "\n",
    "\n",
    "\n",
    "    def prepare_curves(self):\n",
    "        self.df_init = create_df_init(\n",
    "            self.df_cap, self.df_raw_spot, resolution=self.resolution, tau=self.tau\n",
    "        ).query(f\"Tenor <= {self.t_max + 1e-6}\")\n",
    "        self.tenors = self.df_init[\"Tenor\"].values\n",
    "        self.t_arr = self.tenors\n",
    "        self.ids_fwd_canon = self.df_init[\"Forward\"].dropna().index.values\n",
    "        self.num_forwards = len(self.ids_fwd_canon)\n",
    "        self.n_steps = len(self.df_init)\n",
    "\n",
    "    def precompute_vol_surfaces(self):\n",
    "        ttm_mat = self.tenors[None, :] - self.tenors[:,None]\n",
    "        self.ttm_mat = ttm_mat\n",
    "\n",
    "        self.h_mat = self.h(ttm_mat[1:, self.ids_fwd_canon])\n",
    "        self.g_mat = self.g(ttm_mat[:, self.ids_fwd_canon])\n",
    "\n",
    "    def precompute_interpolation(self):\n",
    "        self.interp_func, self.interp_vol_func, self.zcb_interp_func = interp_func_fac(\n",
    "            self.df_init,\n",
    "            resolution=self.resolution,\n",
    "            tau=self.tau,\n",
    "            rho_mat=self.rho_mat,\n",
    "            g_func=self.g,\n",
    "            interp_vol=True,\n",
    "            zcb_interp=True,\n",
    "\n",
    "        )\n",
    "        self.rho_mat_0m_interpolated = interpolate_correlation_matrix(self.rho_mat, self.resolution)\n",
    "        self.theta_mat_0m_interpolated = interpolate_correlation_matrix(self.theta_mat, self.resolution)\n",
    "        self.phi_mat_0m_interpolated = interpolate_correlation_matrix(self.phi_mat, self.resolution)\n",
    "\n",
    "    def simulate_forwards(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "        dt = self.dt\n",
    "        dt_sqrt = np.sqrt(dt)\n",
    "\n",
    "        dZ_f = np.random.multivariate_normal(\n",
    "            np.zeros(self.num_forwards),\n",
    "            self.rho_mat[:self.num_forwards, :self.num_forwards],\n",
    "            self.n_steps-1,\n",
    "        ) * dt_sqrt\n",
    "        dW_s = np.random.multivariate_normal(\n",
    "            np.zeros(self.num_forwards),\n",
    "            self.theta_mat[:self.num_forwards, :self.num_forwards],\n",
    "            self.n_steps-1,\n",
    "        ) * dt_sqrt\n",
    "\n",
    "        f_0 = self.df_init[\"Forward\"].values\n",
    "        f_sim = np.full((self.n_steps, len(f_0)), np.nan)\n",
    "        f_sim[0] = f_0   # temporary adjustment\n",
    "        self.f_sim = f_sim\n",
    "        self.dZ_f = dZ_f\n",
    "        self.dW_s = dW_s\n",
    "\n",
    "        self._simulate_vol_surface()\n",
    "        self._simulate_forward_dynamics()\n",
    "\n",
    "    def _simulate_vol_surface(self):\n",
    "        \n",
    "        \n",
    "        g_mat = self.g_mat\n",
    "        h_mat = self.h_mat\n",
    "\n",
    "        k_mat = np.concatenate([self.k0_exp[:self.num_forwards].reshape(1, -1), (self.k0_exp[:self.num_forwards] * np.cumprod(1 + self.epsilon_exp[:self.num_forwards].reshape(1, -1) * self.dW_s * h_mat, axis=0))])\n",
    "\n",
    "        self.k_mat = k_mat\n",
    "        self.s_mat = g_mat * k_mat\n",
    "        self.k_mat_full_res = np.zeros((self.n_steps, self.n_steps))*np.nan\n",
    "        s_mat_full_res = np.zeros((self.n_steps, self.n_steps))*np.nan\n",
    "        \n",
    "        s_mat_full_res[:, self.ids_fwd_canon] = k_mat * self.g_mat\n",
    "\n",
    "        \n",
    "        self.k_mat_full_res[:,:-self.resolution] = self.interp_vol_func(s_mat_full_res)\n",
    "\n",
    "        self.s_mat_full_res = self.k_mat_full_res * self.g(self.ttm_mat)\n",
    "        \n",
    "    def _simulate_forward_dynamics(self):\n",
    "        interp_func = self.interp_func\n",
    "        k_mat = self.k_mat\n",
    "\n",
    "        f_sim = self.f_sim\n",
    "        dZ_f = self.dZ_f\n",
    "\n",
    "        \n",
    "        ids_rev = self.ids_fwd_canon[::-1]\n",
    "        ids_short_rev = ids_rev // self.resolution\n",
    "        non_canon_idx = np.setdiff1d(np.arange(len(f_sim[0]))[:-self.resolution], self.ids_fwd_canon)\n",
    "        f_sim[0, non_canon_idx] = self.interp_func(f_sim[0])[non_canon_idx]\n",
    "        drift_correction = np.zeros(len(ids_rev))\n",
    "        drift_shared = np.zeros(len(ids_rev))\n",
    "\n",
    "        for t in range(1, self.n_steps):\n",
    "            drift_correction.fill(0)\n",
    "            drift_shared.fill(0)\n",
    "            # next loop runs from longest to shortest tenor\n",
    "            for canon_short_idx, canon_idx in zip(ids_short_rev, ids_rev):\n",
    "                if self.ttm_mat[t, canon_idx] +self.tau+1e-8>= 0:     # TODO <------------ THIS IS IMPORTANT\n",
    "                    s_t = self.s_mat[t-1, canon_short_idx]\n",
    "                    dZ_f_t = dZ_f[t-1,canon_short_idx]\n",
    "                    f_t = f_sim[t-1,canon_idx]\n",
    "                    f_beta_t = f_t**self.beta\n",
    "                    \n",
    "                    drift_f = (-self.g_mat[t, canon_short_idx] * k_mat[t, canon_short_idx] * f_beta_t * drift_shared[canon_short_idx])\n",
    "                    df_t =  drift_f + f_beta_t*s_t*dZ_f_t\n",
    "                    \n",
    "                    f_t_new =  f_t + df_t \n",
    "                    f_sim[t,canon_idx] = f_t_new if f_t_new > 0 else 0 # zero absorbing boundary\n",
    "\n",
    "                    if canon_short_idx > 0:\n",
    "                        drift_correction[canon_short_idx-1] = self.rho_mat[canon_short_idx-1, canon_short_idx] * self.tau * self.g_mat[t,canon_short_idx] * k_mat[t, canon_short_idx] * f_beta_t / (1 + self.tau * f_t)\n",
    "                        drift_shared[canon_short_idx-1] = np.sum(drift_correction[canon_short_idx-1:])\n",
    "\n",
    "            f_sim[t, non_canon_idx] = interp_func(f_sim[t])[non_canon_idx]\n",
    "        # remove lower triangular part of the matrix\n",
    "        f_sim[np.tril_indices_from(f_sim, k=-1)] = np.nan\n",
    "    def simulate(self, seed=None):\n",
    "        #start_time = time.time()\n",
    "        self.prepare_curves()\n",
    "        self.precompute_vol_surfaces()\n",
    "        self.simulate_forwards(seed=seed)\n",
    "\n",
    "        return self.f_sim\n",
    "    \n",
    "    \n",
    "    def get_swap_matrix(self):\n",
    "        \n",
    "        T_idxs = np.arange(self.swap_sim_shape[0])\n",
    "        swap_sim, W = get_swap_matrix( # TODO: make it so we get weights for both hedge and client swaps\n",
    "            self.f_sim, shape=self.swap_sim_shape, resolution=self.resolution, tau=self.tau, tenor=self.tenor, df=self.df_init, expiry_max = self.max_swap_expiry,expiry_min=self.min_swap_expiry, beta=self.beta, B=self.B\n",
    "        )\n",
    "        \n",
    "        self.swap_sim = swap_sim\n",
    "        self.W = W\n",
    "        assert self.k_mat_full_res.shape == self.f_sim.shape, f\"Shape mismatch: {self.k_mat_full_res.shape} != {self.f_sim.shape}\"\n",
    "        return swap_sim, W\n",
    "    \n",
    "\n",
    "    def get_sabr_params(self):\n",
    "        # ==========================================================\n",
    "        #          Create tensors for the SABR parameters\n",
    "        # ==========================================================\n",
    "        \n",
    "        swap_idxs_0 = np.arange(self.swap_sim.shape[0])\n",
    "        swap_idxs_1 = np.arange(self.swap_sim.shape[1])+self.t_to_idx(self.min_swap_expiry)\n",
    "        swap_indexer = make_swap_indexer(n_steps = self.f_sim.shape[0], swap_idxs = self.swap_idxs,resolution=self.resolution, tau=0.5, tenor=self.tenor,expiry=self.max_swap_expiry)\n",
    "        k_tensor = swap_indexer(self.k_mat_full_res)\n",
    "\n",
    "\n",
    "        rho_tensor = self.rho_tensor\n",
    "        theta_tensor = self.theta_tensor\n",
    "        phi_tensor = self.phi_tensor\n",
    "        k_tensor_prod = pairwise_outer(k_tensor)\n",
    "        W_tensor_prod = pairwise_outer(self.W)\n",
    "        # ==========================================================\n",
    "        # for the m,n,k,l tensor, sum such that we have a m,n tensor\n",
    "        \n",
    "        # ===========================================================\n",
    "        #               compute sigma tensor\n",
    "        # ==========================================================\n",
    "        #print(f\"theta_tensor shape: {theta_tensor.shape}, k_tensor shape: {k_tensor.shape}, rho_tensor shape: {rho_tensor.shape}, W_tensor_prod shape: {W_tensor_prod.shape}, G_tensor shape: {self.G_tensor.shape}\")\n",
    "        prod = rho_tensor*W_tensor_prod*k_tensor_prod*self.G_tensor\n",
    "\n",
    "        numerator = np.sum(prod, axis=(2, 3))\n",
    "\n",
    "        denominator = self.ttm_mat[np.ix_(swap_idxs_0, swap_idxs_1)]\n",
    "        sigma_sq = np.divide(\n",
    "            numerator,\n",
    "            denominator,\n",
    "            out=np.zeros_like(numerator),  # fill result with 0 where denominator == 0\n",
    "            where=denominator != 0\n",
    "        )\n",
    "        sigma = np.sqrt(sigma_sq)\n",
    "\n",
    "\n",
    "        # ==========================================================\n",
    "        #              compute V and Phi tensor       \n",
    "        # ==========================================================\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        V_terms = rho_tensor*theta_tensor*W_tensor_prod*k_tensor_prod*self.ggh_tensor\n",
    "        V_sum = np.sum(V_terms, axis=(2, 3))\n",
    "        V_numerator = np.sqrt(2*V_sum)\n",
    "        V_denominator = sigma*self.ttm_mat[np.ix_(swap_idxs_0, swap_idxs_1)]\n",
    "        V = np.divide(\n",
    "            V_numerator, \n",
    "            V_denominator,\n",
    "            out=np.zeros_like(numerator)*np.nan,  # fill result with 0 where denominator == 0\n",
    "            where=denominator != 0\n",
    "        )\n",
    "        # print shape of all component tensors\n",
    "        #print(f\"V_terms shape: {V_terms.shape}, V_sum shape: {V_sum.shape}, V_numerator shape: {V_numerator.shape}, V_denominator shape: {V_denominator.shape}\")\n",
    "\n",
    "        # TODO: ALL TENSORS SHOULD BE SLICED TO HAVE [max_expiry:,...], this removes unnecessary computation\n",
    "        omega_tensor = np.divide(V_terms, V_sum[..., None, None], out=np.zeros_like(V_terms), where=V_sum[..., None, None] != 0)\n",
    "        \n",
    "        phi = np.sum(phi_tensor * omega_tensor, axis=(2, 3))\n",
    "        \n",
    "        # SWAP INDEX EXPIRY OFFSET\n",
    "        swap_hedge_expiry_relative_idx = self.t_to_idx(self.swap_hedge_expiry_relative)\n",
    "        swap_liab_expiry_relative_idx = self.t_to_idx(self.swap_liab_expiry_relative)\n",
    "\n",
    "        \n",
    "\n",
    "        ttm_mat = self.ttm_mat[np.ix_(swap_idxs_0, swap_idxs_1)]\n",
    "        zcb = self.zcb_interp_func(self.f_sim)\n",
    "        annuity = np.sum(swap_indexer(zcb), axis=2) * self.tau\n",
    "        def risk_metrics(offset):\n",
    "            atm_strikes = self.swap_sim.diagonal(offset=offset)\n",
    "            atm_strikes = np.tile(atm_strikes, (self.swap_sim.shape[0], 1))\n",
    "            # remove upper triangular part of the matrix\n",
    "            atm_strikes[np.triu_indices_from(atm_strikes, k=1)] = np.nan\n",
    "            # sabr\n",
    "            start = offset\n",
    "            end = offset + self.swap_sim.shape[0] # each day a new swap, so there must be as many strikes as time steps\n",
    "            sigma_ofs = sigma[:, start:end]\n",
    "            V_ofs = V[:, start:end]\n",
    "            phi_ofs = phi[:, start:end]\n",
    "            # swap\n",
    "            ttm_mat_ofs = ttm_mat[:, start:end]\n",
    "            annuity_ofs = annuity[:, start:end]\n",
    "            \n",
    "            # SWAPTION pricing\n",
    "            swap_sim_ofs = self.swap_sim[:, start:end]\n",
    "            iv = self.sabr_implied_vol(F=swap_sim_ofs, K=atm_strikes, T=ttm_mat_ofs, alpha=sigma_ofs, beta=self.beta, rho=phi_ofs, nu=V_ofs)\n",
    "            price, delta, gamma, vega = self.black_swaption_price(F=swap_sim_ofs, K=atm_strikes, T=ttm_mat_ofs, sigma=iv, annuity=annuity_ofs)\n",
    "            inactive = np.isnan(price)\n",
    "                    # Initialize with zeros (or nan if you prefer to explicitly mask unused rows)\n",
    "            swaption_pnl = np.zeros_like(price)\n",
    "            \n",
    "            # Compute safe differences\n",
    "            swaption_pnl[:-1] = np.subtract(\n",
    "                price[1:, :],       # tomorrow's price\n",
    "                price[:-1, :],      # today's price\n",
    "                out=swaption_pnl[:-1, :],\n",
    "                where=~np.isnan(price[1:, :]) & ~np.isnan(price[:-1, :])\n",
    "            )\n",
    "\n",
    "            \n",
    "            # SWAP pricing\n",
    "            swap_value = annuity_ofs * (swap_sim_ofs- atm_strikes )\n",
    "\n",
    "            swap_pnl = np.zeros_like(swap_value)\n",
    "            swap_pnl[:-1] = np.subtract(\n",
    "                swap_value[1:, :],       # tomorrow's price\n",
    "                swap_value[:-1, :],      # today's price\n",
    "                out=swap_pnl[:-1, :],\n",
    "                where=~np.isnan(swap_value[1:, :]) & ~np.isnan(swap_value[:-1, :])\n",
    "            )\n",
    "\n",
    "            return np.stack([price, delta, gamma, vega, inactive, swaption_pnl], axis=-1), np.stack([swap_value, swap_pnl], axis=-1)\n",
    "        \n",
    "        \n",
    "        hedge_metrics = risk_metrics(swap_hedge_expiry_relative_idx)\n",
    "        liab_metrics = risk_metrics(swap_liab_expiry_relative_idx)\n",
    "\n",
    "        return hedge_metrics, liab_metrics\n",
    "\n",
    "\n",
    "    def sabr_implied_vol(self,\n",
    "    F: np.ndarray,\n",
    "    K: np.ndarray,\n",
    "    T: np.ndarray,\n",
    "    alpha: np.ndarray,\n",
    "    beta: float,\n",
    "    rho: np.ndarray,\n",
    "    nu: np.ndarray,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Hagan SABR implied vol (not just ATM) with numpy broadcasting.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        F : np.ndarray\n",
    "            Forward swap rate (e.g., shape (steps, expiries))\n",
    "        K : np.ndarray\n",
    "            Strike rate (same shape as F for ATM, or broadcastable)\n",
    "        T : np.ndarray\n",
    "            Time to maturity in years\n",
    "        alpha : np.ndarray\n",
    "            Instantaneous vol (sigma0 in SABR)\n",
    "        beta : float\n",
    "            Elasticity parameter\n",
    "        rho : np.ndarray\n",
    "            SABR correlation\n",
    "        nu : np.ndarray\n",
    "            SABR vol-of-vol\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            SABR implied vol, same shape as inputs\n",
    "        \"\"\"\n",
    "        F = np.maximum(F, 1e-8)\n",
    "        K = np.maximum(K, 1e-8)\n",
    "        T = np.maximum(T, 1e-8)\n",
    "        log_FK = np.log(F / K)\n",
    "        z = (nu / alpha) * (F * K) ** ((1 - beta) / 2) * log_FK\n",
    "        x_z = np.log((np.sqrt(1 - 2 * rho * z + z ** 2) + z - rho) / (1 - rho))\n",
    "\n",
    "        # A and B terms\n",
    "        FK_beta = (F * K) ** ((1 - beta) / 2)\n",
    "        A = alpha / (FK_beta * (1 + (1 - beta) ** 2 * log_FK ** 2 / 24 + (1 - beta) ** 4 * log_FK ** 4 / 1920))\n",
    "        B = (\n",
    "            1\n",
    "            + ((1 - beta) ** 2 / 24) * (alpha ** 2 / FK_beta ** 2)\n",
    "            + (rho * beta * nu * alpha) / (4 * FK_beta)\n",
    "            + ((2 - 3 * rho ** 2) * nu ** 2 / 24)\n",
    "        ) * T\n",
    "\n",
    "        # ATM simplified case\n",
    "        atm_mask = np.isclose(F, K)\n",
    "        sigma = np.full_like(F, np.nan)\n",
    "        sigma[atm_mask] = (\n",
    "            alpha[atm_mask]\n",
    "            / (F[atm_mask] ** (1 - beta))\n",
    "            * (1 + ((2 - 3 * rho[atm_mask] ** 2) / 24) * nu[atm_mask] ** 2 * T[atm_mask])\n",
    "        )\n",
    "\n",
    "        # General case\n",
    "        non_atm = ~atm_mask\n",
    "        sigma[non_atm] = A[non_atm] * z[non_atm] / x_z[non_atm] * B[non_atm]\n",
    "\n",
    "        return sigma\n",
    "\n",
    "    def black_swaption_price(self, F, K, T, sigma, annuity=1.0):\n",
    "        \"\"\"\n",
    "        Black's formula for payer swaption pricing.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        F : np.ndarray\n",
    "            Forward swap rate (shape (...,))\n",
    "        K : np.ndarray\n",
    "            Strike swap rate (same shape as F)\n",
    "        T : np.ndarray\n",
    "            Time to maturity (in years)\n",
    "        sigma : np.ndarray\n",
    "            Implied volatility (same shape as F)\n",
    "        annuity : np.ndarray or float\n",
    "            Present value of fixed leg payments (default=1.0)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        price : np.ndarray\n",
    "            Black swaption price\n",
    "        delta : np.ndarray\n",
    "            dPrice/dF\n",
    "        gamma : np.ndarray\n",
    "            d²Price/dF²\n",
    "        vega : np.ndarray\n",
    "            dPrice/dVol\n",
    "        \"\"\"\n",
    "        F = np.maximum(F, 1e-8)\n",
    "        K = np.maximum(K, 1e-8)\n",
    "        # get expiry mask where T is close to 0\n",
    "        expiry_mask = np.isclose(T, 0)\n",
    "        T = np.maximum(T, 1e-8)\n",
    "        \n",
    "        sigma = np.maximum(sigma, 1e-8)\n",
    "\n",
    "        sqrt_T = np.sqrt(T)\n",
    "        d1 = (np.log(F / K) + 0.5 * sigma**2 * T) / (sigma * sqrt_T)\n",
    "        d2 = d1 - sigma * sqrt_T\n",
    "        n_prime = np.exp(-0.5 * d1**2) / np.sqrt(2 * np.pi)\n",
    "\n",
    "        price = annuity * (F * norm.cdf(d1) - K * norm.cdf(d2))\n",
    "        # at ttm == 0, price is just the annuity * max(F-K)\n",
    "        price[expiry_mask] = annuity[expiry_mask] * np.maximum(F[expiry_mask] - K[expiry_mask], 0)\n",
    "        delta = annuity * norm.cdf(d1)\n",
    "        gamma = annuity * n_prime / (F * sigma * sqrt_T)\n",
    "        vega = annuity * F * n_prime * sqrt_T / 100  # divide by 100 for % vol bump\n",
    "\n",
    "        return price, delta, gamma, vega\n",
    "\n",
    "\n",
    "\n",
    "    def plot(self, mat):\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        # Create mesh grid the shape of self.swap_sim\n",
    "        X = np.arange(mat.shape[0])*self.dt\n",
    "        Y = np.arange(mat.shape[1])\n",
    "        X, Y = np.meshgrid(X, Y)\n",
    "        Z = mat.T\n",
    "        # Create surface plot\n",
    "        surf = ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')\n",
    "        # Add labels and colorbar\n",
    "        ax.set_xlabel('Time Steps')\n",
    "        ax.set_ylabel('Expiry')\n",
    "        ax.set_zlabel('Rate')\n",
    "        fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "        # angle so we look down from above\n",
    "        ax.view_init(elev=45, azim=210)\n",
    "\n",
    "\n",
    "lmm = LMMSABR(\n",
    "    rho_mat=rho_mat_0m,\n",
    "    theta_mat=theta_mat_0m,\n",
    "    phi_mat=phi_mat_0m,\n",
    "    g_params=params_g,\n",
    "    h_params=params_h,\n",
    "    epsilon_exp=epsilon_exp,\n",
    "    k0_exp=s0_exp,\n",
    "    df_cap=df_cap,\n",
    "    df_raw_spot=df_raw_spot,\n",
    "    resolution=2, swap_hedge_expiry=1, swap_client_expiry=2, tenor=1, t_max=None, sim_time =1\n",
    ")\n",
    "# print all rows and columns of the numpy array with setting printoptions to remove limits, also reduce precition to 4 \n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)\n",
    "\n",
    "lmm.simulate(seed=42)\n",
    "\n",
    "\n",
    "# shift all values to the right by 1\n",
    "#lmm.get_sabr_params()[-1]\n",
    "#print(\"swap sim shape: \",lmm.swap_sim_shape)\n",
    "np.sum(lmm.ggh_tensor, axis=(2, 3))\n",
    "lmm.get_swap_matrix()\n",
    "\n",
    "position = lmm.get_sabr_params()[0][0][:,:,2]*1000\n",
    "#delta = lmm.get_sabr_params()[0][0, :, 1]\n",
    "\n",
    "#print(lmm.swap_sim)\n",
    "print(position)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_max set to 4\n",
      "Curve preparation: 0.05s\n",
      "G tensor preparation: 0.04s\n",
      "V tensor preparation: 0.71s\n"
     ]
    }
   ],
   "source": [
    "lmm = LMMSABR(\n",
    "    rho_mat=rho_mat_0m,\n",
    "    theta_mat=theta_mat_0m,\n",
    "    phi_mat=phi_mat_0m,\n",
    "    g_params=params_g,\n",
    "    h_params=params_h,\n",
    "    epsilon_exp=epsilon_exp,\n",
    "    k0_exp=s0_exp,\n",
    "    df_cap=df_cap,\n",
    "    df_raw_spot=df_raw_spot,\n",
    "    resolution=26, swap_hedge_expiry=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 53, 6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def single_run(seed):\n",
    "    lmm.simulate(seed=seed)\n",
    "    lmm.get_swap_matrix()\n",
    "    res = lmm.get_sabr_params()\n",
    "    return res\n",
    "single_run(42)\n",
    "n_episodes = 2\n",
    "results = [single_run(i) for i in range(2)]\n",
    "\n",
    "\n",
    "results[0][0][0].shape\n",
    "# episode, hedge/liab, swaption/swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "n_jobs = 4  # uses all available CPU cores\n",
    "n_sims = 1000\n",
    "\n",
    "results = Parallel(n_jobs=n_jobs)(\n",
    "    delayed(single_run)(seed) for seed in range(n_sims))\n",
    "del lmm\n",
    "del results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LMMSABR' object has no attribute 'swap_hedge_expiry_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimportlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reload\n\u001b[1;32m      5\u001b[0m reload(utils)\n\u001b[0;32m----> 7\u001b[0m utils\u001b[38;5;241m.\u001b[39mUtils(\u001b[43mlmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswap_hedge_expiry_idx\u001b[49m, np_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, num_sim \u001b[38;5;241m=\u001b[39m lmm\u001b[38;5;241m.\u001b[39mswap_sim\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mconvert_tensor_to_option_objects(results)\n\u001b[1;32m      8\u001b[0m utils\u001b[38;5;241m.\u001b[39mUtils(lmm\u001b[38;5;241m.\u001b[39mswap_hedge_expiry_idx, np_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, num_sim \u001b[38;5;241m=\u001b[39m lmm\u001b[38;5;241m.\u001b[39mswap_sim\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39magg_poisson_dist(np\u001b[38;5;241m.\u001b[39mones_like(results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]), np\u001b[38;5;241m.\u001b[39mones_like(results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LMMSABR' object has no attribute 'swap_hedge_expiry_idx'"
     ]
    }
   ],
   "source": [
    "# import utils.py\n",
    "import environment.utils as utils\n",
    "# reload import \n",
    "from importlib import reload\n",
    "reload(utils)\n",
    "\n",
    "utils.Utils(lmm.swap_hedge_expiry_idx, np_seed=42, num_sim = lmm.swap_sim.shape[1]).convert_tensor_to_option_objects(results)\n",
    "utils.Utils(lmm.swap_hedge_expiry_idx, np_seed=42, num_sim = lmm.swap_sim.shape[1]).agg_poisson_dist(np.ones_like(results[0][0]), np.ones_like(results[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d,e = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SABR():\n",
    "    def __init__(self, sigma_0, beta, rho, volvol, K, r, q):\n",
    "        self.sigma_0 = sigma_0\n",
    "        self.beta = beta\n",
    "        self.rho = rho\n",
    "        self.volvol = volvol\n",
    "        self.K = K\n",
    "        self.r = r\n",
    "        self.q = q\n",
    "\n",
    "    def get_implied_vol(self, tt, price):\n",
    "        \"\"\"Convert SABR instantaneous vol to option implied vol\n",
    "\n",
    "        Args:\n",
    "            tt (np.ndarray): time to maturity in shape (num_period,)\n",
    "            price (np.ndarray): underlying stock price in shape (num_path, num_period)\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: implied vol in shape (num_path, num_period)\n",
    "        \"\"\"\n",
    "        return self._sabr_implied_vol(self.sigma_0 * np.ones_like(price), tt, price)\n",
    "    def _sabr_implied_vol(self, vol, tt, price):\n",
    "            \"\"\"Convert SABR instantaneous vol to option implied vol\n",
    "\n",
    "            Args:\n",
    "                vol (np.ndarray): SABR instantaneous vol in shape (num_path, num_period)\n",
    "                tt (np.ndarray): time to maturity in shape (num_period,)\n",
    "                price (np.ndarray): underlying stock price in shape (num_path, num_period)\n",
    "\n",
    "            Returns:\n",
    "                np.ndarray: implied vol in shape (num_path, num_period)\n",
    "            \"\"\"\n",
    "            F = price * np.exp((self.r - self.q) * tt)\n",
    "            x = (F * self.K) ** ((1 - self.beta) / 2)\n",
    "            y = (1 - self.beta) * np.log(F / self.K)\n",
    "            A = vol / (x * (1 + y * y / 24 + y * y * y * y / 1920))\n",
    "            B = 1 + tt * (\n",
    "                    ((1 - self.beta) ** 2) * (vol * vol) / (24 * x * x)\n",
    "                    + self.rho * self.beta * self.volvol * vol / (4 * x)\n",
    "                    + self.volvol * self.volvol * (2 - 3 * self.rho * self.rho) / 24\n",
    "            )\n",
    "            Phi = (self.volvol * x / vol) * np.log(F / self.K)\n",
    "            # print(\"CHI INTERNAL\")\n",
    "            # print((np.sqrt(1 - 2 * self.rho * Phi + Phi * Phi) + Phi - self.rho) / (1 - self.rho))\n",
    "            Chi = np.log((np.sqrt(1 - 2 * self.rho * Phi + Phi * Phi) + Phi - self.rho) / (1 - self.rho))\n",
    "            # print(\"JUST CHI\")\n",
    "            # print(Chi)\n",
    "            Chi = Chi\n",
    "\n",
    "            epsilon = 1e-12\n",
    "\n",
    "            # if abs(F - self.K) < epsilon:\n",
    "            #     # ATM case — guard against F being 0\n",
    "            #     safe_F = max(F, epsilon)\n",
    "            #     SABRIV = vol * B / (safe_F ** (1 - self.beta))\n",
    "            # else:\n",
    "            #     # non-ATM case — guard against Chi being too small\n",
    "            #     safe_Chi = Chi if abs(Chi) > epsilon else epsilon\n",
    "            #     SABRIV = A * B * Phi / safe_Chi\n",
    "            SABRIV = np.where(np.isclose(F, self.K), vol * B / (F ** (1 - self.beta)), A * B * Phi / Chi)\n",
    "\n",
    "            return SABRIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "self.resolution = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lmm = LMMSABR(rho_mat_0m, theta_mat_0m, phi_mat, params_g, params_h, epsilon_exp, k0_exp=s0_exp, yell=False)\n",
    "lmm.simulate(self.resolution=self.resolution, max_expiry=10, tenor=1)\n",
    "\n",
    "\n",
    "#V_sum*100\n",
    "\n",
    "np.sum(omega_tensor, axis=(2, 3))\n",
    "phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
